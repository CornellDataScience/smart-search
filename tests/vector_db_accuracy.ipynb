{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary + Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def get_qa_data(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        json_file.close()\n",
    "    return data\n",
    "\n",
    "print(os.getcwd())\n",
    "qa_data = get_qa_data('../summaries.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "qa_df = pd.DataFrame(qa_data)\n",
    "qa_df['ids'] = [str(datetime.now()).replace(' ','-') + \"-id-\" + str(i) for i in range(len(qa_df))]\n",
    "qa_df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Libraries + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "# from tqdm import tqdm\n",
    "# import torch \n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Set up inference prompt\n",
    "\n",
    "# QUERY_PROMPT = \"\"\"### Task\n",
    "# Generate a question that could realistically have the given code snippet as a response.\n",
    "# To be clear, we want to reverse-engineer a question from a given response (code snippet).\n",
    "\n",
    "# ### Code Snippet\n",
    "# ```\n",
    "# {code_snippet}\n",
    "# ```\n",
    "\n",
    "# ### Explanation of Code\n",
    "# {code_summary}\n",
    "\n",
    "# ### Warnings\n",
    "# Do not make your question too specific. Make your question general yet suitable for the resulting code snippet.\n",
    "\n",
    "# ### Potential Question\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # HuggingFace Inference API - Ran out of tokens\n",
    "\n",
    "# client = InferenceClient(\n",
    "#     provider = \"hf-inference\"\n",
    "# )\n",
    "\n",
    "# def query_huggingface(code_snippet, code_summary):\n",
    "#     messages = [\n",
    "#         {\n",
    "#             \"role\" : \"user\",\n",
    "#             \"content\" : QUERY_PROMPT.format(code_snippet = code_snippet, code_summary = code_summary)\n",
    "#         }\n",
    "#     ]\n",
    "#     return client.chat.completions.create(\n",
    "#         model = \"google/gemma-2-2b-it\",\n",
    "#         messages = messages,\n",
    "#         max_tokens = 500,\n",
    "#         stream = False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local Generation with Transformers    \n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map = \"auto\")  # Loads onto available GPU/CPU\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "# def query_local(code_snippet, code_summary):\n",
    "#     return generator(\n",
    "#         QUERY_PROMPT.format(code_snippet = code_snippet, code_summary = code_summary), \n",
    "#         max_length=500, \n",
    "#         temperature=0.7, \n",
    "#         do_sample=True\n",
    "#     )[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local Generation with Ollama\n",
    "# import ollama\n",
    "\n",
    "# def query_ollama(code_snippet, code_summary):\n",
    "#     return ollama.generate(\n",
    "#         model=\"gemma3:1b\",\n",
    "#         prompt= QUERY_PROMPT.format(code_snippet = code_snippet, code_summary = code_summary),\n",
    "#         stream = False,\n",
    "#         options={'num_predict': -1, 'keep_alive': 0},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate summaries and add to dataframe\n",
    "# questions = []\n",
    "# for i in tqdm(range(len(qa_df)), desc=\"Queries to HF API\"):\n",
    "#     row = qa_df.iloc[i]\n",
    "#     questions.append(query_ollama(row[\"code\"], row[\"summary\"]))\n",
    "# qa_df[\"questions\"] = questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to file as backup\n",
    "with open(\"test_dataset.json\", 'w') as json_file:\n",
    "    # json_data = qa_df.to_json()\n",
    "    for i in range(len(qa_df)):\n",
    "        qa_df.iloc[i][\"metadata\"].update({\"code\" : qa_df.iloc[i][\"code\"]})\n",
    "    qa_df = qa_df.to_dict(orient = \"list\")\n",
    "    json.dump(qa_df, json_file, ensure_ascii = False, indent = 4)\n",
    "\n",
    "    json_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")  # Persist data locally\n",
    "\n",
    "# # With huggingface embeddings, in case we ever transition to open source implementation\n",
    "# huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "#     api_key=\"\",\n",
    "#     model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "# )\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(chroma_client.list_collections())\n",
    "if \"test\" in chroma_client.list_collections():\n",
    "    chroma_client.delete_collection(\"test\")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test\",\n",
    "    # embedding_function = huggingface_ef,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\"\n",
    "    }\n",
    ")\n",
    "\n",
    "embeddings = model.encode(qa_df[\"summary\"]).tolist()\n",
    "\n",
    "collection.add(\n",
    "    documents= qa_df[\"summary\"],\n",
    "    metadatas= qa_df[\"metadata\"],\n",
    "    ids= qa_df[\"ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_embeddings=model.encode(\"What functions area available in MathSearch?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
