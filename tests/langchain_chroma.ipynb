{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49bbd96",
   "metadata": {},
   "source": [
    "### LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771f91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = None\n",
    "with open(\"test_dataset.json\", 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    json_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0eb78de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1807015/3242935587.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  EMBEDDINGS = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "EMBEDDINGS = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "PERSIST_DIR = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"test\"\n",
    "\n",
    "# VDB Initialization function\n",
    "def get_vdb():\n",
    "    chroma_client = chromadb.PersistentClient(path=PERSIST_DIR)\n",
    "\n",
    "    # Check if a collection exists\n",
    "    existing_collections = chroma_client.list_collections()\n",
    "    print(existing_collections)\n",
    "    collection_name = \"test\"\n",
    "\n",
    "    if collection_name in existing_collections:\n",
    "        print(f\"Collection '{collection_name}' exists. Getting existing...\")\n",
    "        vectordb = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=EMBEDDINGS,\n",
    "            persist_directory=PERSIST_DIR\n",
    "        )\n",
    "        print(\"Retrieved.\")\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' does not exist.\")\n",
    "        vectordb = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=EMBEDDINGS,\n",
    "            persist_directory=PERSIST_DIR\n",
    "        )\n",
    "        vectordb.add_texts(texts=data['summary'], metadatas=data['metadata'], ids=data['ids'])\n",
    "    return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22fd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic querying function\n",
    "def query(q):\n",
    "    vectordb = get_vdb()\n",
    "    results = vectordb.similarity_search(q)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "984be25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"### Task\n",
    "You are to answer questions about the Cornell Data Science project team.\n",
    "You specialize in answering questions about code repositories.\n",
    "You will be provided with a user's question, along with snippets of code that should provide you with the context you need to answer them.\n",
    "\n",
    "### Warning\n",
    "DO NOT REFERENCE OUTSIDE INFORMATION IN YOUR RESPONSE. Answer only within the bounds of the context provided.\n",
    "\n",
    "### User Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\n",
    "### Answer\n",
    "\"\"\"\n",
    "\n",
    "def get_llm_prompt(q):\n",
    "    docs = query(q)\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"===== Snippet {i} =====\\n<Summary>{doc.page_content}</Summary>\\n<Raw Code>{doc.metadata['code']}</Raw Code\\n<Source>{doc.metadata['context']}</Source>\"\n",
    "    for i, doc in enumerate(docs)])\n",
    "\n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=TEMPLATE\n",
    "    )\n",
    "\n",
    "    formatted_prompt = template.format(\n",
    "        context=context,\n",
    "        question=\"Can you summarize the mathematical features of MathSearch?\"\n",
    "    )\n",
    "\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6a6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test']\n",
      "Collection 'test' exists. Getting existing...\n",
      "Retrieved.\n"
     ]
    }
   ],
   "source": [
    "llm_prompt = get_llm_prompt(\"What functions are available in MathSearch?\")\n",
    "print(llm_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
