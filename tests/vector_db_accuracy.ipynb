{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary + Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure Summary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Libraries + Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "# from tqdm import tqdm\n",
    "# import torch \n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Set up inference prompt\n",
    "\n",
    "# QUERY_PROMPT = \"\"\"### Task\n",
    "# Generate a question that could realistically have the given code snippet as a response.\n",
    "# To be clear, we want to reverse-engineer a question from a given response (code snippet).\n",
    "\n",
    "# ### Code Snippet\n",
    "# ```\n",
    "# {code_snippet}\n",
    "# ```\n",
    "\n",
    "# ### Explanation of Code\n",
    "# {code_summary}\n",
    "\n",
    "# ### Warnings\n",
    "# Do not make your question too specific. Make your question general yet suitable for the resulting code snippet.\n",
    "\n",
    "# ### Potential Question\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local Generation with Transformers    \n",
    "# model_name = \"google/gemma-2-2b-it\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map = \"auto\")  # Loads onto available GPU/CPU\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model = model, tokenizer = tokenizer)\n",
    "\n",
    "# def query_local(code_snippet, code_summary):\n",
    "#     return generator(\n",
    "#         QUERY_PROMPT.format(code_snippet = code_snippet, code_summary = code_summary), \n",
    "#         max_length=500, \n",
    "#         temperature=0.7, \n",
    "#         do_sample=True\n",
    "#     )[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local Generation with Ollama\n",
    "# import ollama\n",
    "\n",
    "# def query_ollama(code_snippet, code_summary):\n",
    "#     return ollama.generate(\n",
    "#         model=\"gemma3:1b\",\n",
    "#         prompt= QUERY_PROMPT.format(code_snippet = code_snippet, code_summary = code_summary),\n",
    "#         stream = False,\n",
    "#         options={'num_predict': -1, 'keep_alive': 0},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data to file as backup\n",
    "with open(\"test_dataset.json\", 'w') as json_file:\n",
    "    # json_data = qa_df.to_json()\n",
    "    for i in range(len(qa_df)):\n",
    "        qa_df.iloc[i][\"metadata\"].update({\"code\" : qa_df.iloc[i][\"code\"]})\n",
    "    qa_df = qa_df.to_dict(orient = \"list\")\n",
    "    json.dump(qa_df, json_file, ensure_ascii = False, indent = 4)\n",
    "\n",
    "    json_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.query(query_embeddings=model.encode(\"What functions area available in MathSearch?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
