{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary + Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_qa_data(path):\n",
    "    with open(path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        json_file.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "qa_data = get_qa_data('path_to_process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-11B-Vision\"\n",
    "HEADERS = {\"Authorization\": f\"Bearer \"}\n",
    "\n",
    "def query_huggingface(prompt):\n",
    "    payload = {\"inputs\": prompt}\n",
    "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "QUERY_PROMPT = \"\"\"### Task\n",
    "Generate a question that could realistically have the given code snippet as a response.\n",
    "To be clear, we want to reverse-engineer a question from a given response (code snippet).\n",
    "\n",
    "### Code Snippet\n",
    "{code_snippet}\n",
    "\n",
    "### Warnings\n",
    "Do not make your question too specific. Make your question general yet suitable for the resulting code snippet.\n",
    "\n",
    "### Potential Question\n",
    "\"\"\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for qa in qa_data:\n",
    "    \n",
    "    response = query_huggingface(QUERY_PROMPT.format(code_snippet = qa['code']))\n",
    "    response.update(qa)\n",
    "    results.append(response)\n",
    "\n",
    "with open(\"test_dataset.json\", 'w') as json_file:\n",
    "    json.dump(results, json_file, ensure_ascii = False, indent = 4)\n",
    "    json_file.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "data = None\n",
    "\n",
    "with open(\"test_dataset.json\", 'w') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "texts = [dictionary.pop(\"summary\") for dictionary in data]\n",
    "metadatas = data\n",
    "ids = [str(datetime.now()) + \"-\" + i for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# With huggingface embeddings, in case we ever transition to open source implementation\n",
    "huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(\n",
    "    api_key=\"\",\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"test\",\n",
    "    embedding_function = huggingface_ef,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\"\n",
    "    }\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    documents= texts,\n",
    "    metadatas= metadatas,\n",
    "    ids= ids\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
