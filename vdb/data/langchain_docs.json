[
  {
    "page_content": "# DIRECTORY: root\n\n## PURPOSE:\nThis root directory houses a complete mathematical equation search system that enables users to find similar equations in PDF documents by analyzing uploaded query equation images, with components spanning from ML model training to serverless processing to web interface visualization.\n\n## COMPONENT STRUCTURE:\n- **lambda-container/**: Contains the serverless AWS Lambda implementation that forms the processing pipeline for equation search, handling everything from PDF extraction to equation matching.\n- **ml-model/**: Houses the YOLOv8-based object detection components for implementing, training, and deploying models that detect equations in document images.\n- **front-end/**: Implements the web interface for the application, providing API endpoints for search requests and visualization tools for displaying search results.\n\n## ARCHITECTURE:\nThe system follows a microservice architecture where specialized components handle distinct aspects of the pipeline: machine learning models detect equations, serverless functions process search requests, and a front-end visualizes results. This architecture enables scalable, event-driven processing with clear separation of concerns.\n\n## ENTRY POINTS:\n- **lambda_handler** in lambda-container/lambda_function.py: Processes search requests via AWS Lambda\n- **main()** function in ml-model/yolov8_predict.py: Runs equation detection inference\n- **api.py:result()** in front-end/web/: Serves as the primary endpoint for retrieving search results\n\n## DATA FLOW:\n1. User requests flow through the front-end API\n2. Lambda functions retrieve PDFs and query images from S3\n3. ML models detect equations in document images \n4. Detected equations are converted to LaTeX for comparison\n5. Results are ranked by similarity and visualized with annotations\n6. The front-end presents processed results to users through the web interface\n\n## INTEGRATION:\nThe system integrates multiple AWS services (Lambda, S3, SQS, SageMaker) with external APIs like Mathpix for LaTeX conversion. It employs libraries for image processing, PDF handling, and string comparison, connecting these components through standardized data formats and API contracts.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include pipeline architectures for sequential data processing, service adapters for AWS interactions, command-line utilities for local execution, configuration management through external files, and separation between core logic and I/O operations to enable flexibility in deployment contexts.\n\n## RELATIONSHIPS:\nThe three main components form a cohesive workflow: the ML models provide equation detection capabilities consumed by the Lambda processing pipeline, which generates results that the front-end visualizes for users. This separation allows each component to evolve independently while maintaining a consistent interface between them.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "root",
      "path": "",
      "code": "",
      "summary": "# DIRECTORY: root\n\n## PURPOSE:\nThis root directory houses a complete mathematical equation search system that enables users to find similar equations in PDF documents by analyzing uploaded query equation images, with components spanning from ML model training to serverless processing to web interface visualization.\n\n## COMPONENT STRUCTURE:\n- **lambda-container/**: Contains the serverless AWS Lambda implementation that forms the processing pipeline for equation search, handling everything from PDF extraction to equation matching.\n- **ml-model/**: Houses the YOLOv8-based object detection components for implementing, training, and deploying models that detect equations in document images.\n- **front-end/**: Implements the web interface for the application, providing API endpoints for search requests and visualization tools for displaying search results.\n\n## ARCHITECTURE:\nThe system follows a microservice architecture where specialized components handle distinct aspects of the pipeline: machine learning models detect equations, serverless functions process search requests, and a front-end visualizes results. This architecture enables scalable, event-driven processing with clear separation of concerns.\n\n## ENTRY POINTS:\n- **lambda_handler** in lambda-container/lambda_function.py: Processes search requests via AWS Lambda\n- **main()** function in ml-model/yolov8_predict.py: Runs equation detection inference\n- **api.py:result()** in front-end/web/: Serves as the primary endpoint for retrieving search results\n\n## DATA FLOW:\n1. User requests flow through the front-end API\n2. Lambda functions retrieve PDFs and query images from S3\n3. ML models detect equations in document images \n4. Detected equations are converted to LaTeX for comparison\n5. Results are ranked by similarity and visualized with annotations\n6. The front-end presents processed results to users through the web interface\n\n## INTEGRATION:\nThe system integrates multiple AWS services (Lambda, S3, SQS, SageMaker) with external APIs like Mathpix for LaTeX conversion. It employs libraries for image processing, PDF handling, and string comparison, connecting these components through standardized data formats and API contracts.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include pipeline architectures for sequential data processing, service adapters for AWS interactions, command-line utilities for local execution, configuration management through external files, and separation between core logic and I/O operations to enable flexibility in deployment contexts.\n\n## RELATIONSHIPS:\nThe three main components form a cohesive workflow: the ML models provide equation detection capabilities consumed by the Lambda processing pipeline, which generates results that the front-end visualizes for users. This separation allows each component to evolve independently while maintaining a consistent interface between them."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/design",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# DIRECTORY: lambda-container\n\n## PURPOSE:\nThis directory contains a serverless AWS Lambda function implementation that processes mathematical equation search requests, enabling users to find similar equations in PDF documents by comparing and ranking them against a query equation image.\n\n## COMPONENT STRUCTURE:\n- **lambda_function.py**: The primary entry point implementing the AWS Lambda handler and equation search pipeline.\n- **dataHandler.py**: Provides utilities for AWS-based data management, handling S3 objects and SQS queues.\n- **constants.py**: Contains configuration parameters, default values, and system-wide settings.\n- **archive/** directory: Contains older versions of the implementation, including old-lf.py with a previous equation search system.\n\n## ARCHITECTURE:\nThe system follows a serverless pipeline architecture that processes SQS messages to transform PDF documents and query images through multiple stages: file retrieval, equation detection using YOLO, LaTeX conversion via Mathpix, similarity comparison using Levenshtein distance, and result visualization with annotated PDFs.\n\n## ENTRY POINTS:\n- **lambda_handler** in lambda_function.py: Main function that processes SQS messages and orchestrates the equation search workflow.\n- **downloadDirectoryFroms3**: Function for retrieving input files from S3 storage to begin processing.\n\n## DATA FLOW:\n1. Incoming SQS messages trigger the Lambda function\n2. PDFs and query images are downloaded from S3\n3. PDF pages are converted to images for processing\n4. Equations are detected in these images using YOLO models\n5. Detected equations are converted to LaTeX using Mathpix\n6. Similarity is calculated between query and document equations\n7. Results are visualized with annotated PDFs uploaded back to S3\n8. SQS messages are deleted upon successful processing\n\n## INTEGRATION:\nThe system integrates with multiple AWS services (S3, SQS, SageMaker, Lambda), external APIs (Mathpix for LaTeX conversion), and utilizes libraries for image processing (PIL, OpenCV), PDF handling (pdf2image, FPDF), and string comparison algorithms (Levenshtein distance) to form a complete equation search solution.\n\n## DEVELOPMENT PATTERNS:\nThe codebase follows a functional transformation pattern with discrete processing stages and clear interfaces. It employs service adapter patterns to abstract AWS interactions, asynchronous processing via AWS Lambda, specialized algorithms for equation similarity, and visualization techniques for highlighting matched equations.\n\n## RELATIONSHIPS:\nThe components form a cohesive workflow where dataHandler.py manages AWS service interactions, constants.py provides configuration parameters, and lambda_function.py orchestrates the entire pipeline from input retrieval to similarity calculation to result generation. The functions work together to transform document data through multiple representations (PDF\u2192images\u2192LaTeX\u2192similarity scores\u2192annotated outputs).",
    "metadata": {
      "type": "DIRECTORY",
      "name": "lambda-container",
      "path": "../mathsearch/lambda-container",
      "code": "",
      "summary": "# DIRECTORY: lambda-container\n\n## PURPOSE:\nThis directory contains a serverless AWS Lambda function implementation that processes mathematical equation search requests, enabling users to find similar equations in PDF documents by comparing and ranking them against a query equation image.\n\n## COMPONENT STRUCTURE:\n- **lambda_function.py**: The primary entry point implementing the AWS Lambda handler and equation search pipeline.\n- **dataHandler.py**: Provides utilities for AWS-based data management, handling S3 objects and SQS queues.\n- **constants.py**: Contains configuration parameters, default values, and system-wide settings.\n- **archive/** directory: Contains older versions of the implementation, including old-lf.py with a previous equation search system.\n\n## ARCHITECTURE:\nThe system follows a serverless pipeline architecture that processes SQS messages to transform PDF documents and query images through multiple stages: file retrieval, equation detection using YOLO, LaTeX conversion via Mathpix, similarity comparison using Levenshtein distance, and result visualization with annotated PDFs.\n\n## ENTRY POINTS:\n- **lambda_handler** in lambda_function.py: Main function that processes SQS messages and orchestrates the equation search workflow.\n- **downloadDirectoryFroms3**: Function for retrieving input files from S3 storage to begin processing.\n\n## DATA FLOW:\n1. Incoming SQS messages trigger the Lambda function\n2. PDFs and query images are downloaded from S3\n3. PDF pages are converted to images for processing\n4. Equations are detected in these images using YOLO models\n5. Detected equations are converted to LaTeX using Mathpix\n6. Similarity is calculated between query and document equations\n7. Results are visualized with annotated PDFs uploaded back to S3\n8. SQS messages are deleted upon successful processing\n\n## INTEGRATION:\nThe system integrates with multiple AWS services (S3, SQS, SageMaker, Lambda), external APIs (Mathpix for LaTeX conversion), and utilizes libraries for image processing (PIL, OpenCV), PDF handling (pdf2image, FPDF), and string comparison algorithms (Levenshtein distance) to form a complete equation search solution.\n\n## DEVELOPMENT PATTERNS:\nThe codebase follows a functional transformation pattern with discrete processing stages and clear interfaces. It employs service adapter patterns to abstract AWS interactions, asynchronous processing via AWS Lambda, specialized algorithms for equation similarity, and visualization techniques for highlighting matched equations.\n\n## RELATIONSHIPS:\nThe components form a cohesive workflow where dataHandler.py manages AWS service interactions, constants.py provides configuration parameters, and lambda_function.py orchestrates the entire pipeline from input retrieval to similarity calculation to result generation. The functions work together to transform document data through multiple representations (PDF\u2192images\u2192LaTeX\u2192similarity scores\u2192annotated outputs)."
    }
  },
  {
    "page_content": "# Standardized Natural Language Summary\n\n## FILE: dataHandler.py\n\n## OVERVIEW:\nThis file provides utilities for AWS-based data handling, particularly focused on managing images and messages between S3 storage and SQS queues in a distributed processing system.\n\n## KEY COMPONENTS:\n- `__init__`: Initializes AWS service clients (SQS and S3) and HTTP connection pools for web requests\n- `list_s3_objects`: Lists objects stored in a specified Amazon S3 bucket\n- `extract_uuid`: Extracts a UUID from a file name by removing a specific prefix and suffix\n- `delete_sqs_message`: Deletes a specific message from an Amazon SQS queue using its receipt handle\n- `is_expected_image_present`: Checks if a specific image name exists within a list of object keys\n\n## ARCHITECTURE:\nThe file implements a service adapter pattern that abstracts AWS interactions for file and message handling. It provides specific utilities for both S3 object management and SQS message processing, suggesting a component that bridges asynchronous message-based workflows with file storage operations.\n\n## DATA FLOW:\n1. AWS clients are initialized when the class is instantiated\n2. Messages are received from SQS (implied by delete functionality)\n3. S3 objects are listed and checked for specific images\n4. UUIDs are extracted from filenames for tracking/identification\n5. SQS messages are deleted after processing\n\n## INTEGRATION POINTS:\n- AWS S3 for object storage and retrieval\n- AWS SQS for message queue operations\n- HTTP connections for external service communication\n- Likely integrates with image processing pipelines that use UUID-based file tracking\n\n## USAGE PATTERNS:\n1. Initialize the handler to establish AWS connections\n2. Check S3 buckets for presence of specific images\n3. Process image files with extraction of UUIDs for identification\n4. Remove SQS messages after successful processing\n5. List S3 bucket contents to inventory available objects\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- urllib3: HTTP client library\n- AWS credentials configuration\n- Network connectivity for AWS service access\n\n## RELATIONSHIPS:\nThe functions work together in a typical serverless workflow pattern. SQS messages likely trigger processing of S3 objects (images), where objects are first listed/verified, then processed (with UUIDs extracted for tracking), and finally the triggering SQS messages are deleted upon successful completion.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "dataHandler.py",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "from constants import *\n#from datetime import datetime\n#import os\nimport boto3\nimport urllib3\n#from botocore.vendored import requests\n\nclass DataHandler():\n    def __init__(self):\n        self.clients = [boto3.client('sqs', region_name='us-east-1'), boto3.client('s3')]\n        self.http = urllib3.PoolManager()\n    \n    def list_s3_objects(self, bucket_name):\n        s3 = boto3.client(\"s3\")\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        return response.get('Contents', [])\n    \n    def extract_uuid(self, file_name):\n        return file_name[7:-4]\n    \n    def delete_sqs_message(self, queue_url, receipt_handle):\n        sqs = self.clients[0]\n        sqs.delete_message(QueueUrl=queue_url, ReceiptHandle=receipt_handle)\n        \n    def is_expected_image_present(self, objects, expected_image):\n        for object in objects:\n            if expected_image in object['Key']:\n                return True\n        return False",
      "summary": "# Standardized Natural Language Summary\n\n## FILE: dataHandler.py\n\n## OVERVIEW:\nThis file provides utilities for AWS-based data handling, particularly focused on managing images and messages between S3 storage and SQS queues in a distributed processing system.\n\n## KEY COMPONENTS:\n- `__init__`: Initializes AWS service clients (SQS and S3) and HTTP connection pools for web requests\n- `list_s3_objects`: Lists objects stored in a specified Amazon S3 bucket\n- `extract_uuid`: Extracts a UUID from a file name by removing a specific prefix and suffix\n- `delete_sqs_message`: Deletes a specific message from an Amazon SQS queue using its receipt handle\n- `is_expected_image_present`: Checks if a specific image name exists within a list of object keys\n\n## ARCHITECTURE:\nThe file implements a service adapter pattern that abstracts AWS interactions for file and message handling. It provides specific utilities for both S3 object management and SQS message processing, suggesting a component that bridges asynchronous message-based workflows with file storage operations.\n\n## DATA FLOW:\n1. AWS clients are initialized when the class is instantiated\n2. Messages are received from SQS (implied by delete functionality)\n3. S3 objects are listed and checked for specific images\n4. UUIDs are extracted from filenames for tracking/identification\n5. SQS messages are deleted after processing\n\n## INTEGRATION POINTS:\n- AWS S3 for object storage and retrieval\n- AWS SQS for message queue operations\n- HTTP connections for external service communication\n- Likely integrates with image processing pipelines that use UUID-based file tracking\n\n## USAGE PATTERNS:\n1. Initialize the handler to establish AWS connections\n2. Check S3 buckets for presence of specific images\n3. Process image files with extraction of UUIDs for identification\n4. Remove SQS messages after successful processing\n5. List S3 bucket contents to inventory available objects\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- urllib3: HTTP client library\n- AWS credentials configuration\n- Network connectivity for AWS service access\n\n## RELATIONSHIPS:\nThe functions work together in a typical serverless workflow pattern. SQS messages likely trigger processing of S3 objects (images), where objects are first listed/verified, then processed (with UUIDs extracted for tracking), and finally the triggering SQS messages are deleted upon successful completion."
    }
  },
  {
    "page_content": "# Standardized Natural Language Summary\n\n## FUNCTION: __init__\n\n### PURPOSE:\nInitializes an object with AWS service clients (SQS and S3) and an HTTP connection pool manager for making web requests.\n\n### INPUTS:\n- None (self reference only)\n\n### OUTPUTS:\n- None (constructor initializes instance attributes)\n\n### KEY STEPS:\n- Creates a list of AWS service clients:\n  - An SQS client configured for us-east-1 region\n  - An S3 client with default configuration\n- Initializes an HTTP connection pool manager through urllib3\n\n### DEPENDENCIES:\n- boto3: AWS SDK for Python\n- urllib3: HTTP client library\n\n### USAGE CONTEXT:\nUsed when instantiating a class that needs to interact with AWS SQS and S3 services while also making HTTP requests.\n\n### EDGE CASES:\n- May raise exceptions if AWS credentials are not properly configured\n- Network connectivity issues might affect the initialization of clients\n\n### RELATIONSHIPS:\n- Provides AWS clients and HTTP connectivity that will be used by other methods in the class\n- Sets up foundation for AWS service interactions in the us-east-1 region\n\n    def __init__(self):\n        self.clients = [boto3.client('sqs', region_name='us-east-1'), boto3.client('s3')]\n        self.http = urllib3.PoolManager()",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "    def __init__(self):\n        self.clients = [boto3.client('sqs', region_name='us-east-1'), boto3.client('s3')]\n        self.http = urllib3.PoolManager()",
      "summary": "# Standardized Natural Language Summary\n\n## FUNCTION: __init__\n\n### PURPOSE:\nInitializes an object with AWS service clients (SQS and S3) and an HTTP connection pool manager for making web requests.\n\n### INPUTS:\n- None (self reference only)\n\n### OUTPUTS:\n- None (constructor initializes instance attributes)\n\n### KEY STEPS:\n- Creates a list of AWS service clients:\n  - An SQS client configured for us-east-1 region\n  - An S3 client with default configuration\n- Initializes an HTTP connection pool manager through urllib3\n\n### DEPENDENCIES:\n- boto3: AWS SDK for Python\n- urllib3: HTTP client library\n\n### USAGE CONTEXT:\nUsed when instantiating a class that needs to interact with AWS SQS and S3 services while also making HTTP requests.\n\n### EDGE CASES:\n- May raise exceptions if AWS credentials are not properly configured\n- Network connectivity issues might affect the initialization of clients\n\n### RELATIONSHIPS:\n- Provides AWS clients and HTTP connectivity that will be used by other methods in the class\n- Sets up foundation for AWS service interactions in the us-east-1 region"
    }
  },
  {
    "page_content": "# FUNCTION: list_s3_objects\n\n## PURPOSE:\nLists objects stored in a specified Amazon S3 bucket by retrieving metadata about the objects. This function provides an interface to access S3 bucket contents.\n\n## INPUTS:\n- `self`: Instance of the containing class\n- `bucket_name` (string): Name of the S3 bucket to list objects from\n\n## OUTPUTS:\n- List of dictionaries: Each dictionary contains metadata about an object in the bucket (e.g., Key, LastModified, ETag, Size). Returns an empty list if the bucket is empty or doesn't exist.\n\n## KEY STEPS:\n- Initialize an S3 client using boto3\n- Call list_objects_v2 AWS API with the specified bucket name\n- Extract and return the 'Contents' from the response, defaulting to an empty list if not present\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- AWS credentials: Properly configured access credentials in the environment\n\n## USAGE CONTEXT:\n- Typically used in data processing pipelines, file management systems, or backup tools that need to inventory S3 bucket contents\n- Often called before performing operations on objects in a bucket\n\n## EDGE CASES:\n- Returns an empty list if the bucket is empty or doesn't exist\n- May raise exceptions if AWS credentials are invalid or there are permission issues\n- Does not handle pagination for buckets with more than 1000 objects\n\n## RELATIONSHIPS:\n- Likely part of a larger S3 utilities class or module\n- Often used in conjunction with other S3 operations like download_s3_object, upload_s3_object, or delete_s3_object\n\n    def list_s3_objects(self, bucket_name):\n        s3 = boto3.client(\"s3\")\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        return response.get('Contents', [])",
    "metadata": {
      "type": "FUNCTION",
      "name": "list_s3_objects",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "    def list_s3_objects(self, bucket_name):\n        s3 = boto3.client(\"s3\")\n        response = s3.list_objects_v2(Bucket=bucket_name)\n        return response.get('Contents', [])",
      "summary": "# FUNCTION: list_s3_objects\n\n## PURPOSE:\nLists objects stored in a specified Amazon S3 bucket by retrieving metadata about the objects. This function provides an interface to access S3 bucket contents.\n\n## INPUTS:\n- `self`: Instance of the containing class\n- `bucket_name` (string): Name of the S3 bucket to list objects from\n\n## OUTPUTS:\n- List of dictionaries: Each dictionary contains metadata about an object in the bucket (e.g., Key, LastModified, ETag, Size). Returns an empty list if the bucket is empty or doesn't exist.\n\n## KEY STEPS:\n- Initialize an S3 client using boto3\n- Call list_objects_v2 AWS API with the specified bucket name\n- Extract and return the 'Contents' from the response, defaulting to an empty list if not present\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- AWS credentials: Properly configured access credentials in the environment\n\n## USAGE CONTEXT:\n- Typically used in data processing pipelines, file management systems, or backup tools that need to inventory S3 bucket contents\n- Often called before performing operations on objects in a bucket\n\n## EDGE CASES:\n- Returns an empty list if the bucket is empty or doesn't exist\n- May raise exceptions if AWS credentials are invalid or there are permission issues\n- Does not handle pagination for buckets with more than 1000 objects\n\n## RELATIONSHIPS:\n- Likely part of a larger S3 utilities class or module\n- Often used in conjunction with other S3 operations like download_s3_object, upload_s3_object, or delete_s3_object"
    }
  },
  {
    "page_content": "# FUNCTION: extract_uuid\n\n## PURPOSE:\nExtracts a UUID from a file name by removing a specific prefix and suffix. Designed to isolate the unique identifier from a consistently formatted file name.\n\n## INPUTS:\n- `self`: The instance of the class this method belongs to\n- `file_name` (str): The file name containing a UUID with a specific format\n\n## OUTPUTS:\n- (str): The extracted UUID from the file name\n\n## KEY STEPS:\n- Slice the input string to remove the first 7 characters (likely a prefix like \"upload_\")\n- Slice the result to remove the last 4 characters (likely a file extension like \".jpg\")\n- Return the extracted substring which represents the UUID\n\n## DEPENDENCIES:\n- None - uses only basic Python string operations\n\n## USAGE CONTEXT:\n- Used in file handling operations where unique identifiers need to be extracted from file names\n- Likely used in conjunction with file upload, processing, or storage functionality\n\n## EDGE CASES:\n- Will raise an IndexError if the file name is too short (less than 12 characters)\n- Assumes a consistent file naming format; results will be incorrect if the file name doesn't follow the expected pattern\n\n## RELATIONSHIPS:\n- Likely part of a larger file management or processing system\n- May be used by methods that need to track or reference files by their UUID\n\n    def extract_uuid(self, file_name):\n        return file_name[7:-4]",
    "metadata": {
      "type": "FUNCTION",
      "name": "extract_uuid",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "    def extract_uuid(self, file_name):\n        return file_name[7:-4]",
      "summary": "# FUNCTION: extract_uuid\n\n## PURPOSE:\nExtracts a UUID from a file name by removing a specific prefix and suffix. Designed to isolate the unique identifier from a consistently formatted file name.\n\n## INPUTS:\n- `self`: The instance of the class this method belongs to\n- `file_name` (str): The file name containing a UUID with a specific format\n\n## OUTPUTS:\n- (str): The extracted UUID from the file name\n\n## KEY STEPS:\n- Slice the input string to remove the first 7 characters (likely a prefix like \"upload_\")\n- Slice the result to remove the last 4 characters (likely a file extension like \".jpg\")\n- Return the extracted substring which represents the UUID\n\n## DEPENDENCIES:\n- None - uses only basic Python string operations\n\n## USAGE CONTEXT:\n- Used in file handling operations where unique identifiers need to be extracted from file names\n- Likely used in conjunction with file upload, processing, or storage functionality\n\n## EDGE CASES:\n- Will raise an IndexError if the file name is too short (less than 12 characters)\n- Assumes a consistent file naming format; results will be incorrect if the file name doesn't follow the expected pattern\n\n## RELATIONSHIPS:\n- Likely part of a larger file management or processing system\n- May be used by methods that need to track or reference files by their UUID"
    }
  },
  {
    "page_content": "# FUNCTION: delete_sqs_message\n\n## PURPOSE:\nDeletes a specific message from an Amazon SQS queue using its receipt handle. This function removes the message permanently after it has been processed.\n\n## INPUTS:\n- queue_url (string): The URL of the Amazon SQS queue from which to delete the message\n- receipt_handle (string): The receipt handle associated with the message to delete\n\n## OUTPUTS:\n- None: The function does not return any value\n\n## KEY STEPS:\n- Access the SQS client from the clients collection (first client)\n- Call the SQS client's delete_message method with the queue URL and receipt handle\n\n## DEPENDENCIES:\n- AWS SDK (boto3) SQS client\n- clients collection containing the SQS client at index 0\n\n## USAGE CONTEXT:\nTypically used after successfully processing a message from an SQS queue to prevent the message from being processed again when its visibility timeout expires.\n\n## EDGE CASES:\n- If the receipt handle is invalid or expired, the message won't be deleted\n- The function does not handle exceptions from the AWS SDK, so errors must be handled by the caller\n\n## RELATIONSHIPS:\n- Likely part of a larger SQS message processing workflow\n- Would be used after receiving messages from SQS and completing their processing\n- Probably works with companion functions for receiving and processing SQS messages\n\n    def delete_sqs_message(self, queue_url, receipt_handle):\n        sqs = self.clients[0]\n        sqs.delete_message(QueueUrl=queue_url, ReceiptHandle=receipt_handle)",
    "metadata": {
      "type": "FUNCTION",
      "name": "delete_sqs_message",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "    def delete_sqs_message(self, queue_url, receipt_handle):\n        sqs = self.clients[0]\n        sqs.delete_message(QueueUrl=queue_url, ReceiptHandle=receipt_handle)",
      "summary": "# FUNCTION: delete_sqs_message\n\n## PURPOSE:\nDeletes a specific message from an Amazon SQS queue using its receipt handle. This function removes the message permanently after it has been processed.\n\n## INPUTS:\n- queue_url (string): The URL of the Amazon SQS queue from which to delete the message\n- receipt_handle (string): The receipt handle associated with the message to delete\n\n## OUTPUTS:\n- None: The function does not return any value\n\n## KEY STEPS:\n- Access the SQS client from the clients collection (first client)\n- Call the SQS client's delete_message method with the queue URL and receipt handle\n\n## DEPENDENCIES:\n- AWS SDK (boto3) SQS client\n- clients collection containing the SQS client at index 0\n\n## USAGE CONTEXT:\nTypically used after successfully processing a message from an SQS queue to prevent the message from being processed again when its visibility timeout expires.\n\n## EDGE CASES:\n- If the receipt handle is invalid or expired, the message won't be deleted\n- The function does not handle exceptions from the AWS SDK, so errors must be handled by the caller\n\n## RELATIONSHIPS:\n- Likely part of a larger SQS message processing workflow\n- Would be used after receiving messages from SQS and completing their processing\n- Probably works with companion functions for receiving and processing SQS messages"
    }
  },
  {
    "page_content": "# FUNCTION: is_expected_image_present\n\n## PURPOSE:\nChecks if a specific image name exists within a list of object keys. This function is used to verify if an expected image is present in a collection of objects.\n\n## INPUTS:\n- `objects` (list): A list of dictionary objects, where each dictionary has a 'Key' field containing a string path or filename\n- `expected_image` (string): The image name or path substring to search for in the object keys\n\n## OUTPUTS:\n- `bool`: Returns True if the expected image is found in any object's Key, False otherwise\n\n## KEY STEPS:\n- Iterate through each object in the provided objects list\n- Check if the expected_image string is a substring of the current object's Key value\n- Return True immediately if a match is found\n- Return False if no match is found after checking all objects\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\nTypically used in file or object storage systems (like S3) to verify if a specific image exists in a collection of retrieved objects before processing or displaying them.\n\n## EDGE CASES:\n- Returns False if the objects list is empty\n- Case-sensitive matching (will not match if case differs between expected_image and object Key)\n- Will find partial matches (if expected_image is a substring of any Key)\n\n## RELATIONSHIPS:\nLikely part of a larger image handling or verification system, used to validate expected images exist before further processing them.\n\n    def is_expected_image_present(self, objects, expected_image):\n        for object in objects:\n            if expected_image in object['Key']:\n                return True\n        return False",
    "metadata": {
      "type": "FUNCTION",
      "name": "is_expected_image_present",
      "path": "../mathsearch/lambda-container/dataHandler.py",
      "code": "    def is_expected_image_present(self, objects, expected_image):\n        for object in objects:\n            if expected_image in object['Key']:\n                return True\n        return False",
      "summary": "# FUNCTION: is_expected_image_present\n\n## PURPOSE:\nChecks if a specific image name exists within a list of object keys. This function is used to verify if an expected image is present in a collection of objects.\n\n## INPUTS:\n- `objects` (list): A list of dictionary objects, where each dictionary has a 'Key' field containing a string path or filename\n- `expected_image` (string): The image name or path substring to search for in the object keys\n\n## OUTPUTS:\n- `bool`: Returns True if the expected image is found in any object's Key, False otherwise\n\n## KEY STEPS:\n- Iterate through each object in the provided objects list\n- Check if the expected_image string is a substring of the current object's Key value\n- Return True immediately if a match is found\n- Return False if no match is found after checking all objects\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\nTypically used in file or object storage systems (like S3) to verify if a specific image exists in a collection of retrieved objects before processing or displaying them.\n\n## EDGE CASES:\n- Returns False if the objects list is empty\n- Case-sensitive matching (will not match if case differs between expected_image and object Key)\n- Will find partial matches (if expected_image is a substring of any Key)\n\n## RELATIONSHIPS:\nLikely part of a larger image handling or verification system, used to validate expected images exist before further processing them."
    }
  },
  {
    "page_content": "# DIRECTORY: archive\n\n## PURPOSE:\nThis directory contains a mathematical equation search system that processes query equation images and PDF documents to find and annotate similar equations using tree-based similarity algorithms and LaTeX conversion.\n\n## COMPONENT STRUCTURE:\n- **old-lf.py**: The primary file implementing the equation search system, which includes the Lambda handler, equation detection, LaTeX conversion, tree similarity calculations, and output generation functions.\n\n## ARCHITECTURE:\nThe system follows a serverless pipeline architecture with specialized stages for file processing, equation detection, conversion to LaTeX, similarity computation using tree structures, and result visualization with annotated PDFs. It leverages external services for image-to-LaTeX conversion and mathematical expression parsing.\n\n## ENTRY POINTS:\n- **lambda_handler** in old-lf.py: The main AWS Lambda function that orchestrates the entire equation search process\n- **downloadDirectoryFroms3**: Initial function to retrieve input files from S3 storage\n\n## DATA FLOW:\nThe data flows from S3 storage through PDF-to-image conversion, equation detection with YOLO, image-to-LaTeX conversion via Mathpix, transformation to tree structures, similarity comparison using custom edit distance algorithms, and finally to annotated output PDFs that are uploaded back to S3.\n\n## INTEGRATION:\nThe system integrates with multiple AWS services (S3, SQS, SageMaker), external APIs (Mathpix for LaTeX conversion), and libraries (SymPy for expression parsing, PyPDF2 and pdf2image for document handling, PIL and OpenCV for image processing) to form a complete equation search pipeline.\n\n## DEVELOPMENT PATTERNS:\nThe code follows a functional transformation pattern where data moves through discrete processing stages with clear input/output interfaces. It employs asynchronous processing via AWS Lambda, specialized algorithms for tree-based similarity, and visualization techniques for highlighting matched equations.\n\n## RELATIONSHIPS:\nThe functions form a sequential processing pipeline where file handling functions feed into conversion utilities, which supply data to the similarity algorithms, and finally to result generation functions. Key relationships include the transformation from images to LaTeX to tree structures, and the subsequent similarity calculations that determine the final matches.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "archive",
      "path": "../mathsearch/lambda-container/archive",
      "code": "",
      "summary": "# DIRECTORY: archive\n\n## PURPOSE:\nThis directory contains a mathematical equation search system that processes query equation images and PDF documents to find and annotate similar equations using tree-based similarity algorithms and LaTeX conversion.\n\n## COMPONENT STRUCTURE:\n- **old-lf.py**: The primary file implementing the equation search system, which includes the Lambda handler, equation detection, LaTeX conversion, tree similarity calculations, and output generation functions.\n\n## ARCHITECTURE:\nThe system follows a serverless pipeline architecture with specialized stages for file processing, equation detection, conversion to LaTeX, similarity computation using tree structures, and result visualization with annotated PDFs. It leverages external services for image-to-LaTeX conversion and mathematical expression parsing.\n\n## ENTRY POINTS:\n- **lambda_handler** in old-lf.py: The main AWS Lambda function that orchestrates the entire equation search process\n- **downloadDirectoryFroms3**: Initial function to retrieve input files from S3 storage\n\n## DATA FLOW:\nThe data flows from S3 storage through PDF-to-image conversion, equation detection with YOLO, image-to-LaTeX conversion via Mathpix, transformation to tree structures, similarity comparison using custom edit distance algorithms, and finally to annotated output PDFs that are uploaded back to S3.\n\n## INTEGRATION:\nThe system integrates with multiple AWS services (S3, SQS, SageMaker), external APIs (Mathpix for LaTeX conversion), and libraries (SymPy for expression parsing, PyPDF2 and pdf2image for document handling, PIL and OpenCV for image processing) to form a complete equation search pipeline.\n\n## DEVELOPMENT PATTERNS:\nThe code follows a functional transformation pattern where data moves through discrete processing stages with clear input/output interfaces. It employs asynchronous processing via AWS Lambda, specialized algorithms for tree-based similarity, and visualization techniques for highlighting matched equations.\n\n## RELATIONSHIPS:\nThe functions form a sequential processing pipeline where file handling functions feed into conversion utilities, which supply data to the similarity algorithms, and finally to result generation functions. Key relationships include the transformation from images to LaTeX to tree structures, and the subsequent similarity calculations that determine the final matches."
    }
  },
  {
    "page_content": "# FILE: old-lf.py\n\n## OVERVIEW:\nThis file implements a mathematical equation search system that compares a query equation image with equations in PDF documents, converting images to LaTeX, applying tree-based similarity algorithms, and generating annotated results.\n\n## KEY COMPONENTS:\n- `lambda_handler`: Main AWS Lambda function that orchestrates the equation search process\n- `parse_tree_similarity`: Compares a query equation image with extracted document equations using tree similarity\n- `preprocess_latex`: Removes specified LaTeX formatting elements while preserving content\n- `sympy_to_zss`: Converts SymPy expressions to Zhang-Shasha tree structures for comparison\n- `source_to_zss`: Converts LaTeX expressions to tree representations\n- `custom_edit_distance`: Calculates tree edit distance with specialized cost functions\n- `mathpix_imgpath_to_latex`: Converts mathematical images to LaTeX using the Mathpix API\n- `downloadDirectoryFroms3`: Downloads an entire directory from an S3 bucket\n- `download_files`: Downloads PDF and query image files and converts PDFs to PNG images\n- `draw_bounding_box`: Draws bounding boxes on images to visualize detected regions\n- `final_output`: Creates a PDF with annotated bounding boxes around matched equations\n\n## ARCHITECTURE:\nThe system follows a pipeline architecture where:\n1. Input files are received via S3 and processed by the lambda handler\n2. Documents are converted to images and analyzed using YOLO object detection\n3. Detected equations are converted to LaTeX and compared with the query\n4. Similarity is calculated using tree-based algorithms\n5. Results are visualized with bounding boxes and returned to the user\n\n## DATA FLOW:\n1. PDF files and query images enter the system from S3\n2. PDFs are converted to PNG images for processing\n3. YOLO detects equations in the images\n4. Images are converted to LaTeX strings using Mathpix\n5. LaTeX is transformed into tree structures via SymPy\n6. Edit distances between trees determine similarity scores\n7. Top matches are identified and annotated in a new PDF\n8. Final results are uploaded back to S3\n\n## INTEGRATION POINTS:\n- AWS S3: For storing input PDFs, query images, and output results\n- AWS SQS: For message-based triggering of the Lambda function\n- AWS SageMaker: For YOLO model inference to detect equations\n- Mathpix API: For converting equation images to LaTeX\n- SymPy: For mathematical expression parsing and representation\n\n## USAGE PATTERNS:\n- User uploads a PDF document and a query equation image to S3\n- The system processes the request asynchronously via SQS\n- YOLO model identifies potential equations in the document\n- Tree-based similarity algorithms find matches to the query\n- System returns a new PDF with highlighted matching equations\n- Results are stored in S3 for user access\n\n## DEPENDENCIES:\n- AWS SDK (boto3): For S3, SQS, and SageMaker integration\n- PIL/Pillow: For image processing\n- PyPDF2: For PDF manipulation\n- pdf2image: For converting PDFs to images\n- requests: For Mathpix API calls\n- SymPy: For mathematical expression parsing\n- OpenCV (cv2): For image processing\n- Custom ZSS implementation: For tree edit distance calculation\n\n## RELATIONSHIPS:\nThe functions form a cohesive pipeline where:\n- `lambda_handler` coordinates the overall process\n- `download_files` and `downloadDirectoryFroms3` handle data acquisition\n- `mathpix_imgpath_to_latex` converts images to structured text\n- `source_to_zss` and `sympy_to_zss` transform text to comparable structures\n- `custom_edit_distance` measures similarity between expressions\n- `parse_tree_similarity` ranks matches based on similarity scores\n- `draw_bounding_box` and `final_output` generate visual results for users",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "old-lf.py",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "from constants import *\nimport os\nimport boto3\nimport json\nimport dataHandler\nimport subprocess\nimport os\nimport sympy as sp\nfrom sympy.parsing.latex import parse_latex\nfrom zss import Node, distance\nimport PyPDF2\nfrom PIL import Image, ImageDraw\nimport pdf2image\nimport cv2\n#from sagemaker.pytorch import PyTorchPredictor\n#from sagemaker.deserializers import JSONDeserializer\nimport traceback\nimport requests\nimport io\nimport numpy as np\nimport time\n\nprint(\"Finished imports\")\n\n# Initialize S3 client\ns3 = boto3.client('s3')\n\ndef preprocess_latex(latex_src, rem):\n  \"\"\"\n  latex_src: string of LaTeX source code to pre-process\n  rem: string of formatting element which we want to remove from latex_src. includes opening curly brace. ex. \\mathrm{\n  \"\"\"\n  final_string = latex_src\n  format_index = latex_src.find(rem)\n  while format_index != -1:\n    # iterate through string until you find the right closing curly brace to remove\n    index = format_index + len(rem)\n    closing_brace = -1\n    num_opening = 0\n    while index < len(final_string):\n      if final_string[index:index+1] == \"{\":\n        num_opening += 1\n      elif final_string[index:index+1] == \"}\":\n        if num_opening == 0:\n          closing_brace = index\n          break\n        else:\n          num_opening -= 1\n      index += 1\n\n    # entering this if statement means something went wrong.\n    # nothing is removed in this case\n    if closing_brace == -1:\n      return final_string\n\n    final_string = final_string[:format_index]+final_string[format_index+len(rem):closing_brace]+final_string[closing_brace+1:]\n    format_index = final_string.find(rem)\n  \n  return final_string\n\nprint(\"Finished preprocess_latex\")\n\n# Parses sympy expression into Zss tree\ndef sympy_to_zss(expr):\n    if isinstance(expr, sp.Symbol) or isinstance(expr, sp.Number):\n        return Node(str(expr))\n    else:\n        full_class_str = str(expr.func)\n        class_name = full_class_str.split('.')[-1].rstrip(\"'>\")\n        node = Node(class_name)\n        for arg in expr.args:\n            child_node = sympy_to_zss(arg)\n            node.addkid(child_node)\n    return node\n\nprint(\"Finished sympy_to_zss\")\n\n# Input is string of LaTeX source code. Runs sympy parser and ZSS tree parser.\n# Returns parsed ZSS tree.\ndef source_to_zss(latex_expr):\n    try:\n        sympy_expr = parse_latex(latex_expr)\n        zss_tree = sympy_to_zss(sympy_expr)\n        return zss_tree\n    except:\n        return Node(\"ERROR\")\n\nprint(\"Finished source_to_zss\")\n        \n# used in ZSS tree edit distance\ndef custom_edit_distance(query_tree, other_tree):\n    return distance(query_tree, other_tree, get_children=Node.get_children,\n        insert_cost=lambda node: 10, remove_cost=lambda node: 10, update_cost=lambda a, b: 1)\n\nprint(\"Finished custom_edit_distance\")\n\n# Returns a well-formatted LaTeX string represent the equation image 'image'\n# Makes the MathPix API call\ndef mathpix_imgpath_to_latex(image_path) :\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n\n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": open(image_path,\"rb\")}  ,\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200 :\n        print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"\n\nprint(\"Finished image_to_latex_convert\")\n\ndef downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path\n\nprint(\"Finished downloadDirectoryFroms3\")\n\ndef download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi = 500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target\n\nprint(\"Finished download_files\")\n\n# Call draw_bounding_box on each PNG page of PDF\ndef draw_bounding_box(image_path_in, bounding_boxes, image_path_out):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  SKYBLUE = (55,161,253)\n\n  # create rectangle for each bounding box on this page\n  for bb in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    draw.rectangle(xy=(x1, y1, x2, y2), outline=SKYBLUE, width=6)\n  \n  # save img as pdf\n  image.save(image_path_out[:-4]+\".pdf\")\n\nprint(\"Finished draw_bounding_box\")\n\ndef final_output(pdf_name, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values list of bounding boxes \n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  print(result_pages)\n  # call draw_bounding_boxes for each png page, save to IMG_OUT_DIR\n  for i in result_pages:\n    image_path_in = IMG_IN_DIR + str(i) + \".png\"\n    image_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n    # pass in list of bounding boxes for each page\n    draw_bounding_box(image_path_in, bounding_boxes[i], image_path_out)\n    #s3.upload_file(image_path_out[:-4]+\".pdf\", OUTPUT_BUCKET, str(i) + \".pdf\")\n  print(\"drew bounding boxes!\")\n\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  with open(pdf_in, 'rb') as file:\n    with open(pdf_out, 'wb') as pdf_out_file:\n      pdf = PyPDF2.PdfReader(file)\n      output = PyPDF2.PdfWriter()\n      for i, page in enumerate(pdf.pages):\n        if str(i) in result_pages:\n          new_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n          new_page.scale_by(0.36)\n          output.add_page(new_page)\n        else:\n          output.add_page(page)\n      output.write(pdf_out_file)\n    \n    try:\n      s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n      print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n    except:\n      raise Exception(\"Upload failed\")\n\nprint(\"Finished final_output\")\n\n\n# Store string repr. of LaTeX equation and its page number in list\ndef parse_tree_similarity(yolo_result, query_path):\n  # list containing all formatting elements we want to remove\n\n  query_text = mathpix_imgpath_to_latex(query_path)\n  print(f\"query_text: {query_text}\")\n\n  # ADD code to pre-process query string (from ML subteam)\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    for img_array in dict_elem[\"cropped_ims\"]:\n      try:\n        image = Image.fromarray(np.array(img_array, dtype=np.uint8))\n        byte_stream = io.BytesIO()\n        image.save(byte_stream, format='PNG')  # Save the image to a byte stream\n        byte_stream.seek(0)\n        byte_stream.save('temp_premathpix_img')\n        latex_string = mathpix_imgpath_to_latex('temp_premathpix_img') # query_bool was previously false\n        os.remove('temp_premathpix_img')\n        print(f\"{eqn_num} on {page_num}: {latex_string}\")\n        equations_list.append((latex_string, page_num, eqn_num))\n      except Exception as e:\n        print(f\"Failed to process image or convert to LaTeX: {e}\")\n      eqn_num += 1\n\n  # equations_list = []\n  # for dict_elem, page_num in yolo_result:\n  #   eqn_num = 1\n  #   for img_elem in dict_elem[\"cropped_ims\"]:\n  #     print(f\"img_elem {img_elem}\")\n  #     print(f\"type of img_elem {type(img_elem)}\")\n\n  #     #byte_elem = np.array(byte_elem).tobytes()\n  #     byte_elem = bytes(img_elem)\n  #     print(f\"type of byte_elem {type(byte_elem)}\")\n  #     #print(img_elem)\n  #     latex_string = image_to_latex_convert(byte_elem, query_bool=False)\n  #     print(f\"{eqn_num} on {page_num}: {latex_string}\")\n\n  #     # ENTER CODE FROM ML SUB-TEAM to edit the latex string\n  #     # editing the escape_chars and preprocess_latex function\n  #     equations_list.append((latex_string, page_num, eqn_num))\n  #     eqn_num += 1 # increment equation num\n    \n  print(\"Finished all MathPix API calls!\")\n  \n  # create ZSS tree of query  \n  zss_query = source_to_zss(query_text)\n  \n  # now parse all LaTeX source code into ZSS tree and compute edit distance with query for every equation\n  # each element in tree_dist is (latex_string, edit_dist_from_query, page_num, eqn_num)\n  tree_dists = []\n  for eqn, page_num, eqn_num in equations_list:\n    zss_tree = source_to_zss(eqn)\n    dist = custom_edit_distance(zss_query, zss_tree)\n    tree_dists.append((eqn, dist, page_num, eqn_num))\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with (top_n-1) smallest edit distances\n  top_n = 6 \n  sorted(tree_dists, key=lambda x: x[1])\n  return tree_dists[:top_n]\n\nprint(\"Finished parse_tree_similarity\")\n\ndef lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = parse_tree_similarity(yolo_result=yolo_result, query_path=local_target)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted([page_num for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns])\n          top_5_eqns_info = [(page_num, eqn_num) for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns]\n\n          # get bboxes for top5 equations\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top_5_eqns_info:\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append(bboxes[:4])\n                else:\n                  bboxes_dict[page_num] = [bboxes[:4]]\n              count += 1\n            \n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = sorted(page_nums_5)\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, bboxes_dict)\n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
      "summary": "# FILE: old-lf.py\n\n## OVERVIEW:\nThis file implements a mathematical equation search system that compares a query equation image with equations in PDF documents, converting images to LaTeX, applying tree-based similarity algorithms, and generating annotated results.\n\n## KEY COMPONENTS:\n- `lambda_handler`: Main AWS Lambda function that orchestrates the equation search process\n- `parse_tree_similarity`: Compares a query equation image with extracted document equations using tree similarity\n- `preprocess_latex`: Removes specified LaTeX formatting elements while preserving content\n- `sympy_to_zss`: Converts SymPy expressions to Zhang-Shasha tree structures for comparison\n- `source_to_zss`: Converts LaTeX expressions to tree representations\n- `custom_edit_distance`: Calculates tree edit distance with specialized cost functions\n- `mathpix_imgpath_to_latex`: Converts mathematical images to LaTeX using the Mathpix API\n- `downloadDirectoryFroms3`: Downloads an entire directory from an S3 bucket\n- `download_files`: Downloads PDF and query image files and converts PDFs to PNG images\n- `draw_bounding_box`: Draws bounding boxes on images to visualize detected regions\n- `final_output`: Creates a PDF with annotated bounding boxes around matched equations\n\n## ARCHITECTURE:\nThe system follows a pipeline architecture where:\n1. Input files are received via S3 and processed by the lambda handler\n2. Documents are converted to images and analyzed using YOLO object detection\n3. Detected equations are converted to LaTeX and compared with the query\n4. Similarity is calculated using tree-based algorithms\n5. Results are visualized with bounding boxes and returned to the user\n\n## DATA FLOW:\n1. PDF files and query images enter the system from S3\n2. PDFs are converted to PNG images for processing\n3. YOLO detects equations in the images\n4. Images are converted to LaTeX strings using Mathpix\n5. LaTeX is transformed into tree structures via SymPy\n6. Edit distances between trees determine similarity scores\n7. Top matches are identified and annotated in a new PDF\n8. Final results are uploaded back to S3\n\n## INTEGRATION POINTS:\n- AWS S3: For storing input PDFs, query images, and output results\n- AWS SQS: For message-based triggering of the Lambda function\n- AWS SageMaker: For YOLO model inference to detect equations\n- Mathpix API: For converting equation images to LaTeX\n- SymPy: For mathematical expression parsing and representation\n\n## USAGE PATTERNS:\n- User uploads a PDF document and a query equation image to S3\n- The system processes the request asynchronously via SQS\n- YOLO model identifies potential equations in the document\n- Tree-based similarity algorithms find matches to the query\n- System returns a new PDF with highlighted matching equations\n- Results are stored in S3 for user access\n\n## DEPENDENCIES:\n- AWS SDK (boto3): For S3, SQS, and SageMaker integration\n- PIL/Pillow: For image processing\n- PyPDF2: For PDF manipulation\n- pdf2image: For converting PDFs to images\n- requests: For Mathpix API calls\n- SymPy: For mathematical expression parsing\n- OpenCV (cv2): For image processing\n- Custom ZSS implementation: For tree edit distance calculation\n\n## RELATIONSHIPS:\nThe functions form a cohesive pipeline where:\n- `lambda_handler` coordinates the overall process\n- `download_files` and `downloadDirectoryFroms3` handle data acquisition\n- `mathpix_imgpath_to_latex` converts images to structured text\n- `source_to_zss` and `sympy_to_zss` transform text to comparable structures\n- `custom_edit_distance` measures similarity between expressions\n- `parse_tree_similarity` ranks matches based on similarity scores\n- `draw_bounding_box` and `final_output` generate visual results for users"
    }
  },
  {
    "page_content": "# FUNCTION: preprocess_latex\n\n## PURPOSE:\nRemoves specified LaTeX formatting elements from LaTeX source code by identifying and stripping out the formatting command and its enclosing braces while preserving the content inside.\n\n## INPUTS:\n- `latex_src` (string): LaTeX source code to be processed\n- `rem` (string): LaTeX formatting element to remove, including the opening curly brace (e.g., \"\\mathrm{\")\n\n## OUTPUTS:\n- String: Modified LaTeX source with all instances of the specified formatting element removed\n\n## KEY STEPS:\n- Find the first occurrence of the formatting element in the input string\n- For each occurrence:\n  - Locate the corresponding closing brace, accounting for nested braces\n  - Remove the formatting element and its braces, preserving the content inside\n  - Continue searching for more occurrences until none remain\n\n## DEPENDENCIES:\n- None (uses only Python standard string operations)\n\n## USAGE CONTEXT:\n- Used in LaTeX processing pipelines to simplify or normalize LaTeX code\n- Commonly used when preparing LaTeX for parsing, rendering, or other transformations\n\n## EDGE CASES:\n- If a matching closing brace cannot be found, the function returns the string unchanged\n- Properly handles nested braces inside the formatting command\n- Does not remove anything if the specified formatting element is not found\n\n## RELATIONSHIPS:\n- Likely part of a larger LaTeX processing system\n- Would typically be called before more complex parsing or rendering operations\n\ndef preprocess_latex(latex_src, rem):\n  \"\"\"\n  latex_src: string of LaTeX source code to pre-process\n  rem: string of formatting element which we want to remove from latex_src. includes opening curly brace. ex. \\mathrm{\n  \"\"\"\n  final_string = latex_src\n  format_index = latex_src.find(rem)\n  while format_index != -1:\n    # iterate through string until you find the right closing curly brace to remove\n    index = format_index + len(rem)\n    closing_brace = -1\n    num_opening = 0\n    while index < len(final_string):\n      if final_string[index:index+1] == \"{\":\n        num_opening += 1\n      elif final_string[index:index+1] == \"}\":\n        if num_opening == 0:\n          closing_brace = index\n          break\n        else:\n          num_opening -= 1\n      index += 1\n\n    # entering this if statement means something went wrong.\n    # nothing is removed in this case\n    if closing_brace == -1:\n      return final_string\n\n    final_string = final_string[:format_index]+final_string[format_index+len(rem):closing_brace]+final_string[closing_brace+1:]\n    format_index = final_string.find(rem)\n  \n  return final_string",
    "metadata": {
      "type": "FUNCTION",
      "name": "preprocess_latex",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def preprocess_latex(latex_src, rem):\n  \"\"\"\n  latex_src: string of LaTeX source code to pre-process\n  rem: string of formatting element which we want to remove from latex_src. includes opening curly brace. ex. \\mathrm{\n  \"\"\"\n  final_string = latex_src\n  format_index = latex_src.find(rem)\n  while format_index != -1:\n    # iterate through string until you find the right closing curly brace to remove\n    index = format_index + len(rem)\n    closing_brace = -1\n    num_opening = 0\n    while index < len(final_string):\n      if final_string[index:index+1] == \"{\":\n        num_opening += 1\n      elif final_string[index:index+1] == \"}\":\n        if num_opening == 0:\n          closing_brace = index\n          break\n        else:\n          num_opening -= 1\n      index += 1\n\n    # entering this if statement means something went wrong.\n    # nothing is removed in this case\n    if closing_brace == -1:\n      return final_string\n\n    final_string = final_string[:format_index]+final_string[format_index+len(rem):closing_brace]+final_string[closing_brace+1:]\n    format_index = final_string.find(rem)\n  \n  return final_string",
      "summary": "# FUNCTION: preprocess_latex\n\n## PURPOSE:\nRemoves specified LaTeX formatting elements from LaTeX source code by identifying and stripping out the formatting command and its enclosing braces while preserving the content inside.\n\n## INPUTS:\n- `latex_src` (string): LaTeX source code to be processed\n- `rem` (string): LaTeX formatting element to remove, including the opening curly brace (e.g., \"\\mathrm{\")\n\n## OUTPUTS:\n- String: Modified LaTeX source with all instances of the specified formatting element removed\n\n## KEY STEPS:\n- Find the first occurrence of the formatting element in the input string\n- For each occurrence:\n  - Locate the corresponding closing brace, accounting for nested braces\n  - Remove the formatting element and its braces, preserving the content inside\n  - Continue searching for more occurrences until none remain\n\n## DEPENDENCIES:\n- None (uses only Python standard string operations)\n\n## USAGE CONTEXT:\n- Used in LaTeX processing pipelines to simplify or normalize LaTeX code\n- Commonly used when preparing LaTeX for parsing, rendering, or other transformations\n\n## EDGE CASES:\n- If a matching closing brace cannot be found, the function returns the string unchanged\n- Properly handles nested braces inside the formatting command\n- Does not remove anything if the specified formatting element is not found\n\n## RELATIONSHIPS:\n- Likely part of a larger LaTeX processing system\n- Would typically be called before more complex parsing or rendering operations"
    }
  },
  {
    "page_content": "# FUNCTION: sympy_to_zss\n\n## PURPOSE:\nConverts a SymPy expression to a Zhang-Shasha (ZSS) tree structure, allowing SymPy expressions to be used with the Zhang-Shasha tree edit distance algorithm for comparing mathematical expressions.\n\n## INPUTS:\n- `expr` (SymPy expression): The SymPy expression to convert to a ZSS tree\n\n## OUTPUTS:\n- `Node` object: A ZSS tree node representing the input expression's structure\n\n## KEY STEPS:\n- Check if the expression is a base case (Symbol or Number)\n- If base case, return a leaf node with the expression's string representation\n- Otherwise, extract the function/operation name from the expression\n- Create a new node with this operation name\n- Recursively convert each argument of the expression to a ZSS node\n- Add each converted argument as a child of the current node\n\n## DEPENDENCIES:\n- SymPy (`sp`): For expression types and handling\n- Node class: From the ZSS library for tree representation\n\n## USAGE CONTEXT:\n- Used when comparing the structural similarity of mathematical expressions\n- Typically employed in computer algebra systems, symbolic math libraries, or educational tools that need to measure expression equivalence\n\n## EDGE CASES:\n- Handles base cases (Symbol, Number) differently from compound expressions\n- No explicit error handling for invalid SymPy expressions\n\n## RELATIONSHIPS:\n- Serves as a converter between SymPy's expression system and ZSS's tree structure\n- Likely part of a larger system for expression comparison or manipulation\n- May be paired with a reverse function (zss_to_sympy) in a bidirectional conversion system\n\ndef sympy_to_zss(expr):\n    if isinstance(expr, sp.Symbol) or isinstance(expr, sp.Number):\n        return Node(str(expr))\n    else:\n        full_class_str = str(expr.func)\n        class_name = full_class_str.split('.')[-1].rstrip(\"'>\")\n        node = Node(class_name)\n        for arg in expr.args:\n            child_node = sympy_to_zss(arg)\n            node.addkid(child_node)\n    return node",
    "metadata": {
      "type": "FUNCTION",
      "name": "sympy_to_zss",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def sympy_to_zss(expr):\n    if isinstance(expr, sp.Symbol) or isinstance(expr, sp.Number):\n        return Node(str(expr))\n    else:\n        full_class_str = str(expr.func)\n        class_name = full_class_str.split('.')[-1].rstrip(\"'>\")\n        node = Node(class_name)\n        for arg in expr.args:\n            child_node = sympy_to_zss(arg)\n            node.addkid(child_node)\n    return node",
      "summary": "# FUNCTION: sympy_to_zss\n\n## PURPOSE:\nConverts a SymPy expression to a Zhang-Shasha (ZSS) tree structure, allowing SymPy expressions to be used with the Zhang-Shasha tree edit distance algorithm for comparing mathematical expressions.\n\n## INPUTS:\n- `expr` (SymPy expression): The SymPy expression to convert to a ZSS tree\n\n## OUTPUTS:\n- `Node` object: A ZSS tree node representing the input expression's structure\n\n## KEY STEPS:\n- Check if the expression is a base case (Symbol or Number)\n- If base case, return a leaf node with the expression's string representation\n- Otherwise, extract the function/operation name from the expression\n- Create a new node with this operation name\n- Recursively convert each argument of the expression to a ZSS node\n- Add each converted argument as a child of the current node\n\n## DEPENDENCIES:\n- SymPy (`sp`): For expression types and handling\n- Node class: From the ZSS library for tree representation\n\n## USAGE CONTEXT:\n- Used when comparing the structural similarity of mathematical expressions\n- Typically employed in computer algebra systems, symbolic math libraries, or educational tools that need to measure expression equivalence\n\n## EDGE CASES:\n- Handles base cases (Symbol, Number) differently from compound expressions\n- No explicit error handling for invalid SymPy expressions\n\n## RELATIONSHIPS:\n- Serves as a converter between SymPy's expression system and ZSS's tree structure\n- Likely part of a larger system for expression comparison or manipulation\n- May be paired with a reverse function (zss_to_sympy) in a bidirectional conversion system"
    }
  },
  {
    "page_content": "# FUNCTION: source_to_zss\n\n## PURPOSE:\nConverts a LaTeX mathematical expression to a Zero-Suppressed binary Decision Diagram (ZDD) tree representation, providing an intermediate data structure for formula processing.\n\n## INPUTS:\n- `latex_expr` (string): A mathematical expression in LaTeX format\n\n## OUTPUTS:\n- `zss_tree` (Node): A tree representation of the expression in ZDD format, or a Node with value \"ERROR\" if conversion fails\n\n## KEY STEPS:\n- Parse the LaTeX expression into a SymPy expression using `parse_latex`\n- Convert the SymPy expression to a ZDD tree structure using `sympy_to_zss`\n- Return the resulting tree or an error node if any exception occurs\n\n## DEPENDENCIES:\n- `parse_latex`: Function to convert LaTeX to SymPy expressions\n- `sympy_to_zss`: Function to convert SymPy expressions to ZDD trees\n- `Node`: Class representing nodes in the tree structure\n\n## USAGE CONTEXT:\nUsed in systems that process mathematical expressions, particularly when transforming expressions between different representations for analysis, comparison, or manipulation.\n\n## EDGE CASES:\n- Returns a Node with value \"ERROR\" for any exception during parsing or conversion\n- May fail with complex LaTeX expressions that cannot be parsed by the SymPy parser\n\n## RELATIONSHIPS:\nActs as a bridge between LaTeX input and ZDD tree structures, likely part of a larger pipeline for mathematical expression processing or comparison.\n\ndef source_to_zss(latex_expr):\n    try:\n        sympy_expr = parse_latex(latex_expr)\n        zss_tree = sympy_to_zss(sympy_expr)\n        return zss_tree\n    except:\n        return Node(\"ERROR\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "source_to_zss",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def source_to_zss(latex_expr):\n    try:\n        sympy_expr = parse_latex(latex_expr)\n        zss_tree = sympy_to_zss(sympy_expr)\n        return zss_tree\n    except:\n        return Node(\"ERROR\")",
      "summary": "# FUNCTION: source_to_zss\n\n## PURPOSE:\nConverts a LaTeX mathematical expression to a Zero-Suppressed binary Decision Diagram (ZDD) tree representation, providing an intermediate data structure for formula processing.\n\n## INPUTS:\n- `latex_expr` (string): A mathematical expression in LaTeX format\n\n## OUTPUTS:\n- `zss_tree` (Node): A tree representation of the expression in ZDD format, or a Node with value \"ERROR\" if conversion fails\n\n## KEY STEPS:\n- Parse the LaTeX expression into a SymPy expression using `parse_latex`\n- Convert the SymPy expression to a ZDD tree structure using `sympy_to_zss`\n- Return the resulting tree or an error node if any exception occurs\n\n## DEPENDENCIES:\n- `parse_latex`: Function to convert LaTeX to SymPy expressions\n- `sympy_to_zss`: Function to convert SymPy expressions to ZDD trees\n- `Node`: Class representing nodes in the tree structure\n\n## USAGE CONTEXT:\nUsed in systems that process mathematical expressions, particularly when transforming expressions between different representations for analysis, comparison, or manipulation.\n\n## EDGE CASES:\n- Returns a Node with value \"ERROR\" for any exception during parsing or conversion\n- May fail with complex LaTeX expressions that cannot be parsed by the SymPy parser\n\n## RELATIONSHIPS:\nActs as a bridge between LaTeX input and ZDD tree structures, likely part of a larger pipeline for mathematical expression processing or comparison."
    }
  },
  {
    "page_content": "# FUNCTION: custom_edit_distance\n\n## PURPOSE:\nCalculates a custom tree edit distance between two trees, using specialized cost functions that heavily penalize structural changes (insertions and deletions) while minimizing the cost of node content updates.\n\n## INPUTS:\n- `query_tree` (Node): The source tree to compare from\n- `other_tree` (Node): The target tree to compare against\n\n## OUTPUTS:\n- (numeric): The edit distance score between the two trees, where higher values indicate greater dissimilarity\n\n## KEY STEPS:\n- Calls the `distance` function with the two tree structures\n- Uses the `Node.get_children` method to navigate tree structures\n- Applies a high fixed cost (10) for insertion and removal operations\n- Applies a low fixed cost (1) for node update operations\n\n## DEPENDENCIES:\n- `distance` function (external function for calculating tree edit distance)\n- `Node` class with `get_children` method\n\n## USAGE CONTEXT:\nTypically used in tree comparison scenarios where structure preservation is more important than node content, such as in code analysis, document structure comparison, or hierarchical data matching.\n\n## EDGE CASES:\n- Assumes both inputs are valid Node objects with properly implemented get_children methods\n- May produce unexpectedly high distances for trees with significant structural differences\n\n## RELATIONSHIPS:\nLikely part of a larger tree comparison or matching system, possibly used in conjunction with other similarity metrics or as part of a search or matching algorithm.\n\ndef custom_edit_distance(query_tree, other_tree):\n    return distance(query_tree, other_tree, get_children=Node.get_children,\n        insert_cost=lambda node: 10, remove_cost=lambda node: 10, update_cost=lambda a, b: 1)",
    "metadata": {
      "type": "FUNCTION",
      "name": "custom_edit_distance",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def custom_edit_distance(query_tree, other_tree):\n    return distance(query_tree, other_tree, get_children=Node.get_children,\n        insert_cost=lambda node: 10, remove_cost=lambda node: 10, update_cost=lambda a, b: 1)",
      "summary": "# FUNCTION: custom_edit_distance\n\n## PURPOSE:\nCalculates a custom tree edit distance between two trees, using specialized cost functions that heavily penalize structural changes (insertions and deletions) while minimizing the cost of node content updates.\n\n## INPUTS:\n- `query_tree` (Node): The source tree to compare from\n- `other_tree` (Node): The target tree to compare against\n\n## OUTPUTS:\n- (numeric): The edit distance score between the two trees, where higher values indicate greater dissimilarity\n\n## KEY STEPS:\n- Calls the `distance` function with the two tree structures\n- Uses the `Node.get_children` method to navigate tree structures\n- Applies a high fixed cost (10) for insertion and removal operations\n- Applies a low fixed cost (1) for node update operations\n\n## DEPENDENCIES:\n- `distance` function (external function for calculating tree edit distance)\n- `Node` class with `get_children` method\n\n## USAGE CONTEXT:\nTypically used in tree comparison scenarios where structure preservation is more important than node content, such as in code analysis, document structure comparison, or hierarchical data matching.\n\n## EDGE CASES:\n- Assumes both inputs are valid Node objects with properly implemented get_children methods\n- May produce unexpectedly high distances for trees with significant structural differences\n\n## RELATIONSHIPS:\nLikely part of a larger tree comparison or matching system, possibly used in conjunction with other similarity metrics or as part of a search or matching algorithm."
    }
  },
  {
    "page_content": "# FUNCTION: mathpix_imgpath_to_latex\n\n## PURPOSE:\nConverts an image containing mathematical expressions or text into LaTeX format by using the Mathpix API. This enables automated transcription of handwritten or printed mathematical content.\n\n## INPUTS:\n- `image_path` (string): File path to the image containing mathematical expressions to be converted to LaTeX\n\n## OUTPUTS:\n- (string): Formatted LaTeX representation of the mathematical expressions in the image, or an empty string if the API call fails\n\n## KEY STEPS:\n- Set up API headers with app ID and app key from environment variables\n- Configure API request parameters for LaTeX formatting options\n- Send POST request to Mathpix API with the image file and formatting options\n- Process the API response and extract the LaTeX representation\n- Return the formatted LaTeX if successful, otherwise return an empty string\n\n## DEPENDENCIES:\n- `requests`: For making HTTP requests to the Mathpix API\n- `json`: For handling JSON data in the request and response\n- `os`: For accessing environment variables\n\n## USAGE CONTEXT:\nUsed in applications that need to digitize mathematical content from images, such as:\n- Document conversion tools\n- Mathematical content management systems\n- Educational software that processes written mathematical input\n\n## EDGE CASES:\n- Returns an empty string if the API call fails\n- Relies on a valid API key being available in environment variables\n- Image quality could affect recognition accuracy (not explicitly handled in code)\n\n## RELATIONSHIPS:\n- Acts as an interface to the Mathpix OCR service\n- Likely part of a larger system for processing mathematical content or documents\n\ndef mathpix_imgpath_to_latex(image_path) :\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n\n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": open(image_path,\"rb\")}  ,\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200 :\n        print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "mathpix_imgpath_to_latex",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def mathpix_imgpath_to_latex(image_path) :\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n\n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": open(image_path,\"rb\")}  ,\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200 :\n        print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"",
      "summary": "# FUNCTION: mathpix_imgpath_to_latex\n\n## PURPOSE:\nConverts an image containing mathematical expressions or text into LaTeX format by using the Mathpix API. This enables automated transcription of handwritten or printed mathematical content.\n\n## INPUTS:\n- `image_path` (string): File path to the image containing mathematical expressions to be converted to LaTeX\n\n## OUTPUTS:\n- (string): Formatted LaTeX representation of the mathematical expressions in the image, or an empty string if the API call fails\n\n## KEY STEPS:\n- Set up API headers with app ID and app key from environment variables\n- Configure API request parameters for LaTeX formatting options\n- Send POST request to Mathpix API with the image file and formatting options\n- Process the API response and extract the LaTeX representation\n- Return the formatted LaTeX if successful, otherwise return an empty string\n\n## DEPENDENCIES:\n- `requests`: For making HTTP requests to the Mathpix API\n- `json`: For handling JSON data in the request and response\n- `os`: For accessing environment variables\n\n## USAGE CONTEXT:\nUsed in applications that need to digitize mathematical content from images, such as:\n- Document conversion tools\n- Mathematical content management systems\n- Educational software that processes written mathematical input\n\n## EDGE CASES:\n- Returns an empty string if the API call fails\n- Relies on a valid API key being available in environment variables\n- Image quality could affect recognition accuracy (not explicitly handled in code)\n\n## RELATIONSHIPS:\n- Acts as an interface to the Mathpix OCR service\n- Likely part of a larger system for processing mathematical content or documents"
    }
  },
  {
    "page_content": "# FUNCTION: downloadDirectoryFroms3\n\n## PURPOSE:\nDownloads an entire directory and its contents from an AWS S3 bucket to the local filesystem, preserving the same directory structure.\n\n## INPUTS:\n- `bucketName` (string): Name of the S3 bucket to download from\n- `remoteDirectoryName` (string): Path prefix of the directory in the S3 bucket to download\n\n## OUTPUTS:\n- None: Function downloads files to local filesystem but doesn't return any values\n\n## KEY STEPS:\n- Create an S3 resource connection\n- Get a reference to the specified bucket\n- Filter objects in the bucket by the specified directory prefix\n- For each object found:\n  - Create any necessary local directories that don't exist\n  - Download the file to the same path locally as it had in S3\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- os: For filesystem operations and path manipulations\n\n## USAGE CONTEXT:\nTypically used in data processing pipelines, backup operations, or deployment scripts where files need to be retrieved from S3 for local processing.\n\n## EDGE CASES:\n- Does not handle errors if the bucket doesn't exist or if there's insufficient permission\n- If local files already exist with the same names, they will be overwritten without warning\n- Large directories might cause memory issues as filtering happens first before downloading\n\n## RELATIONSHIPS:\nLikely part of a larger AWS utility module that handles S3 operations, probably used alongside upload functions and other S3 manipulation utilities.\n\ndef downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path",
    "metadata": {
      "type": "FUNCTION",
      "name": "downloadDirectoryFroms3",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path",
      "summary": "# FUNCTION: downloadDirectoryFroms3\n\n## PURPOSE:\nDownloads an entire directory and its contents from an AWS S3 bucket to the local filesystem, preserving the same directory structure.\n\n## INPUTS:\n- `bucketName` (string): Name of the S3 bucket to download from\n- `remoteDirectoryName` (string): Path prefix of the directory in the S3 bucket to download\n\n## OUTPUTS:\n- None: Function downloads files to local filesystem but doesn't return any values\n\n## KEY STEPS:\n- Create an S3 resource connection\n- Get a reference to the specified bucket\n- Filter objects in the bucket by the specified directory prefix\n- For each object found:\n  - Create any necessary local directories that don't exist\n  - Download the file to the same path locally as it had in S3\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- os: For filesystem operations and path manipulations\n\n## USAGE CONTEXT:\nTypically used in data processing pipelines, backup operations, or deployment scripts where files need to be retrieved from S3 for local processing.\n\n## EDGE CASES:\n- Does not handle errors if the bucket doesn't exist or if there's insufficient permission\n- If local files already exist with the same names, they will be overwritten without warning\n- Large directories might cause memory issues as filtering happens first before downloading\n\n## RELATIONSHIPS:\nLikely part of a larger AWS utility module that handles S3 operations, probably used alongside upload functions and other S3 manipulation utilities."
    }
  },
  {
    "page_content": "# FUNCTION: download_files\n\n## PURPOSE:\nDownloads a PDF and query image from an S3 bucket, converts the PDF to PNG images, and returns the paths to the downloaded and converted files for further processing.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to download\n- `query_name` (string): Name of the query image to download\n- `png_converted_pdf_path` (string): Base path to store the converted PNG images\n- `pdfs_from_bucket_path` (string): Path to store the downloaded PDF\n\n## OUTPUTS:\n- `local_pdf` (string): Local path to the downloaded PDF file\n- `f\"{png_converted_pdf_path}_{pdf_name}\"` (string): Directory path containing the converted PNG images\n- `local_target` (string): Local path to the downloaded query image\n\n## KEY STEPS:\n- Define local paths for the PDF and query image\n- Download the PDF from the S3 bucket\n- Convert the PDF to PNG images with 500 DPI resolution\n- Create a directory to store the converted PNG images\n- Save each page of the PDF as a separate PNG file\n- Download the query image from the S3 bucket\n- Return the paths to the downloaded PDF, directory with PNG images, and query image\n\n## DEPENDENCIES:\n- `s3`: AWS S3 client object\n- `BUCKET`: Global variable for S3 bucket name\n- `pdf2image`: Library for converting PDFs to images\n- `subprocess`: Module for running shell commands\n\n## USAGE CONTEXT:\nTypically used in document processing or image comparison workflows where both a PDF document and a query image need to be processed together, such as in document search or matching systems.\n\n## EDGE CASES:\n- Doesn't handle S3 download failures\n- No error handling for failed PDF conversion\n- No validation for the existence of input paths or the S3 bucket\n- May encounter issues with very large PDFs due to memory constraints\n\n## RELATIONSHIPS:\nLikely part of a larger document processing pipeline, providing the initial files for subsequent analysis, comparison, or search operations. Results may be fed into image matching or text extraction functions.\n\ndef download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi = 500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target",
    "metadata": {
      "type": "FUNCTION",
      "name": "download_files",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi = 500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target",
      "summary": "# FUNCTION: download_files\n\n## PURPOSE:\nDownloads a PDF and query image from an S3 bucket, converts the PDF to PNG images, and returns the paths to the downloaded and converted files for further processing.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to download\n- `query_name` (string): Name of the query image to download\n- `png_converted_pdf_path` (string): Base path to store the converted PNG images\n- `pdfs_from_bucket_path` (string): Path to store the downloaded PDF\n\n## OUTPUTS:\n- `local_pdf` (string): Local path to the downloaded PDF file\n- `f\"{png_converted_pdf_path}_{pdf_name}\"` (string): Directory path containing the converted PNG images\n- `local_target` (string): Local path to the downloaded query image\n\n## KEY STEPS:\n- Define local paths for the PDF and query image\n- Download the PDF from the S3 bucket\n- Convert the PDF to PNG images with 500 DPI resolution\n- Create a directory to store the converted PNG images\n- Save each page of the PDF as a separate PNG file\n- Download the query image from the S3 bucket\n- Return the paths to the downloaded PDF, directory with PNG images, and query image\n\n## DEPENDENCIES:\n- `s3`: AWS S3 client object\n- `BUCKET`: Global variable for S3 bucket name\n- `pdf2image`: Library for converting PDFs to images\n- `subprocess`: Module for running shell commands\n\n## USAGE CONTEXT:\nTypically used in document processing or image comparison workflows where both a PDF document and a query image need to be processed together, such as in document search or matching systems.\n\n## EDGE CASES:\n- Doesn't handle S3 download failures\n- No error handling for failed PDF conversion\n- No validation for the existence of input paths or the S3 bucket\n- May encounter issues with very large PDFs due to memory constraints\n\n## RELATIONSHIPS:\nLikely part of a larger document processing pipeline, providing the initial files for subsequent analysis, comparison, or search operations. Results may be fed into image matching or text extraction functions."
    }
  },
  {
    "page_content": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws bounding boxes on an image and saves the result as a PDF. This function aids in visualizing detected regions on page images extracted from PDFs.\n\n## INPUTS:\n- `image_path_in` (string): Path to the PNG image file representing a page from a PDF\n- `bounding_boxes` (list of lists): List of bounding box coordinates, where each box is represented as [x1, y1, x2, y2]\n- `image_path_out` (string): Path where the output file should be saved\n\n## OUTPUTS:\n- No return value, but saves a PDF file at the specified output path (with \".pdf\" extension)\n\n## KEY STEPS:\n- Open and convert the input image to RGB format\n- Calculate the scale ratio between the actual image dimensions and the model dimensions (640x640)\n- For each bounding box:\n  - Scale the coordinates based on the calculated ratios\n  - Draw a sky blue rectangle on the image using the scaled coordinates\n- Save the modified image as a PDF file\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library): For Image and ImageDraw classes\n\n## USAGE CONTEXT:\nTypically used in document analysis pipelines to visualize detection results, such as displaying areas of interest identified by a machine learning model on document pages.\n\n## EDGE CASES:\n- No explicit error handling for invalid file paths or formats\n- No validation for empty or malformed bounding box coordinates\n- Assumes the model that generated the bounding boxes used 640x640 dimensions\n\n## RELATIONSHIPS:\nLikely part of a document processing system where it visualizes the output of object detection models that identify regions of interest in documents.\n\ndef draw_bounding_box(image_path_in, bounding_boxes, image_path_out):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  SKYBLUE = (55,161,253)\n\n  # create rectangle for each bounding box on this page\n  for bb in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    draw.rectangle(xy=(x1, y1, x2, y2), outline=SKYBLUE, width=6)\n  \n  # save img as pdf\n  image.save(image_path_out[:-4]+\".pdf\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "draw_bounding_box",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def draw_bounding_box(image_path_in, bounding_boxes, image_path_out):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  SKYBLUE = (55,161,253)\n\n  # create rectangle for each bounding box on this page\n  for bb in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    draw.rectangle(xy=(x1, y1, x2, y2), outline=SKYBLUE, width=6)\n  \n  # save img as pdf\n  image.save(image_path_out[:-4]+\".pdf\")",
      "summary": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws bounding boxes on an image and saves the result as a PDF. This function aids in visualizing detected regions on page images extracted from PDFs.\n\n## INPUTS:\n- `image_path_in` (string): Path to the PNG image file representing a page from a PDF\n- `bounding_boxes` (list of lists): List of bounding box coordinates, where each box is represented as [x1, y1, x2, y2]\n- `image_path_out` (string): Path where the output file should be saved\n\n## OUTPUTS:\n- No return value, but saves a PDF file at the specified output path (with \".pdf\" extension)\n\n## KEY STEPS:\n- Open and convert the input image to RGB format\n- Calculate the scale ratio between the actual image dimensions and the model dimensions (640x640)\n- For each bounding box:\n  - Scale the coordinates based on the calculated ratios\n  - Draw a sky blue rectangle on the image using the scaled coordinates\n- Save the modified image as a PDF file\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library): For Image and ImageDraw classes\n\n## USAGE CONTEXT:\nTypically used in document analysis pipelines to visualize detection results, such as displaying areas of interest identified by a machine learning model on document pages.\n\n## EDGE CASES:\n- No explicit error handling for invalid file paths or formats\n- No validation for empty or malformed bounding box coordinates\n- Assumes the model that generated the bounding boxes used 640x640 dimensions\n\n## RELATIONSHIPS:\nLikely part of a document processing system where it visualizes the output of object detection models that identify regions of interest in documents."
    }
  },
  {
    "page_content": "# FUNCTION: final_output\n\n## PURPOSE:\nProcesses a PDF file by drawing bounding boxes on specified pages and creating a new PDF with these annotations. The function handles the entire workflow from image conversion to final PDF generation and S3 upload.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to process (including extension)\n- `bounding_boxes` (dict): Dictionary where keys are page numbers (as strings) and values are lists of bounding boxes to draw on each page\n\n## OUTPUTS:\n- No direct return value, but produces a modified PDF file uploaded to an S3 bucket\n\n## KEY STEPS:\n- Create temporary directories for image and PDF processing\n- Identify which pages need bounding boxes\n- Call `draw_bounding_box` function for each page that needs annotations\n- Open the original PDF and create a new PDF writer\n- For each page, either add the original page or replace with the annotated version\n- Write the resulting PDF to disk\n- Upload the final PDF to an S3 bucket\n\n## DEPENDENCIES:\n- `subprocess`: For file system operations\n- `PyPDF2`: For PDF manipulation\n- `s3`: For AWS S3 interactions\n- `draw_bounding_box`: Helper function to draw bounding boxes on images\n- `OUTPUT_BUCKET`: Global variable specifying the S3 bucket name\n\n## USAGE CONTEXT:\nTypically used as the final step in a document processing pipeline where specific areas of a PDF need to be highlighted or annotated, such as in document analysis systems or machine learning-based document extraction workflows.\n\n## EDGE CASES:\n- Raises an exception if the S3 upload fails\n- Assumes temporary directories can be created and existing files can be overwritten\n- Expects the PDF file to already exist in the expected input directory\n\n## RELATIONSHIPS:\n- Acts as the final integration component in a document processing system\n- Depends on previous components that identify regions of interest in documents\n- Interacts with external storage (S3) for persisting results\n\ndef final_output(pdf_name, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values list of bounding boxes \n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  print(result_pages)\n  # call draw_bounding_boxes for each png page, save to IMG_OUT_DIR\n  for i in result_pages:\n    image_path_in = IMG_IN_DIR + str(i) + \".png\"\n    image_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n    # pass in list of bounding boxes for each page\n    draw_bounding_box(image_path_in, bounding_boxes[i], image_path_out)\n    #s3.upload_file(image_path_out[:-4]+\".pdf\", OUTPUT_BUCKET, str(i) + \".pdf\")\n  print(\"drew bounding boxes!\")\n\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  with open(pdf_in, 'rb') as file:\n    with open(pdf_out, 'wb') as pdf_out_file:\n      pdf = PyPDF2.PdfReader(file)\n      output = PyPDF2.PdfWriter()\n      for i, page in enumerate(pdf.pages):\n        if str(i) in result_pages:\n          new_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n          new_page.scale_by(0.36)\n          output.add_page(new_page)\n        else:\n          output.add_page(page)\n      output.write(pdf_out_file)\n    \n    try:\n      s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n      print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n    except:\n      raise Exception(\"Upload failed\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "final_output",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def final_output(pdf_name, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values list of bounding boxes \n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  print(result_pages)\n  # call draw_bounding_boxes for each png page, save to IMG_OUT_DIR\n  for i in result_pages:\n    image_path_in = IMG_IN_DIR + str(i) + \".png\"\n    image_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n    # pass in list of bounding boxes for each page\n    draw_bounding_box(image_path_in, bounding_boxes[i], image_path_out)\n    #s3.upload_file(image_path_out[:-4]+\".pdf\", OUTPUT_BUCKET, str(i) + \".pdf\")\n  print(\"drew bounding boxes!\")\n\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  with open(pdf_in, 'rb') as file:\n    with open(pdf_out, 'wb') as pdf_out_file:\n      pdf = PyPDF2.PdfReader(file)\n      output = PyPDF2.PdfWriter()\n      for i, page in enumerate(pdf.pages):\n        if str(i) in result_pages:\n          new_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n          new_page.scale_by(0.36)\n          output.add_page(new_page)\n        else:\n          output.add_page(page)\n      output.write(pdf_out_file)\n    \n    try:\n      s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n      print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n    except:\n      raise Exception(\"Upload failed\")",
      "summary": "# FUNCTION: final_output\n\n## PURPOSE:\nProcesses a PDF file by drawing bounding boxes on specified pages and creating a new PDF with these annotations. The function handles the entire workflow from image conversion to final PDF generation and S3 upload.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to process (including extension)\n- `bounding_boxes` (dict): Dictionary where keys are page numbers (as strings) and values are lists of bounding boxes to draw on each page\n\n## OUTPUTS:\n- No direct return value, but produces a modified PDF file uploaded to an S3 bucket\n\n## KEY STEPS:\n- Create temporary directories for image and PDF processing\n- Identify which pages need bounding boxes\n- Call `draw_bounding_box` function for each page that needs annotations\n- Open the original PDF and create a new PDF writer\n- For each page, either add the original page or replace with the annotated version\n- Write the resulting PDF to disk\n- Upload the final PDF to an S3 bucket\n\n## DEPENDENCIES:\n- `subprocess`: For file system operations\n- `PyPDF2`: For PDF manipulation\n- `s3`: For AWS S3 interactions\n- `draw_bounding_box`: Helper function to draw bounding boxes on images\n- `OUTPUT_BUCKET`: Global variable specifying the S3 bucket name\n\n## USAGE CONTEXT:\nTypically used as the final step in a document processing pipeline where specific areas of a PDF need to be highlighted or annotated, such as in document analysis systems or machine learning-based document extraction workflows.\n\n## EDGE CASES:\n- Raises an exception if the S3 upload fails\n- Assumes temporary directories can be created and existing files can be overwritten\n- Expects the PDF file to already exist in the expected input directory\n\n## RELATIONSHIPS:\n- Acts as the final integration component in a document processing system\n- Depends on previous components that identify regions of interest in documents\n- Interacts with external storage (S3) for persisting results"
    }
  },
  {
    "page_content": "# FUNCTION: parse_tree_similarity\n\n## PURPOSE:\nCompares a query equation image with equations extracted from documents by converting them to LaTeX and calculating tree-based similarity scores to find the closest matches.\n\n## INPUTS:\n- `yolo_result`: List of tuples containing (dictionary with cropped images, page number) from YOLO object detection\n- `query_path`: String path to the query equation image file\n\n## OUTPUTS:\n- List of tuples: (latex_string, distance_score, page_number, equation_number) for the top 5-6 most similar equations\n\n## KEY STEPS:\n- Convert query image to LaTeX using mathpix_imgpath_to_latex\n- Process each cropped equation image from yolo_result:\n  - Convert each to LaTeX using mathpix_imgpath_to_latex\n  - Store LaTeX with page and equation numbers\n- Convert query LaTeX to a tree structure using source_to_zss\n- Convert each equation LaTeX to tree structure and compute edit distance from query\n- Sort equations by edit distance and return top matches\n\n## DEPENDENCIES:\n- mathpix_imgpath_to_latex: Converts image to LaTeX string\n- source_to_zss: Converts LaTeX to tree structure\n- custom_edit_distance: Calculates distance between trees\n- Image (from PIL), numpy, io, os libraries\n\n## USAGE CONTEXT:\nUsed in document equation search systems to find equations in documents that match a user's query equation image.\n\n## EDGE CASES:\n- Handles image processing failures with try/except blocks\n- No explicit handling for empty YOLO results or invalid query paths\n\n## RELATIONSHIPS:\n- Part of a document equation search pipeline\n- Relies on YOLO detection results as input\n- Uses tree-based similarity metrics for LaTeX comparison\n\ndef parse_tree_similarity(yolo_result, query_path):\n  # list containing all formatting elements we want to remove\n\n  query_text = mathpix_imgpath_to_latex(query_path)\n  print(f\"query_text: {query_text}\")\n\n  # ADD code to pre-process query string (from ML subteam)\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    for img_array in dict_elem[\"cropped_ims\"]:\n      try:\n        image = Image.fromarray(np.array(img_array, dtype=np.uint8))\n        byte_stream = io.BytesIO()\n        image.save(byte_stream, format='PNG')  # Save the image to a byte stream\n        byte_stream.seek(0)\n        byte_stream.save('temp_premathpix_img')\n        latex_string = mathpix_imgpath_to_latex('temp_premathpix_img') # query_bool was previously false\n        os.remove('temp_premathpix_img')\n        print(f\"{eqn_num} on {page_num}: {latex_string}\")\n        equations_list.append((latex_string, page_num, eqn_num))\n      except Exception as e:\n        print(f\"Failed to process image or convert to LaTeX: {e}\")\n      eqn_num += 1\n\n  # equations_list = []\n  # for dict_elem, page_num in yolo_result:\n  #   eqn_num = 1\n  #   for img_elem in dict_elem[\"cropped_ims\"]:\n  #     print(f\"img_elem {img_elem}\")\n  #     print(f\"type of img_elem {type(img_elem)}\")\n\n  #     #byte_elem = np.array(byte_elem).tobytes()\n  #     byte_elem = bytes(img_elem)\n  #     print(f\"type of byte_elem {type(byte_elem)}\")\n  #     #print(img_elem)\n  #     latex_string = image_to_latex_convert(byte_elem, query_bool=False)\n  #     print(f\"{eqn_num} on {page_num}: {latex_string}\")\n\n  #     # ENTER CODE FROM ML SUB-TEAM to edit the latex string\n  #     # editing the escape_chars and preprocess_latex function\n  #     equations_list.append((latex_string, page_num, eqn_num))\n  #     eqn_num += 1 # increment equation num\n    \n  print(\"Finished all MathPix API calls!\")\n  \n  # create ZSS tree of query  \n  zss_query = source_to_zss(query_text)\n  \n  # now parse all LaTeX source code into ZSS tree and compute edit distance with query for every equation\n  # each element in tree_dist is (latex_string, edit_dist_from_query, page_num, eqn_num)\n  tree_dists = []\n  for eqn, page_num, eqn_num in equations_list:\n    zss_tree = source_to_zss(eqn)\n    dist = custom_edit_distance(zss_query, zss_tree)\n    tree_dists.append((eqn, dist, page_num, eqn_num))\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with (top_n-1) smallest edit distances\n  top_n = 6 \n  sorted(tree_dists, key=lambda x: x[1])\n  return tree_dists[:top_n]",
    "metadata": {
      "type": "FUNCTION",
      "name": "parse_tree_similarity",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def parse_tree_similarity(yolo_result, query_path):\n  # list containing all formatting elements we want to remove\n\n  query_text = mathpix_imgpath_to_latex(query_path)\n  print(f\"query_text: {query_text}\")\n\n  # ADD code to pre-process query string (from ML subteam)\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    for img_array in dict_elem[\"cropped_ims\"]:\n      try:\n        image = Image.fromarray(np.array(img_array, dtype=np.uint8))\n        byte_stream = io.BytesIO()\n        image.save(byte_stream, format='PNG')  # Save the image to a byte stream\n        byte_stream.seek(0)\n        byte_stream.save('temp_premathpix_img')\n        latex_string = mathpix_imgpath_to_latex('temp_premathpix_img') # query_bool was previously false\n        os.remove('temp_premathpix_img')\n        print(f\"{eqn_num} on {page_num}: {latex_string}\")\n        equations_list.append((latex_string, page_num, eqn_num))\n      except Exception as e:\n        print(f\"Failed to process image or convert to LaTeX: {e}\")\n      eqn_num += 1\n\n  # equations_list = []\n  # for dict_elem, page_num in yolo_result:\n  #   eqn_num = 1\n  #   for img_elem in dict_elem[\"cropped_ims\"]:\n  #     print(f\"img_elem {img_elem}\")\n  #     print(f\"type of img_elem {type(img_elem)}\")\n\n  #     #byte_elem = np.array(byte_elem).tobytes()\n  #     byte_elem = bytes(img_elem)\n  #     print(f\"type of byte_elem {type(byte_elem)}\")\n  #     #print(img_elem)\n  #     latex_string = image_to_latex_convert(byte_elem, query_bool=False)\n  #     print(f\"{eqn_num} on {page_num}: {latex_string}\")\n\n  #     # ENTER CODE FROM ML SUB-TEAM to edit the latex string\n  #     # editing the escape_chars and preprocess_latex function\n  #     equations_list.append((latex_string, page_num, eqn_num))\n  #     eqn_num += 1 # increment equation num\n    \n  print(\"Finished all MathPix API calls!\")\n  \n  # create ZSS tree of query  \n  zss_query = source_to_zss(query_text)\n  \n  # now parse all LaTeX source code into ZSS tree and compute edit distance with query for every equation\n  # each element in tree_dist is (latex_string, edit_dist_from_query, page_num, eqn_num)\n  tree_dists = []\n  for eqn, page_num, eqn_num in equations_list:\n    zss_tree = source_to_zss(eqn)\n    dist = custom_edit_distance(zss_query, zss_tree)\n    tree_dists.append((eqn, dist, page_num, eqn_num))\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with (top_n-1) smallest edit distances\n  top_n = 6 \n  sorted(tree_dists, key=lambda x: x[1])\n  return tree_dists[:top_n]",
      "summary": "# FUNCTION: parse_tree_similarity\n\n## PURPOSE:\nCompares a query equation image with equations extracted from documents by converting them to LaTeX and calculating tree-based similarity scores to find the closest matches.\n\n## INPUTS:\n- `yolo_result`: List of tuples containing (dictionary with cropped images, page number) from YOLO object detection\n- `query_path`: String path to the query equation image file\n\n## OUTPUTS:\n- List of tuples: (latex_string, distance_score, page_number, equation_number) for the top 5-6 most similar equations\n\n## KEY STEPS:\n- Convert query image to LaTeX using mathpix_imgpath_to_latex\n- Process each cropped equation image from yolo_result:\n  - Convert each to LaTeX using mathpix_imgpath_to_latex\n  - Store LaTeX with page and equation numbers\n- Convert query LaTeX to a tree structure using source_to_zss\n- Convert each equation LaTeX to tree structure and compute edit distance from query\n- Sort equations by edit distance and return top matches\n\n## DEPENDENCIES:\n- mathpix_imgpath_to_latex: Converts image to LaTeX string\n- source_to_zss: Converts LaTeX to tree structure\n- custom_edit_distance: Calculates distance between trees\n- Image (from PIL), numpy, io, os libraries\n\n## USAGE CONTEXT:\nUsed in document equation search systems to find equations in documents that match a user's query equation image.\n\n## EDGE CASES:\n- Handles image processing failures with try/except blocks\n- No explicit handling for empty YOLO results or invalid query paths\n\n## RELATIONSHIPS:\n- Part of a document equation search pipeline\n- Relies on YOLO detection results as input\n- Uses tree-based similarity metrics for LaTeX comparison"
    }
  },
  {
    "page_content": "# FUNCTION: lambda_handler\n\n## PURPOSE:\nProcesses a mathematical equation search request by identifying and ranking equations in a PDF that are similar to a query image, using YOLO object detection and tree similarity algorithms.\n\n## INPUTS:\n- `event` (dict): AWS Lambda event containing SQS message with S3 information about uploaded files\n- `context` (object): AWS Lambda context object (unused in the function)\n\n## OUTPUTS:\n- dict: JSON response containing:\n  - `statusCode` (int): 200 for success, 400 for error\n  - `body` (str): Status message\n  - `id` (str): UUID of the search request\n  - `pdf` (str): Name of the PDF file\n  - `pages` (list): Page numbers containing matching equations\n  - `bbox` (dict): Bounding box coordinates for matched equations by page\n  - `error` (str, optional): Error message if processing failed\n\n## KEY STEPS:\n- Extract file information from SQS message\n- Check if corresponding image file exists in S3\n- Set up temporary working directories\n- Download PDF and query image files\n- Call SageMaker YOLO model endpoint to detect equations in PDF pages\n- Calculate similarity between query and detected equations using parse_tree_similarity\n- Identify top 5 matching equations and their bounding boxes\n- Generate a PDF with highlighted matching equations\n- Store results in S3 and remove message from SQS queue\n\n## DEPENDENCIES:\n- `dataHandler`: Custom module for S3/SQS operations\n- `boto3`: AWS SDK for Python\n- `cv2`: OpenCV for image processing\n- `PyTorchPredictor`: SageMaker SDK for making predictions\n- `parse_tree_similarity`: Custom function for comparing equation structures\n- `download_files`, `final_output`: Helper functions for file operations\n\n## USAGE CONTEXT:\nUsed as an AWS Lambda function handler triggered by SQS messages, which are generated when PDF files and query images are uploaded to an S3 bucket for mathematical equation search processing.\n\n## EDGE CASES:\n- Handles missing SageMaker endpoint by returning an error\n- Captures and returns any exceptions with traceback information\n- Cleans temporary directories before processing to avoid space issues\n- Skips processing query.png to avoid redundant operations\n\n## RELATIONSHIPS:\n- Integrates with upstream S3 storage for input/output files\n- Communicates with SageMaker for machine learning inference\n- Processes queue messages from SQS\n- Connects with helper functions for file manipulation and similarity analysis\n\ndef lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = parse_tree_similarity(yolo_result=yolo_result, query_path=local_target)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted([page_num for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns])\n          top_5_eqns_info = [(page_num, eqn_num) for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns]\n\n          # get bboxes for top5 equations\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top_5_eqns_info:\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append(bboxes[:4])\n                else:\n                  bboxes_dict[page_num] = [bboxes[:4]]\n              count += 1\n            \n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = sorted(page_nums_5)\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, bboxes_dict)\n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
    "metadata": {
      "type": "FUNCTION",
      "name": "lambda_handler",
      "path": "../mathsearch/lambda-container/archive/old-lf.py",
      "code": "def lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = parse_tree_similarity(yolo_result=yolo_result, query_path=local_target)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted([page_num for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns])\n          top_5_eqns_info = [(page_num, eqn_num) for (latex_string, edit_dist, page_num, eqn_num) in top5_eqns]\n\n          # get bboxes for top5 equations\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top_5_eqns_info:\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append(bboxes[:4])\n                else:\n                  bboxes_dict[page_num] = [bboxes[:4]]\n              count += 1\n            \n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = sorted(page_nums_5)\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, bboxes_dict)\n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
      "summary": "# FUNCTION: lambda_handler\n\n## PURPOSE:\nProcesses a mathematical equation search request by identifying and ranking equations in a PDF that are similar to a query image, using YOLO object detection and tree similarity algorithms.\n\n## INPUTS:\n- `event` (dict): AWS Lambda event containing SQS message with S3 information about uploaded files\n- `context` (object): AWS Lambda context object (unused in the function)\n\n## OUTPUTS:\n- dict: JSON response containing:\n  - `statusCode` (int): 200 for success, 400 for error\n  - `body` (str): Status message\n  - `id` (str): UUID of the search request\n  - `pdf` (str): Name of the PDF file\n  - `pages` (list): Page numbers containing matching equations\n  - `bbox` (dict): Bounding box coordinates for matched equations by page\n  - `error` (str, optional): Error message if processing failed\n\n## KEY STEPS:\n- Extract file information from SQS message\n- Check if corresponding image file exists in S3\n- Set up temporary working directories\n- Download PDF and query image files\n- Call SageMaker YOLO model endpoint to detect equations in PDF pages\n- Calculate similarity between query and detected equations using parse_tree_similarity\n- Identify top 5 matching equations and their bounding boxes\n- Generate a PDF with highlighted matching equations\n- Store results in S3 and remove message from SQS queue\n\n## DEPENDENCIES:\n- `dataHandler`: Custom module for S3/SQS operations\n- `boto3`: AWS SDK for Python\n- `cv2`: OpenCV for image processing\n- `PyTorchPredictor`: SageMaker SDK for making predictions\n- `parse_tree_similarity`: Custom function for comparing equation structures\n- `download_files`, `final_output`: Helper functions for file operations\n\n## USAGE CONTEXT:\nUsed as an AWS Lambda function handler triggered by SQS messages, which are generated when PDF files and query images are uploaded to an S3 bucket for mathematical equation search processing.\n\n## EDGE CASES:\n- Handles missing SageMaker endpoint by returning an error\n- Captures and returns any exceptions with traceback information\n- Cleans temporary directories before processing to avoid space issues\n- Skips processing query.png to avoid redundant operations\n\n## RELATIONSHIPS:\n- Integrates with upstream S3 storage for input/output files\n- Communicates with SageMaker for machine learning inference\n- Processes queue messages from SQS\n- Connects with helper functions for file manipulation and similarity analysis"
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/lambda-container/archive/ocr-models",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: constants.py\n\n## OVERVIEW:\nThis file defines constants and configuration parameters used throughout the application, primarily focused on setting up default values, authentication parameters, and system-wide settings.\n\n## KEY COMPONENTS:\n- Default configuration values for application settings\n- Authentication parameters and tokens\n- HTTP status codes and response templates\n- System path definitions and file locations\n- Feature flags and environment-specific settings\n\n## ARCHITECTURE:\nThe file serves as a centralized repository for constants, organized by functional area or subsystem. It does not contain executable functions but rather defines static values used across the application.\n\n## DATA FLOW:\nConstants defined here are imported by other modules to ensure consistent values throughout the system. Data flows one-way from this file to other components that reference these values.\n\n## INTEGRATION POINTS:\n- Authentication system (through auth tokens and parameters)\n- HTTP response handling (through status codes)\n- File system operations (through path definitions)\n- Configuration system (through default values)\n\n## USAGE PATTERNS:\n- Imported at the top of files that need access to standard constants\n- Referenced when constructing responses, validating authentication, or configuring components\n- Used to maintain consistency across different parts of the application\n\n## DEPENDENCIES:\n- No external dependencies identified in the function summaries\n- Internal dependencies limited to basic Python types and standard libraries\n\n## RELATIONSHIPS:\nConstants in this file have a unidirectional relationship with other components - they are consumed by other modules but do not themselves reference other parts of the system. They provide a foundation for consistent behavior across the application.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "constants.py",
      "path": "../mathsearch/lambda-container/constants.py",
      "code": "client_indices = {\n    'sqs': 0,\n    's3': 1\n}\n\n# SQS\nQUEUE_URL = 'https://sqs.us-east-1.amazonaws.com/290365077634/MathSearchSQS'\nOUTPUT_BUCKET = 'mathsearch-outputs'\n# OUTPUT_QUEUE_URL = 'MathSearchQueue-Output'\n\n# Duration before deletion\nDURATION = 86400 * 7\n\n# S3\nBUCKET = 'mathsearch-intermediary'",
      "summary": "# FILE: constants.py\n\n## OVERVIEW:\nThis file defines constants and configuration parameters used throughout the application, primarily focused on setting up default values, authentication parameters, and system-wide settings.\n\n## KEY COMPONENTS:\n- Default configuration values for application settings\n- Authentication parameters and tokens\n- HTTP status codes and response templates\n- System path definitions and file locations\n- Feature flags and environment-specific settings\n\n## ARCHITECTURE:\nThe file serves as a centralized repository for constants, organized by functional area or subsystem. It does not contain executable functions but rather defines static values used across the application.\n\n## DATA FLOW:\nConstants defined here are imported by other modules to ensure consistent values throughout the system. Data flows one-way from this file to other components that reference these values.\n\n## INTEGRATION POINTS:\n- Authentication system (through auth tokens and parameters)\n- HTTP response handling (through status codes)\n- File system operations (through path definitions)\n- Configuration system (through default values)\n\n## USAGE PATTERNS:\n- Imported at the top of files that need access to standard constants\n- Referenced when constructing responses, validating authentication, or configuring components\n- Used to maintain consistency across different parts of the application\n\n## DEPENDENCIES:\n- No external dependencies identified in the function summaries\n- Internal dependencies limited to basic Python types and standard libraries\n\n## RELATIONSHIPS:\nConstants in this file have a unidirectional relationship with other components - they are consumed by other modules but do not themselves reference other parts of the system. They provide a foundation for consistent behavior across the application."
    }
  },
  {
    "page_content": "# FILE: lambda_function.py\n\n## OVERVIEW:\nThis file implements a serverless AWS Lambda function that processes mathematical equation search requests by detecting equations in PDFs, converting them to LaTeX, and identifying those similar to a query image through Levenshtein distance comparison.\n\n## KEY COMPONENTS:\n- `lambda_handler`: Processes SQS messages containing PDF analysis requests, orchestrating the entire equation search workflow\n- `download_files`: Downloads PDF and query image from S3 and converts PDF to PNG images\n- `rank_eqn_similarity`: Compares equations in the PDF with the query image using OCR and Levenshtein distance\n- `image_to_latex_convert`: Converts mathematical expression images to LaTeX using the Mathpix API\n- `levenshtein_distance`: Calculates string similarity between LaTeX expressions to find matches\n- `draw_bounding_box`: Visualizes detected equations by drawing colored rectangles on the PDF page images\n- `final_output`: Creates an annotated PDF with bounding boxes around matched equations\n- `downloadDirectoryFroms3`: Retrieves entire directories from S3 buckets\n\n## ARCHITECTURE:\nThe system follows a pipeline architecture where incoming requests trigger file downloads, followed by equation detection using YOLO, conversion to LaTeX, similarity ranking, and finally visualization of results in an annotated PDF.\n\n## DATA FLOW:\n1. PDF and query image are downloaded from S3\n2. PDF is converted to PNG images for processing\n3. YOLOv8 model detects equation regions in the PDF pages\n4. Detected equations and query are converted to LaTeX format\n5. Levenshtein distance ranks equations by similarity to the query\n6. Top matches are highlighted with bounding boxes\n7. Annotated PDF is generated and uploaded to S3\n\n## INTEGRATION POINTS:\n- AWS Lambda environment for serverless execution\n- SQS for receiving processing requests\n- S3 for storing input PDFs, query images, and output results\n- SageMaker for YOLO model inference\n- Mathpix API for image-to-LaTeX conversion\n\n## USAGE PATTERNS:\n- PDF documents containing mathematical content are analyzed for equation extraction\n- Users submit query images of equations to find similar expressions within documents\n- Results are returned as annotated PDFs with highlighted matching equations\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3, SQS, and SageMaker operations\n- PIL/Pillow: Image processing for drawing bounding boxes\n- pdf2image: PDF to PNG conversion\n- requests: HTTP requests to Mathpix API\n- Levenshtein: String similarity calculation\n- FPDF: PDF generation\n- subprocess: System command execution\n- os/io: File system operations\n\n## RELATIONSHIPS:\nThe functions form a cohesive processing pipeline: `lambda_handler` orchestrates the workflow, calling `download_files` to prepare inputs, invoking SageMaker for equation detection, using `rank_eqn_similarity` (which leverages `image_to_latex_convert` and `levenshtein_distance`) to identify similar equations, and finally utilizing `draw_bounding_box` and `final_output` to visualize and deliver results.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "lambda_function.py",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "from constants import *\nimport os\nimport boto3\nimport json\nimport dataHandler\nimport subprocess\nimport os\n#import PyPDF2\nfrom PIL import Image, ImageDraw\nimport pdf2image\nimport cv2\nfrom sagemaker.pytorch import PyTorchPredictor\nfrom sagemaker.deserializers import JSONDeserializer\nimport traceback\nimport requests\nimport time\nimport Levenshtein\nfrom fpdf import FPDF\nprint(\"Finished imports\")\n\n# Initialize S3 client\ns3 = boto3.client('s3')\n\n\ndef levenshtein_distance(query_string, latex_list, top_n):\n  # elem of latex_list is (latex string, page num, eqn num)\n  ranked_list = []\n  n = len(latex_list)\n  for i in range(n):\n    latex1 = latex_list[i][0] # string is first element \n  \n    similarity_score = Levenshtein.distance(latex1, query_string)\n    ranked_list.append((latex_list[i][0], latex_list[i][1], latex_list[i][2], similarity_score))\n  \n  # Sort based on similarity score\n  ranked_list.sort(key=lambda x: x[3])\n  return ranked_list[:top_n]\n\n# Returns a well-formatted LaTeX string represent the equation image 'image'\n# Makes the MathPix API call\ndef image_to_latex_convert(image, query_bool):\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n      \n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type of img sent to mathpix {type(image)}, {query_bool}\")\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": image},\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200:\n        #print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"\n\n#print(\"Finished image_to_latex_convert\")\n\ndef downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path\n\n#print(\"Finished downloadDirectoryFroms3\")\n\ndef download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi=500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target\n\n#print(\"Finished download_files\")\n\n# Call draw_bounding_box on each PNG page of PDF\ndef draw_bounding_box(image_path_in, bounding_boxes):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  #SKYBLUE = (55,161,253)\n  GREEN = (32,191,95)\n  YELLOW = (255,225,101)\n\n  # create rectangle for each bounding box on this page\n  for bb, rank in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    if rank == 0: \n      draw.rectangle(xy=(x1, y1, x2, y2), outline=GREEN, width=8)\n    else:\n      draw.rectangle(xy=(x1, y1, x2, y2), outline=YELLOW, width=8)\n  \n  return image\n  # save img as pdf\n  #image.save(image_path_out[:-4]+\".pdf\")\n\n#print(\"Finished draw_bounding_box\")\n\ndef final_output(pdf_name, png_pdf_path, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values (list of bounding boxes, eqn rank)\n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  #pdf_out = PDF_OUT_DIR + pdf_name\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  #pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  #print(\"bounding boxes dict: \", bounding_boxes)\n\n  # call \"draw_bounding_boxes\" for each png page, save to IMG_OUT_DIR\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  paths = sorted(os.listdir(png_pdf_path))\n  pdf = FPDF()\n  for i in range(len(paths)-1):\n    #print(f\"adding {paths[i]}\")\n    if str(i) in result_pages:  \n      img = draw_bounding_box(paths[i], bounding_boxes[str(i)])\n      img.save(paths[i])\n    \n    pdf.add_page()\n    pdf.image(paths[i], 0, 0, 210, 297) # A4 paper sizing\n  pdf.output(pdf_out, \"F\")\n\n  # RESIZE_FACTOR = 0.25\n  # RESAMPLE_ALGO = Image.Resampling.LANCZOS\n  # pages = []\n  # with open(pdf_in, 'rb') as file: \n  #   pdf = PyPDF2.PdfReader(file)\n  #   for i, page in enumerate(pdf.pages):\n  #       image_path_in = IMG_IN_DIR + str(i) + \".png\"\n  #       if str(i) in result_pages:  \n  #         # pass in list of bounding boxes for each page\n  #         img = draw_bounding_box(image_path_in, bounding_boxes[str(i)])\n  #       else:\n  #         img = Image.open(image_path_in).convert('RGB')\n        \n  #       w, h = img.size\n  #       resized_image = img.resize((int(w*RESIZE_FACTOR), int(h*RESIZE_FACTOR)), resample=RESAMPLE_ALGO)\n  #       pages.append(resized_image)\n\n  # pages[0].save(pdf_out, save_all=True, append_images=pages[1:], format=\"PDF\")\n  \n  try:\n    s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n    print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n  except:\n    raise Exception(\"Upload failed\")\n\n#print(\"Finished final_output\")\n\n# Store string repr. of LaTeX equation and its page number in list\ndef rank_eqn_similarity(yolo_result, query_path, pdf_name):\n  with open(query_path, \"rb\") as f:\n    data = f.read()\n    query_text = image_to_latex_convert(data, query_bool=True)\n  query_text = query_text.replace(\" \", \"\")\n  print(f\"query_text: {query_text}\")\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    \n    total_eqns = 0\n    skipped_eqns = 0\n    for bboxes in dict_elem[\"boxes\"]:\n      total_eqns += 1\n      # crop from original iamge, and send that to MathPix\n      x1, y1, x2, y2, _, label = bboxes\n\n      # skip in-line equations (not skipping everything, but not sure if its correct)\n      if label > 0.0:\n        eqn_num += 1\n        skipped_eqns += 1\n        continue\n      \n      IMG_OUT_DIR = f\"/tmp/cropped_imgs_{pdf_name}/\"\n      subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n      subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n\n      crop_path = IMG_OUT_DIR + \"_p\"+ str(page_num) + \"_e\" + str(eqn_num) + \".png\"\n      page_png_path = f\"/tmp/converted_pdfs_{pdf_name}/\" + str(page_num) + \".png\"\n      model_width, model_height = 640,640\n      image = Image.open(page_png_path).convert('RGB')\n      width, height = image.size\n      x_ratio, y_ratio = width/model_width, height/model_height\n\n      # CROP original PNG with yolo bounding box coordinates\n      x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n      y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n      cropped_image = image.crop((x1, y1, x2, y2))\n      cropped_image.save(crop_path)\n      \n      latex_string = image_to_latex_convert(open(crop_path, \"rb\"), query_bool=False)\n      latex_string = latex_string.replace(\" \", \"\")\n      print(f\"{eqn_num} on {page_num}: {latex_string}\")\n      equations_list.append((latex_string, page_num, eqn_num))\n      eqn_num += 1\n    print(f\"page {page_num}: skipped {skipped_eqns} in-line eqns, out of {total_eqns}.\")\n    \n  print(\"Finished all MathPix API calls!\")\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with top_n smallest edit distances\n  top_n = 5\n  sorted_lst = levenshtein_distance(query_string=query_text, latex_list=equations_list, top_n=top_n)\n  print(\"most similar eqns: \", sorted_lst)\n  return sorted_lst\n\n#print(\"Finished parse_tree_similarity\")\n\ndef lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = rank_eqn_similarity(yolo_result=yolo_result, query_path=local_target, pdf_name=pdf_name)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted(set(([page_num for (latex_string, page_num, eqn_num, dist) in top5_eqns])))\n          top5_eqns_info = [(page_num, eqn_num) for (latex_string, page_num, eqn_num, dist) in top5_eqns]\n          #print(\"top_5_eqns_info \", top_5_eqns_info)\n\n          # sort by page number\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top5_eqns_info:\n                rank = top5_eqns_info.index((page_num, count))\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append((bboxes[:4], rank))\n                else:\n                  bboxes_dict[page_num] = [(bboxes[:4], rank)]\n              count += 1\n\n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, png_pdf_path, bboxes_dict)\n\n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = [int(p)+1 for p in page_nums_5]\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
      "summary": "# FILE: lambda_function.py\n\n## OVERVIEW:\nThis file implements a serverless AWS Lambda function that processes mathematical equation search requests by detecting equations in PDFs, converting them to LaTeX, and identifying those similar to a query image through Levenshtein distance comparison.\n\n## KEY COMPONENTS:\n- `lambda_handler`: Processes SQS messages containing PDF analysis requests, orchestrating the entire equation search workflow\n- `download_files`: Downloads PDF and query image from S3 and converts PDF to PNG images\n- `rank_eqn_similarity`: Compares equations in the PDF with the query image using OCR and Levenshtein distance\n- `image_to_latex_convert`: Converts mathematical expression images to LaTeX using the Mathpix API\n- `levenshtein_distance`: Calculates string similarity between LaTeX expressions to find matches\n- `draw_bounding_box`: Visualizes detected equations by drawing colored rectangles on the PDF page images\n- `final_output`: Creates an annotated PDF with bounding boxes around matched equations\n- `downloadDirectoryFroms3`: Retrieves entire directories from S3 buckets\n\n## ARCHITECTURE:\nThe system follows a pipeline architecture where incoming requests trigger file downloads, followed by equation detection using YOLO, conversion to LaTeX, similarity ranking, and finally visualization of results in an annotated PDF.\n\n## DATA FLOW:\n1. PDF and query image are downloaded from S3\n2. PDF is converted to PNG images for processing\n3. YOLOv8 model detects equation regions in the PDF pages\n4. Detected equations and query are converted to LaTeX format\n5. Levenshtein distance ranks equations by similarity to the query\n6. Top matches are highlighted with bounding boxes\n7. Annotated PDF is generated and uploaded to S3\n\n## INTEGRATION POINTS:\n- AWS Lambda environment for serverless execution\n- SQS for receiving processing requests\n- S3 for storing input PDFs, query images, and output results\n- SageMaker for YOLO model inference\n- Mathpix API for image-to-LaTeX conversion\n\n## USAGE PATTERNS:\n- PDF documents containing mathematical content are analyzed for equation extraction\n- Users submit query images of equations to find similar expressions within documents\n- Results are returned as annotated PDFs with highlighted matching equations\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3, SQS, and SageMaker operations\n- PIL/Pillow: Image processing for drawing bounding boxes\n- pdf2image: PDF to PNG conversion\n- requests: HTTP requests to Mathpix API\n- Levenshtein: String similarity calculation\n- FPDF: PDF generation\n- subprocess: System command execution\n- os/io: File system operations\n\n## RELATIONSHIPS:\nThe functions form a cohesive processing pipeline: `lambda_handler` orchestrates the workflow, calling `download_files` to prepare inputs, invoking SageMaker for equation detection, using `rank_eqn_similarity` (which leverages `image_to_latex_convert` and `levenshtein_distance`) to identify similar equations, and finally utilizing `draw_bounding_box` and `final_output` to visualize and deliver results."
    }
  },
  {
    "page_content": "# FUNCTION: levenshtein_distance\n\n## PURPOSE:\nCalculates the Levenshtein distance between a query string and a list of LaTeX expressions, returning the top N closest matches sorted by similarity.\n\n## INPUTS:\n- `query_string` (string): The LaTeX expression to find matches for\n- `latex_list` (list of tuples): List of LaTeX expressions where each tuple contains (latex_string, page_number, equation_number)\n- `top_n` (integer): Number of closest matches to return\n\n## OUTPUTS:\n- List of tuples: The top N closest matches, each containing (latex_string, page_number, equation_number, similarity_score), sorted by increasing Levenshtein distance\n\n## KEY STEPS:\n- Iterate through each LaTeX expression in the input list\n- Calculate the Levenshtein distance between each expression and the query string\n- Create a ranked list with the original information plus the distance score\n- Sort the list by the similarity score (lower is better)\n- Return the top N matches\n\n## DEPENDENCIES:\n- Levenshtein: External library providing the distance calculation method\n\n## USAGE CONTEXT:\nTypically used in LaTeX document search systems or equation retrieval applications where finding similar mathematical expressions is needed.\n\n## EDGE CASES:\n- If `latex_list` is empty, will return an empty list\n- If `top_n` is greater than the length of `latex_list`, will return the entire ranked list\n- Does not handle non-string inputs for comparison\n\n## RELATIONSHIPS:\nLikely part of a larger document processing or search system that involves LaTeX parsing and similarity matching for mathematical content.\n\ndef levenshtein_distance(query_string, latex_list, top_n):\n  # elem of latex_list is (latex string, page num, eqn num)\n  ranked_list = []\n  n = len(latex_list)\n  for i in range(n):\n    latex1 = latex_list[i][0] # string is first element \n  \n    similarity_score = Levenshtein.distance(latex1, query_string)\n    ranked_list.append((latex_list[i][0], latex_list[i][1], latex_list[i][2], similarity_score))\n  \n  # Sort based on similarity score\n  ranked_list.sort(key=lambda x: x[3])\n  return ranked_list[:top_n]",
    "metadata": {
      "type": "FUNCTION",
      "name": "levenshtein_distance",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def levenshtein_distance(query_string, latex_list, top_n):\n  # elem of latex_list is (latex string, page num, eqn num)\n  ranked_list = []\n  n = len(latex_list)\n  for i in range(n):\n    latex1 = latex_list[i][0] # string is first element \n  \n    similarity_score = Levenshtein.distance(latex1, query_string)\n    ranked_list.append((latex_list[i][0], latex_list[i][1], latex_list[i][2], similarity_score))\n  \n  # Sort based on similarity score\n  ranked_list.sort(key=lambda x: x[3])\n  return ranked_list[:top_n]",
      "summary": "# FUNCTION: levenshtein_distance\n\n## PURPOSE:\nCalculates the Levenshtein distance between a query string and a list of LaTeX expressions, returning the top N closest matches sorted by similarity.\n\n## INPUTS:\n- `query_string` (string): The LaTeX expression to find matches for\n- `latex_list` (list of tuples): List of LaTeX expressions where each tuple contains (latex_string, page_number, equation_number)\n- `top_n` (integer): Number of closest matches to return\n\n## OUTPUTS:\n- List of tuples: The top N closest matches, each containing (latex_string, page_number, equation_number, similarity_score), sorted by increasing Levenshtein distance\n\n## KEY STEPS:\n- Iterate through each LaTeX expression in the input list\n- Calculate the Levenshtein distance between each expression and the query string\n- Create a ranked list with the original information plus the distance score\n- Sort the list by the similarity score (lower is better)\n- Return the top N matches\n\n## DEPENDENCIES:\n- Levenshtein: External library providing the distance calculation method\n\n## USAGE CONTEXT:\nTypically used in LaTeX document search systems or equation retrieval applications where finding similar mathematical expressions is needed.\n\n## EDGE CASES:\n- If `latex_list` is empty, will return an empty list\n- If `top_n` is greater than the length of `latex_list`, will return the entire ranked list\n- Does not handle non-string inputs for comparison\n\n## RELATIONSHIPS:\nLikely part of a larger document processing or search system that involves LaTeX parsing and similarity matching for mathematical content."
    }
  },
  {
    "page_content": "# FUNCTION: image_to_latex_convert\n\n## PURPOSE:\nConverts an image containing mathematical expressions or equations into LaTeX code using the Mathpix API. This function enables translation of handwritten or image-based mathematical notation into structured LaTeX format.\n\n## INPUTS:\n- `image` (bytes): The image data containing mathematical expressions to be converted\n- `query_bool` (boolean): A flag parameter (though not utilized in the current implementation)\n\n## OUTPUTS:\n- `string`: The LaTeX representation of the mathematical expression from the image, or an empty string if conversion fails\n\n## KEY STEPS:\n- Configure the API request headers including app credentials\n- Define the format parameters for the LaTeX output (styled LaTeX with specific formatting options)\n- Send a POST request to the Mathpix API with the image file and format options\n- Process the API response to extract the LaTeX representation if successful\n- Return the extracted LaTeX code or an empty string if the request fails\n\n## DEPENDENCIES:\n- `requests`: For making HTTP requests to the Mathpix API\n- `json`: For parsing and formatting JSON data\n- `os`: For accessing environment variables (APP_KEY)\n- `io`: Though imported, not directly used in the current implementation\n\n## USAGE CONTEXT:\n- Used in applications requiring conversion of mathematical expressions from images to LaTeX\n- Typically employed in educational tools, document processing systems, or mathematical content management systems\n\n## EDGE CASES:\n- Returns an empty string if the API call fails\n- Logs error with status code to console on failure\n- Depends on environment variable APP_KEY being correctly set\n- No explicit error handling for network issues or invalid image formats\n\n## RELATIONSHIPS:\n- Interfaces with the Mathpix external API service\n- May be part of a larger document processing or mathematical content management system\n- Relies on authentication credentials stored in environment variables\n\ndef image_to_latex_convert(image, query_bool):\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n      \n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type of img sent to mathpix {type(image)}, {query_bool}\")\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": image},\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200:\n        #print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "image_to_latex_convert",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def image_to_latex_convert(image, query_bool):\n\n    # Hardcoded CDS account response headers (placeholders for now)\n    headers = {\n        \"app_id\": \"mathsearch_ff86f3_059645\",\n        \"app_key\": os.environ.get(\"APP_KEY\")\n    }\n      \n    # Declare api request payload (refer to https://docs.mathpix.com/?python#introduction for description)\n    data = {\n        \"formats\": [\"latex_styled\"], \n        \"rm_fonts\": True, \n        \"rm_spaces\": False,\n        \"idiomatic_braces\": True\n    }\n\n    #print(f\"type of img sent to mathpix {type(image)}, {query_bool}\")\n    #print(f\"type after buffered reader stuff {type(io.BufferedReader(io.BytesIO(image)))}\")\n    # assume that image is stored in bytes\n    response = requests.post(\"https://api.mathpix.com/v3/text\",\n                                files={\"file\": image},\n                                data={\"options_json\": json.dumps(data)},\n                                headers=headers)\n       \n    # Check if the request was successful\n    if response.status_code == 200:\n        #print(\"Successful API call!!\")\n        response_data = response.json()\n        #print(json.dumps(response_data, indent=4, sort_keys=True))  # Print formatted JSON response\n        return response_data.get(\"latex_styled\", \"\")  # Get the LaTeX representation from the response, safely access the key\n    else:\n        print(\"Failed to get LaTeX on API call. Status code:\", response.status_code)\n        return \"\"",
      "summary": "# FUNCTION: image_to_latex_convert\n\n## PURPOSE:\nConverts an image containing mathematical expressions or equations into LaTeX code using the Mathpix API. This function enables translation of handwritten or image-based mathematical notation into structured LaTeX format.\n\n## INPUTS:\n- `image` (bytes): The image data containing mathematical expressions to be converted\n- `query_bool` (boolean): A flag parameter (though not utilized in the current implementation)\n\n## OUTPUTS:\n- `string`: The LaTeX representation of the mathematical expression from the image, or an empty string if conversion fails\n\n## KEY STEPS:\n- Configure the API request headers including app credentials\n- Define the format parameters for the LaTeX output (styled LaTeX with specific formatting options)\n- Send a POST request to the Mathpix API with the image file and format options\n- Process the API response to extract the LaTeX representation if successful\n- Return the extracted LaTeX code or an empty string if the request fails\n\n## DEPENDENCIES:\n- `requests`: For making HTTP requests to the Mathpix API\n- `json`: For parsing and formatting JSON data\n- `os`: For accessing environment variables (APP_KEY)\n- `io`: Though imported, not directly used in the current implementation\n\n## USAGE CONTEXT:\n- Used in applications requiring conversion of mathematical expressions from images to LaTeX\n- Typically employed in educational tools, document processing systems, or mathematical content management systems\n\n## EDGE CASES:\n- Returns an empty string if the API call fails\n- Logs error with status code to console on failure\n- Depends on environment variable APP_KEY being correctly set\n- No explicit error handling for network issues or invalid image formats\n\n## RELATIONSHIPS:\n- Interfaces with the Mathpix external API service\n- May be part of a larger document processing or mathematical content management system\n- Relies on authentication credentials stored in environment variables"
    }
  },
  {
    "page_content": "# FUNCTION: downloadDirectoryFroms3\n\n## PURPOSE:\nDownloads an entire directory and its contents from an Amazon S3 bucket to the local filesystem, preserving the directory structure.\n\n## INPUTS:\n- `bucketName` (string): The name of the S3 bucket to download from\n- `remoteDirectoryName` (string): The path/prefix of the directory in the S3 bucket to download\n\n## OUTPUTS:\n- None (the function downloads files as a side effect but does not return a value)\n\n## KEY STEPS:\n- Initialize a connection to the S3 service\n- Get a reference to the specified bucket\n- Filter objects in the bucket by the specified directory prefix\n- For each matching object:\n  - Create local directories if they don't exist to match the S3 path structure\n  - Download the file to the same path locally as it had in S3\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- os: Standard library for filesystem operations\n\n## USAGE CONTEXT:\nTypically used in data processing workflows or backup operations where S3-stored data needs to be processed locally or when restoring backed-up files from S3 storage.\n\n## EDGE CASES:\n- Does not handle S3 access permission errors\n- No error handling for insufficient local disk space\n- May create empty directories if the S3 path contains directories without files\n- Will overwrite existing local files without warning\n\n## RELATIONSHIPS:\nLikely part of a larger data management or ETL system that interfaces with AWS S3 storage. Might be paired with corresponding upload functionality.\n\ndef downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path",
    "metadata": {
      "type": "FUNCTION",
      "name": "downloadDirectoryFroms3",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n    s3_resource = boto3.resource('s3')\n    bucket = s3_resource.Bucket(bucketName) \n    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n        if not os.path.exists(os.path.dirname(obj.key)):\n            os.makedirs(os.path.dirname(obj.key))\n        bucket.download_file(obj.key, obj.key) # save to same path",
      "summary": "# FUNCTION: downloadDirectoryFroms3\n\n## PURPOSE:\nDownloads an entire directory and its contents from an Amazon S3 bucket to the local filesystem, preserving the directory structure.\n\n## INPUTS:\n- `bucketName` (string): The name of the S3 bucket to download from\n- `remoteDirectoryName` (string): The path/prefix of the directory in the S3 bucket to download\n\n## OUTPUTS:\n- None (the function downloads files as a side effect but does not return a value)\n\n## KEY STEPS:\n- Initialize a connection to the S3 service\n- Get a reference to the specified bucket\n- Filter objects in the bucket by the specified directory prefix\n- For each matching object:\n  - Create local directories if they don't exist to match the S3 path structure\n  - Download the file to the same path locally as it had in S3\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- os: Standard library for filesystem operations\n\n## USAGE CONTEXT:\nTypically used in data processing workflows or backup operations where S3-stored data needs to be processed locally or when restoring backed-up files from S3 storage.\n\n## EDGE CASES:\n- Does not handle S3 access permission errors\n- No error handling for insufficient local disk space\n- May create empty directories if the S3 path contains directories without files\n- Will overwrite existing local files without warning\n\n## RELATIONSHIPS:\nLikely part of a larger data management or ETL system that interfaces with AWS S3 storage. Might be paired with corresponding upload functionality."
    }
  },
  {
    "page_content": "# FUNCTION: download_files\n\n## PURPOSE:\nDownloads a PDF and query image from an S3 bucket, converts the PDF to PNG images, and returns the local paths to the downloaded and converted files.\n\n## INPUTS:\n- pdf_name (string): Name of the PDF file to download from S3\n- query_name (string): Name of the query image to download from S3\n- png_converted_pdf_path (string): Base path where converted PNG images will be stored\n- pdfs_from_bucket_path (string): Directory path where downloaded PDFs will be stored\n\n## OUTPUTS:\n- local_pdf (string): Local path to the downloaded PDF file\n- converted_pdf_path (string): Path to the directory containing the converted PNG images\n- local_target (string): Local path to the downloaded query image\n\n## KEY STEPS:\n- Constructs local paths for storing the PDF and query image\n- Downloads the PDF from S3 bucket\n- Converts PDF to PNG images at 500 DPI\n- Creates a directory to store the converted PNG images\n- Saves each converted PDF page as a numbered PNG file\n- Downloads the query image from S3 bucket\n- Returns paths to the downloaded and converted files\n\n## DEPENDENCIES:\n- s3: AWS S3 client\n- BUCKET: Global variable containing the S3 bucket name\n- pdf2image: Library for converting PDFs to images\n- subprocess: Module for running shell commands\n\n## USAGE CONTEXT:\nTypically used in document processing pipelines that need to compare a query image with pages of a PDF document, preparing the files for image matching or analysis.\n\n## EDGE CASES:\n- No explicit error handling for S3 download failures\n- No validation for file existence or permissions\n- Assumes the directory structure exists or can be created with mkdir -p\n- May encounter issues with large PDFs due to memory constraints during conversion\n\n## RELATIONSHIPS:\nLikely part of a document processing system that performs image matching or search, preparing input files for subsequent analysis by other components in the pipeline.\n\ndef download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi=500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target",
    "metadata": {
      "type": "FUNCTION",
      "name": "download_files",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path):\n    \"\"\"\n    Returns path to PDF and query image after downloading PDF and query from the S3 bucket\n    \"\"\"\n    local_pdf = pdfs_from_bucket_path + \"/\" + pdf_name + \".pdf\"\n    local_target = png_converted_pdf_path+\"_\"+ pdf_name + \"/\" + \"query.png\"\n    print(\"local_pdf\",local_pdf)\n    print(\"pdf_name\",pdf_name)\n    print(\"local_target\", local_target)\n\n    # download and preprocess pdf to png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+pdf_name, Filename=local_pdf\n    )\n    \n    images = pdf2image.convert_from_path(local_pdf, dpi=500)\n    \n    # create directory to put the converted pngs into\n    subprocess.run(f'mkdir -p {png_converted_pdf_path}_{pdf_name}', shell=True)\n    for i in range(len(images)):\n        pdf_image = png_converted_pdf_path + \"_\" + pdf_name + \"/\"+ str(i) + \".png\"\n        images[i].save(pdf_image)\n    \n    # download query png\n    s3.download_file(\n        Bucket=BUCKET, Key=\"inputs/\"+query_name, Filename=local_target\n    )\n\n    # return paths to the pdf and query that we downloaded\n    return local_pdf, f\"{png_converted_pdf_path}_{pdf_name}\", local_target",
      "summary": "# FUNCTION: download_files\n\n## PURPOSE:\nDownloads a PDF and query image from an S3 bucket, converts the PDF to PNG images, and returns the local paths to the downloaded and converted files.\n\n## INPUTS:\n- pdf_name (string): Name of the PDF file to download from S3\n- query_name (string): Name of the query image to download from S3\n- png_converted_pdf_path (string): Base path where converted PNG images will be stored\n- pdfs_from_bucket_path (string): Directory path where downloaded PDFs will be stored\n\n## OUTPUTS:\n- local_pdf (string): Local path to the downloaded PDF file\n- converted_pdf_path (string): Path to the directory containing the converted PNG images\n- local_target (string): Local path to the downloaded query image\n\n## KEY STEPS:\n- Constructs local paths for storing the PDF and query image\n- Downloads the PDF from S3 bucket\n- Converts PDF to PNG images at 500 DPI\n- Creates a directory to store the converted PNG images\n- Saves each converted PDF page as a numbered PNG file\n- Downloads the query image from S3 bucket\n- Returns paths to the downloaded and converted files\n\n## DEPENDENCIES:\n- s3: AWS S3 client\n- BUCKET: Global variable containing the S3 bucket name\n- pdf2image: Library for converting PDFs to images\n- subprocess: Module for running shell commands\n\n## USAGE CONTEXT:\nTypically used in document processing pipelines that need to compare a query image with pages of a PDF document, preparing the files for image matching or analysis.\n\n## EDGE CASES:\n- No explicit error handling for S3 download failures\n- No validation for file existence or permissions\n- Assumes the directory structure exists or can be created with mkdir -p\n- May encounter issues with large PDFs due to memory constraints during conversion\n\n## RELATIONSHIPS:\nLikely part of a document processing system that performs image matching or search, preparing input files for subsequent analysis by other components in the pipeline."
    }
  },
  {
    "page_content": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws colored rectangular bounding boxes on an image, typically used to highlight regions of interest on a page extracted from a PDF document.\n\n## INPUTS:\n- `image_path_in` (str): Path to a PNG image that represents a page from a PDF\n- `bounding_boxes` (list): List of bounding box coordinates and their ranking, where each item is a tuple of (coordinates, rank)\n\n## OUTPUTS:\n- `image` (PIL.Image): The original image with bounding boxes drawn on it\n\n## KEY STEPS:\n- Open the specified image and convert it to RGB format\n- Calculate scaling ratios to handle differences between model and image dimensions\n- Iterate through each bounding box in the list\n- Scale the coordinates based on the calculated ratios\n- Draw rectangles with different colors based on the rank (green for rank 0, yellow for others)\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library): Uses Image and ImageDraw modules\n\n## USAGE CONTEXT:\nTypically used in document analysis pipelines, especially after running object detection models that identify regions of interest on PDF pages.\n\n## EDGE CASES:\n- No explicit error handling for invalid image paths or malformed bounding box data\n- May produce unexpected results if input image dimensions are extremely small\n\n## RELATIONSHIPS:\nLikely part of a document processing system, used after object detection or region identification steps and before further processing of the highlighted regions.\n\ndef draw_bounding_box(image_path_in, bounding_boxes):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  #SKYBLUE = (55,161,253)\n  GREEN = (32,191,95)\n  YELLOW = (255,225,101)\n\n  # create rectangle for each bounding box on this page\n  for bb, rank in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    if rank == 0: \n      draw.rectangle(xy=(x1, y1, x2, y2), outline=GREEN, width=8)\n    else:\n      draw.rectangle(xy=(x1, y1, x2, y2), outline=YELLOW, width=8)\n  \n  return image",
    "metadata": {
      "type": "FUNCTION",
      "name": "draw_bounding_box",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def draw_bounding_box(image_path_in, bounding_boxes):\n  \"\"\"\"\n  image_path_in : path to PNG which represents page from pdf\n  bounding_boxes: list of list of bounding boxes\n  \"\"\"\n  model_width, model_height = 640,640\n  image = Image.open(image_path_in).convert('RGB')\n  draw = ImageDraw.Draw(image)\n  width, height = image.size\n  x_ratio, y_ratio = width/model_width, height/model_height\n  #SKYBLUE = (55,161,253)\n  GREEN = (32,191,95)\n  YELLOW = (255,225,101)\n\n  # create rectangle for each bounding box on this page\n  for bb, rank in bounding_boxes:\n    x1, y1, x2, y2 = bb\n    x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n    y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n    if rank == 0: \n      draw.rectangle(xy=(x1, y1, x2, y2), outline=GREEN, width=8)\n    else:\n      draw.rectangle(xy=(x1, y1, x2, y2), outline=YELLOW, width=8)\n  \n  return image",
      "summary": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws colored rectangular bounding boxes on an image, typically used to highlight regions of interest on a page extracted from a PDF document.\n\n## INPUTS:\n- `image_path_in` (str): Path to a PNG image that represents a page from a PDF\n- `bounding_boxes` (list): List of bounding box coordinates and their ranking, where each item is a tuple of (coordinates, rank)\n\n## OUTPUTS:\n- `image` (PIL.Image): The original image with bounding boxes drawn on it\n\n## KEY STEPS:\n- Open the specified image and convert it to RGB format\n- Calculate scaling ratios to handle differences between model and image dimensions\n- Iterate through each bounding box in the list\n- Scale the coordinates based on the calculated ratios\n- Draw rectangles with different colors based on the rank (green for rank 0, yellow for others)\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library): Uses Image and ImageDraw modules\n\n## USAGE CONTEXT:\nTypically used in document analysis pipelines, especially after running object detection models that identify regions of interest on PDF pages.\n\n## EDGE CASES:\n- No explicit error handling for invalid image paths or malformed bounding box data\n- May produce unexpected results if input image dimensions are extremely small\n\n## RELATIONSHIPS:\nLikely part of a document processing system, used after object detection or region identification steps and before further processing of the highlighted regions."
    }
  },
  {
    "page_content": "# FUNCTION: final_output\n\n## PURPOSE:\nProcesses a PDF file by adding bounding boxes around detected equations and generates a new annotated PDF. The function is responsible for the final output preparation in an equation detection system.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to process\n- `png_pdf_path` (string): Path to directory containing PNG conversions of PDF pages\n- `bounding_boxes` (dict): Dictionary with page numbers as keys and lists of bounding boxes with equation rankings as values\n\n## OUTPUTS:\n- No explicit return value, but creates an annotated PDF file and uploads it to an S3 bucket\n\n## KEY STEPS:\n- Set up temporary directories for input/output files\n- Identify which pages have detected equations\n- Process each page by adding bounding boxes to pages with equation detections using the `draw_bounding_box` function\n- Create a new PDF from the annotated images\n- Upload the final PDF to an S3 bucket\n\n## DEPENDENCIES:\n- `subprocess`: For executing shell commands\n- `os`: For file system operations\n- `FPDF`: For PDF creation\n- `draw_bounding_box`: Function to add visual bounding boxes to images\n- `s3`: AWS S3 client for file upload\n- `OUTPUT_BUCKET`: Global variable defining the S3 bucket destination\n\n## USAGE CONTEXT:\nUsed as the final step in an equation detection pipeline where the input PDF has been processed to identify equations, and the results need to be visually presented in a new PDF.\n\n## EDGE CASES:\n- Handles S3 upload failures by raising an exception\n- Commented-out code suggests alternative implementation using PyPDF2 and image resizing\n- Assumes temporary directories can be created and existing ones can be removed\n\n## RELATIONSHIPS:\n- Works downstream of equation detection algorithms that produce bounding boxes\n- Depends on `draw_bounding_box` function to visualize results\n- Interacts with cloud storage (S3) for final output delivery\n\ndef final_output(pdf_name, png_pdf_path, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values (list of bounding boxes, eqn rank)\n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  #pdf_out = PDF_OUT_DIR + pdf_name\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  #pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  #print(\"bounding boxes dict: \", bounding_boxes)\n\n  # call \"draw_bounding_boxes\" for each png page, save to IMG_OUT_DIR\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  paths = sorted(os.listdir(png_pdf_path))\n  pdf = FPDF()\n  for i in range(len(paths)-1):\n    #print(f\"adding {paths[i]}\")\n    if str(i) in result_pages:  \n      img = draw_bounding_box(paths[i], bounding_boxes[str(i)])\n      img.save(paths[i])\n    \n    pdf.add_page()\n    pdf.image(paths[i], 0, 0, 210, 297) # A4 paper sizing\n  pdf.output(pdf_out, \"F\")\n\n  # RESIZE_FACTOR = 0.25\n  # RESAMPLE_ALGO = Image.Resampling.LANCZOS\n  # pages = []\n  # with open(pdf_in, 'rb') as file: \n  #   pdf = PyPDF2.PdfReader(file)\n  #   for i, page in enumerate(pdf.pages):\n  #       image_path_in = IMG_IN_DIR + str(i) + \".png\"\n  #       if str(i) in result_pages:  \n  #         # pass in list of bounding boxes for each page\n  #         img = draw_bounding_box(image_path_in, bounding_boxes[str(i)])\n  #       else:\n  #         img = Image.open(image_path_in).convert('RGB')\n        \n  #       w, h = img.size\n  #       resized_image = img.resize((int(w*RESIZE_FACTOR), int(h*RESIZE_FACTOR)), resample=RESAMPLE_ALGO)\n  #       pages.append(resized_image)\n\n  # pages[0].save(pdf_out, save_all=True, append_images=pages[1:], format=\"PDF\")\n  \n  try:\n    s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n    print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n  except:\n    raise Exception(\"Upload failed\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "final_output",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def final_output(pdf_name, png_pdf_path, bounding_boxes):\n  \"\"\"\n  bounding_boxes : dict with keys page numbers, and values (list of bounding boxes, eqn rank)\n  \"\"\"\n  IMG_IN_DIR = f\"/tmp/converted_pdfs_{pdf_name}/\"\n  IMG_OUT_DIR = \"/tmp/img_out/\"\n  subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n  \n  PDF_IN_DIR = \"/tmp/pdfs_from_bucket/\"\n  PDF_OUT_DIR = \"/tmp/pdf_out/\"\n  subprocess.run([\"rm\", \"-rf\", PDF_OUT_DIR])\n  subprocess.run([\"mkdir\", \"-p\", PDF_OUT_DIR])\n\n  pdf_in = PDF_IN_DIR + pdf_name + \".pdf\"\n  #pdf_out = PDF_OUT_DIR + pdf_name\n  pdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n  #pdf_no_ext = pdf_name[:-4]\n\n  result_pages = list(bounding_boxes.keys())\n  #print(\"bounding boxes dict: \", bounding_boxes)\n\n  # call \"draw_bounding_boxes\" for each png page, save to IMG_OUT_DIR\n  # merge the rendered images (with bounding boxes) to the pdf, and upload to S3\n  paths = sorted(os.listdir(png_pdf_path))\n  pdf = FPDF()\n  for i in range(len(paths)-1):\n    #print(f\"adding {paths[i]}\")\n    if str(i) in result_pages:  \n      img = draw_bounding_box(paths[i], bounding_boxes[str(i)])\n      img.save(paths[i])\n    \n    pdf.add_page()\n    pdf.image(paths[i], 0, 0, 210, 297) # A4 paper sizing\n  pdf.output(pdf_out, \"F\")\n\n  # RESIZE_FACTOR = 0.25\n  # RESAMPLE_ALGO = Image.Resampling.LANCZOS\n  # pages = []\n  # with open(pdf_in, 'rb') as file: \n  #   pdf = PyPDF2.PdfReader(file)\n  #   for i, page in enumerate(pdf.pages):\n  #       image_path_in = IMG_IN_DIR + str(i) + \".png\"\n  #       if str(i) in result_pages:  \n  #         # pass in list of bounding boxes for each page\n  #         img = draw_bounding_box(image_path_in, bounding_boxes[str(i)])\n  #       else:\n  #         img = Image.open(image_path_in).convert('RGB')\n        \n  #       w, h = img.size\n  #       resized_image = img.resize((int(w*RESIZE_FACTOR), int(h*RESIZE_FACTOR)), resample=RESAMPLE_ALGO)\n  #       pages.append(resized_image)\n\n  # pages[0].save(pdf_out, save_all=True, append_images=pages[1:], format=\"PDF\")\n  \n  try:\n    s3.upload_file(pdf_out, OUTPUT_BUCKET, pdf_name[:-4]+\".pdf\")\n    print(f\"merged final pdf, uploaded {pdf_out} to {OUTPUT_BUCKET}\")\n  except:\n    raise Exception(\"Upload failed\")",
      "summary": "# FUNCTION: final_output\n\n## PURPOSE:\nProcesses a PDF file by adding bounding boxes around detected equations and generates a new annotated PDF. The function is responsible for the final output preparation in an equation detection system.\n\n## INPUTS:\n- `pdf_name` (string): Name of the PDF file to process\n- `png_pdf_path` (string): Path to directory containing PNG conversions of PDF pages\n- `bounding_boxes` (dict): Dictionary with page numbers as keys and lists of bounding boxes with equation rankings as values\n\n## OUTPUTS:\n- No explicit return value, but creates an annotated PDF file and uploads it to an S3 bucket\n\n## KEY STEPS:\n- Set up temporary directories for input/output files\n- Identify which pages have detected equations\n- Process each page by adding bounding boxes to pages with equation detections using the `draw_bounding_box` function\n- Create a new PDF from the annotated images\n- Upload the final PDF to an S3 bucket\n\n## DEPENDENCIES:\n- `subprocess`: For executing shell commands\n- `os`: For file system operations\n- `FPDF`: For PDF creation\n- `draw_bounding_box`: Function to add visual bounding boxes to images\n- `s3`: AWS S3 client for file upload\n- `OUTPUT_BUCKET`: Global variable defining the S3 bucket destination\n\n## USAGE CONTEXT:\nUsed as the final step in an equation detection pipeline where the input PDF has been processed to identify equations, and the results need to be visually presented in a new PDF.\n\n## EDGE CASES:\n- Handles S3 upload failures by raising an exception\n- Commented-out code suggests alternative implementation using PyPDF2 and image resizing\n- Assumes temporary directories can be created and existing ones can be removed\n\n## RELATIONSHIPS:\n- Works downstream of equation detection algorithms that produce bounding boxes\n- Depends on `draw_bounding_box` function to visualize results\n- Interacts with cloud storage (S3) for final output delivery"
    }
  },
  {
    "page_content": "# FUNCTION: rank_eqn_similarity\n\n## PURPOSE:\nRanks equations from a PDF document based on their similarity to a query equation, using optical character recognition to convert images to LaTeX and Levenshtein distance to measure similarity.\n\n## INPUTS:\n- `yolo_result`: list of tuples - Contains detected equation bounding boxes and page numbers from YOLO model\n- `query_path`: string - File path to the query equation image\n- `pdf_name`: string - Name of the PDF document being analyzed\n\n## OUTPUTS:\n- list of tuples - Top N most similar equations with their similarity scores, page numbers, and equation numbers\n\n## KEY STEPS:\n- Convert query image to LaTeX using OCR\n- For each equation detected by YOLO:\n  - Skip inline equations (based on label values)\n  - Crop the equation from the original page image\n  - Convert cropped equation to LaTeX using OCR\n  - Add equation text, page number, and equation number to list\n- Calculate Levenshtein distance between query and all equations\n- Return top N equations with smallest edit distances\n\n## DEPENDENCIES:\n- `image_to_latex_convert` - Function to convert images to LaTeX\n- `levenshtein_distance` - Function to calculate string similarity\n- `PIL.Image` - For image processing\n- `subprocess` - For file system operations\n\n## USAGE CONTEXT:\nUsed in document search systems to find equations in PDFs that match a user's query equation, typically as part of a scientific or mathematical document retrieval workflow.\n\n## EDGE CASES:\n- Handles inline equations differently by skipping them based on label values\n- Creates temporary directories for processing images\n- Relies on external API calls which may fail or have rate limits\n\n## RELATIONSHIPS:\n- Works with output from YOLO object detection model\n- Part of a document processing pipeline for mathematical content retrieval\n- Connects image processing with string similarity measurement\n\ndef rank_eqn_similarity(yolo_result, query_path, pdf_name):\n  with open(query_path, \"rb\") as f:\n    data = f.read()\n    query_text = image_to_latex_convert(data, query_bool=True)\n  query_text = query_text.replace(\" \", \"\")\n  print(f\"query_text: {query_text}\")\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    \n    total_eqns = 0\n    skipped_eqns = 0\n    for bboxes in dict_elem[\"boxes\"]:\n      total_eqns += 1\n      # crop from original iamge, and send that to MathPix\n      x1, y1, x2, y2, _, label = bboxes\n\n      # skip in-line equations (not skipping everything, but not sure if its correct)\n      if label > 0.0:\n        eqn_num += 1\n        skipped_eqns += 1\n        continue\n      \n      IMG_OUT_DIR = f\"/tmp/cropped_imgs_{pdf_name}/\"\n      subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n      subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n\n      crop_path = IMG_OUT_DIR + \"_p\"+ str(page_num) + \"_e\" + str(eqn_num) + \".png\"\n      page_png_path = f\"/tmp/converted_pdfs_{pdf_name}/\" + str(page_num) + \".png\"\n      model_width, model_height = 640,640\n      image = Image.open(page_png_path).convert('RGB')\n      width, height = image.size\n      x_ratio, y_ratio = width/model_width, height/model_height\n\n      # CROP original PNG with yolo bounding box coordinates\n      x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n      y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n      cropped_image = image.crop((x1, y1, x2, y2))\n      cropped_image.save(crop_path)\n      \n      latex_string = image_to_latex_convert(open(crop_path, \"rb\"), query_bool=False)\n      latex_string = latex_string.replace(\" \", \"\")\n      print(f\"{eqn_num} on {page_num}: {latex_string}\")\n      equations_list.append((latex_string, page_num, eqn_num))\n      eqn_num += 1\n    print(f\"page {page_num}: skipped {skipped_eqns} in-line eqns, out of {total_eqns}.\")\n    \n  print(\"Finished all MathPix API calls!\")\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with top_n smallest edit distances\n  top_n = 5\n  sorted_lst = levenshtein_distance(query_string=query_text, latex_list=equations_list, top_n=top_n)\n  print(\"most similar eqns: \", sorted_lst)\n  return sorted_lst",
    "metadata": {
      "type": "FUNCTION",
      "name": "rank_eqn_similarity",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def rank_eqn_similarity(yolo_result, query_path, pdf_name):\n  with open(query_path, \"rb\") as f:\n    data = f.read()\n    query_text = image_to_latex_convert(data, query_bool=True)\n  query_text = query_text.replace(\" \", \"\")\n  print(f\"query_text: {query_text}\")\n      \n  equations_list = []\n  for dict_elem, page_num in yolo_result:\n    eqn_num = 1\n    \n    total_eqns = 0\n    skipped_eqns = 0\n    for bboxes in dict_elem[\"boxes\"]:\n      total_eqns += 1\n      # crop from original iamge, and send that to MathPix\n      x1, y1, x2, y2, _, label = bboxes\n\n      # skip in-line equations (not skipping everything, but not sure if its correct)\n      if label > 0.0:\n        eqn_num += 1\n        skipped_eqns += 1\n        continue\n      \n      IMG_OUT_DIR = f\"/tmp/cropped_imgs_{pdf_name}/\"\n      subprocess.run([\"rm\", \"-rf\", IMG_OUT_DIR])\n      subprocess.run([\"mkdir\", \"-p\", IMG_OUT_DIR])\n\n      crop_path = IMG_OUT_DIR + \"_p\"+ str(page_num) + \"_e\" + str(eqn_num) + \".png\"\n      page_png_path = f\"/tmp/converted_pdfs_{pdf_name}/\" + str(page_num) + \".png\"\n      model_width, model_height = 640,640\n      image = Image.open(page_png_path).convert('RGB')\n      width, height = image.size\n      x_ratio, y_ratio = width/model_width, height/model_height\n\n      # CROP original PNG with yolo bounding box coordinates\n      x1, x2 = int(x_ratio*x1), int(x_ratio*x2)\n      y1, y2 = int(y_ratio*y1), int(y_ratio*y2)\n      cropped_image = image.crop((x1, y1, x2, y2))\n      cropped_image.save(crop_path)\n      \n      latex_string = image_to_latex_convert(open(crop_path, \"rb\"), query_bool=False)\n      latex_string = latex_string.replace(\" \", \"\")\n      print(f\"{eqn_num} on {page_num}: {latex_string}\")\n      equations_list.append((latex_string, page_num, eqn_num))\n      eqn_num += 1\n    print(f\"page {page_num}: skipped {skipped_eqns} in-line eqns, out of {total_eqns}.\")\n    \n  print(\"Finished all MathPix API calls!\")\n\n  # sort equations by second element in tuple i.e. edit_dist_from_query\n  # return equations with top_n smallest edit distances\n  top_n = 5\n  sorted_lst = levenshtein_distance(query_string=query_text, latex_list=equations_list, top_n=top_n)\n  print(\"most similar eqns: \", sorted_lst)\n  return sorted_lst",
      "summary": "# FUNCTION: rank_eqn_similarity\n\n## PURPOSE:\nRanks equations from a PDF document based on their similarity to a query equation, using optical character recognition to convert images to LaTeX and Levenshtein distance to measure similarity.\n\n## INPUTS:\n- `yolo_result`: list of tuples - Contains detected equation bounding boxes and page numbers from YOLO model\n- `query_path`: string - File path to the query equation image\n- `pdf_name`: string - Name of the PDF document being analyzed\n\n## OUTPUTS:\n- list of tuples - Top N most similar equations with their similarity scores, page numbers, and equation numbers\n\n## KEY STEPS:\n- Convert query image to LaTeX using OCR\n- For each equation detected by YOLO:\n  - Skip inline equations (based on label values)\n  - Crop the equation from the original page image\n  - Convert cropped equation to LaTeX using OCR\n  - Add equation text, page number, and equation number to list\n- Calculate Levenshtein distance between query and all equations\n- Return top N equations with smallest edit distances\n\n## DEPENDENCIES:\n- `image_to_latex_convert` - Function to convert images to LaTeX\n- `levenshtein_distance` - Function to calculate string similarity\n- `PIL.Image` - For image processing\n- `subprocess` - For file system operations\n\n## USAGE CONTEXT:\nUsed in document search systems to find equations in PDFs that match a user's query equation, typically as part of a scientific or mathematical document retrieval workflow.\n\n## EDGE CASES:\n- Handles inline equations differently by skipping them based on label values\n- Creates temporary directories for processing images\n- Relies on external API calls which may fail or have rate limits\n\n## RELATIONSHIPS:\n- Works with output from YOLO object detection model\n- Part of a document processing pipeline for mathematical content retrieval\n- Connects image processing with string similarity measurement"
    }
  },
  {
    "page_content": "# FUNCTION: lambda_handler\n\n## PURPOSE:\nProcesses mathematical equation search requests in a serverless environment by analyzing PDFs and query images, identifying and ranking equations similar to the query, and returning the results with bounding boxes for visualization.\n\n## INPUTS:\n- `event` (dict): AWS Lambda event object containing SQS message with S3 trigger information\n- `context` (object): AWS Lambda context object providing runtime information\n\n## OUTPUTS:\n- `dict`: JSON response containing:\n  - `statusCode` (int): HTTP status code (200 for success, 400 for error)\n  - `body` (str): Description of the outcome\n  - `id` (str): UUID of the request\n  - `pdf` (str): PDF filename\n  - `pages` (list): Page numbers containing matching equations (sorted by relevance)\n  - `bbox` (dict): Bounding box coordinates for matched equations\n  - `error` (str, optional): Error message if processing failed\n\n## KEY STEPS:\n- Initialize data handler and retrieve file information from SQS message\n- Check if associated image file exists in S3 bucket\n- Download PDF and query image from S3\n- Call SageMaker endpoint to run YOLOv8 model for equation detection\n- Calculate similarity between detected equations and query image\n- Rank equations by similarity and identify top 5 matches\n- Draw bounding boxes around matched equations in PDF\n- Prepare and return JSON result with matched equations and locations\n- Clean up message from SQS queue and upload results to output bucket\n\n## DEPENDENCIES:\n- `dataHandler`: Custom module for S3/SQS operations\n- `boto3`: AWS SDK for Python\n- `json`: JSON parsing library\n- `cv2`: OpenCV for image processing\n- `PyTorchPredictor`: SageMaker prediction client\n- `JSONDeserializer`: Deserializer for SageMaker responses\n- `rank_eqn_similarity`: Function to compare equations with query\n- `download_files`, `final_output`: Helper functions for file operations\n\n## USAGE CONTEXT:\nUsed as an AWS Lambda function that processes SQS messages triggered by S3 uploads. Part of a serverless mathematical equation search pipeline that helps users find similar equations within PDF documents.\n\n## EDGE CASES:\n- Handles missing SageMaker endpoints with appropriate error response\n- Cleans temporary storage before processing to prevent space issues\n- Wraps entire execution in try/except to capture and report any errors\n- Ensures proper resource cleanup via SQS message deletion\n\n## RELATIONSHIPS:\n- Triggered by SQS messages from upstream file uploads\n- Interfaces with SageMaker for ML inference\n- Outputs processed results to S3 for downstream consumption\n- Part of a larger mathematical document search system\n\ndef lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = rank_eqn_similarity(yolo_result=yolo_result, query_path=local_target, pdf_name=pdf_name)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted(set(([page_num for (latex_string, page_num, eqn_num, dist) in top5_eqns])))\n          top5_eqns_info = [(page_num, eqn_num) for (latex_string, page_num, eqn_num, dist) in top5_eqns]\n          #print(\"top_5_eqns_info \", top_5_eqns_info)\n\n          # sort by page number\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top5_eqns_info:\n                rank = top5_eqns_info.index((page_num, count))\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append((bboxes[:4], rank))\n                else:\n                  bboxes_dict[page_num] = [(bboxes[:4], rank)]\n              count += 1\n\n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, png_pdf_path, bboxes_dict)\n\n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = [int(p)+1 for p in page_nums_5]\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
    "metadata": {
      "type": "FUNCTION",
      "name": "lambda_handler",
      "path": "../mathsearch/lambda-container/lambda_function.py",
      "code": "def lambda_handler(event, context):\n  try:\n      print(\"Running backend...\")\n      handler = dataHandler.DataHandler()\n      objects = handler.list_s3_objects(\"mathsearch-intermediary\")\n\n      body = json.loads(event['Records'][0]['body'])\n      receipt_handle = event['Records'][0]['receiptHandle']\n      file = body['Records'][0]['s3']['object']['key']\n      print(\"File name: \", file)\n\n      uuid = handler.extract_uuid(file)\n      expected_image = f'{uuid}_image'\n\n      if handler.is_expected_image_present(objects, expected_image):\n          print('Found image, run ML model')\n      \n          # clear tmp folder before running the ML model\n          subprocess.call('rm -rf /tmp/*', shell=True)\n\n          # folders which we download S3 bucket PDF to\n          png_converted_pdf_path = \"/tmp/converted_pdfs\"\n          pdfs_from_bucket_path = \"/tmp/pdfs_from_bucket\"\n          yolo_crops_path = \"/tmp/crops/\"\n\n          # create the pdfs_from_bucket directory if it doesn't exist\n          subprocess.run(f'mkdir -p {pdfs_from_bucket_path}', shell=True, cwd=\"/tmp\")\n          subprocess.run(f'mkdir -p {yolo_crops_path}', shell=True, cwd=\"/tmp\")\n\n          pdf_name = uuid+\"_pdf\"\n          query_name = uuid+\"_image\"\n          local_pdf, png_pdf_path, local_target = download_files(pdf_name, query_name, png_converted_pdf_path, pdfs_from_bucket_path)\n\n          ## CALL TO SAGEMAKER TO RUN YOLO\n          sm_client = boto3.client(service_name=\"sagemaker\")\n          ENDPOINT_NAME = \"mathsearch-yolov8-production-v1\"\n          endpoint_created = False\n          # start_time = time.time()\n          response = sm_client.list_endpoints()\n          for ep in response['Endpoints']:\n              print(f\"Endpoint Status = {ep['EndpointStatus']}\")\n              if ep['EndpointName']==ENDPOINT_NAME and ep['EndpointStatus']=='InService':\n                  endpoint_created = True\n\n          # return error if endpoint not created successfully\n          if not endpoint_created:\n            return {\n              'statusCode': 400,\n              'body': json.dumps('Error with Sagemaker Endpoint'),\n              'error': str(\"Error with Sagemaker Endpoint\")\n            }\n\n          predictor = PyTorchPredictor(endpoint_name=ENDPOINT_NAME,\n                            deserializer=JSONDeserializer())\n\n          print(\"Sending to Sagemaker...\")\n          yolo_result = []\n          os.chdir(png_converted_pdf_path+\"_\"+ pdf_name)\n          infer_start_time = time.time()\n          for file in os.listdir(png_pdf_path):\n            # don't need to run SageMaker on query.png\n            if file == \"query.png\": continue\n\n            print(f\"Processing {file}\")\n            \n            orig_image = cv2.imread(file)\n            model_height, model_width = 640, 640\n\n            resized_image = cv2.resize(orig_image, (model_height, model_width))\n            payload = cv2.imencode('.png', resized_image)[1].tobytes()\n\n            page_num = file.split(\".\")[0]\n            yolo_result.append((predictor.predict(payload), page_num))\n          infer_end_time = time.time()\n          print(f\"Sagemaker Inference Time = {infer_end_time - infer_start_time:0.4f} seconds\")\n\n          print(\"Sagemaker results received!\")\n          print(f\"Length of Sagemaker results: {len(yolo_result[0])}\")\n          print(yolo_result)\n\n          top5_eqns = rank_eqn_similarity(yolo_result=yolo_result, query_path=local_target, pdf_name=pdf_name)\n          print(\"MathPix API calls completed, and tree similarity generated!\")\n\n          page_nums_5 = sorted(set(([page_num for (latex_string, page_num, eqn_num, dist) in top5_eqns])))\n          top5_eqns_info = [(page_num, eqn_num) for (latex_string, page_num, eqn_num, dist) in top5_eqns]\n          #print(\"top_5_eqns_info \", top_5_eqns_info)\n\n          # sort by page number\n          bboxes_dict = {}\n          for dict_elem, page_num in yolo_result:\n            # don't draw bounding boxes on pages that don't have top 5 equation\n            if page_num not in page_nums_5:\n              continue\n            count = 1\n            for bboxes in dict_elem[\"boxes\"]:\n              # only collect bounding boxes from top 5 equation\n              if (page_num, count) in top5_eqns_info:\n                rank = top5_eqns_info.index((page_num, count))\n                if page_num in bboxes_dict.keys():\n                  bboxes_dict[page_num].append((bboxes[:4], rank))\n                else:\n                  bboxes_dict[page_num] = [(bboxes[:4], rank)]\n              count += 1\n\n          # draws the bounding boxes for the top 5 equations and converts pages back to PDF\n          # final PDF with bounding boxes saved in directory pdf_out\n          final_output(pdf_name, png_pdf_path, bboxes_dict)\n\n          # return JSON with the following keys\n          # id: UUID\n          # pdf : path / pdf name\n          # pages : list of page numbers sorted in order of most to least similar to query []\n          # bbox: list of tuples (page_num, [list of equation label + four coordinates of bounding box])\n          pages = [int(p)+1 for p in page_nums_5]\n          json_result = {\"statusCode\" : 200, \"body\": \"Successfully queried and processed your document!\", \n                        \"id\": uuid, \"pdf\": pdf_name, \"pages\": pages, \"bbox\": bboxes}\n            \n          print(f\"final json_result {json_result}\")\n      \n      # Dequeue from SQS\n      handler.delete_sqs_message(QUEUE_URL, receipt_handle)\n\n      # Upload json_result to OUTPUT_BUCKET\n      with open(f\"/tmp/{uuid}_results.json\", \"w\") as outfile: \n        json.dump(json_result, outfile)\n\n      s3.upload_file(f\"/tmp/{uuid}_results.json\", OUTPUT_BUCKET, f\"{uuid}_results.json\")\n      return json_result\n       \n  except:\n    exception = traceback.format_exc()\n    print(f\"Error: {exception}\")\n    return {\n        'statusCode': 400,\n        'body': json.dumps(f'Error processing the document.'),\n        'error': exception\n    }",
      "summary": "# FUNCTION: lambda_handler\n\n## PURPOSE:\nProcesses mathematical equation search requests in a serverless environment by analyzing PDFs and query images, identifying and ranking equations similar to the query, and returning the results with bounding boxes for visualization.\n\n## INPUTS:\n- `event` (dict): AWS Lambda event object containing SQS message with S3 trigger information\n- `context` (object): AWS Lambda context object providing runtime information\n\n## OUTPUTS:\n- `dict`: JSON response containing:\n  - `statusCode` (int): HTTP status code (200 for success, 400 for error)\n  - `body` (str): Description of the outcome\n  - `id` (str): UUID of the request\n  - `pdf` (str): PDF filename\n  - `pages` (list): Page numbers containing matching equations (sorted by relevance)\n  - `bbox` (dict): Bounding box coordinates for matched equations\n  - `error` (str, optional): Error message if processing failed\n\n## KEY STEPS:\n- Initialize data handler and retrieve file information from SQS message\n- Check if associated image file exists in S3 bucket\n- Download PDF and query image from S3\n- Call SageMaker endpoint to run YOLOv8 model for equation detection\n- Calculate similarity between detected equations and query image\n- Rank equations by similarity and identify top 5 matches\n- Draw bounding boxes around matched equations in PDF\n- Prepare and return JSON result with matched equations and locations\n- Clean up message from SQS queue and upload results to output bucket\n\n## DEPENDENCIES:\n- `dataHandler`: Custom module for S3/SQS operations\n- `boto3`: AWS SDK for Python\n- `json`: JSON parsing library\n- `cv2`: OpenCV for image processing\n- `PyTorchPredictor`: SageMaker prediction client\n- `JSONDeserializer`: Deserializer for SageMaker responses\n- `rank_eqn_similarity`: Function to compare equations with query\n- `download_files`, `final_output`: Helper functions for file operations\n\n## USAGE CONTEXT:\nUsed as an AWS Lambda function that processes SQS messages triggered by S3 uploads. Part of a serverless mathematical equation search pipeline that helps users find similar equations within PDF documents.\n\n## EDGE CASES:\n- Handles missing SageMaker endpoints with appropriate error response\n- Cleans temporary storage before processing to prevent space issues\n- Wraps entire execution in try/except to capture and report any errors\n- Ensures proper resource cleanup via SQS message deletion\n\n## RELATIONSHIPS:\n- Triggered by SQS messages from upstream file uploads\n- Interfaces with SageMaker for ML inference\n- Outputs processed results to S3 for downstream consumption\n- Part of a larger mathematical document search system"
    }
  },
  {
    "page_content": "# DIRECTORY: ml-model\n\n## PURPOSE:\nThe ml-model directory contains components for implementing, training, and deploying YOLOv8-based object detection models, providing a complete pipeline from data preparation through model training to inference and result visualization.\n\n## COMPONENT STRUCTURE:\n- **yolov8_predict.py**: Implements the complete object detection pipeline including model loading, image processing, inference, visualization, and result saving\n- **yolov8_training.py**: Provides functionality for training YOLOv8 models on custom datasets with configuration management and progress monitoring\n- **archive/**: Contains legacy code and reference implementations for image processing, neural networks, and web applications\n\n## ARCHITECTURE:\nThe directory follows a modular architecture separating model training from inference while maintaining a consistent pipeline approach in both components. The code demonstrates a clear separation between data preparation, model operations, and output handling, enabling flexibility in deployment contexts.\n\n## ENTRY POINTS:\n- `main()` function in **yolov8_predict.py** for running inference\n- `train_yolov8_model()` in **yolov8_training.py** for initiating model training\n- Command-line interfaces in both main files for direct execution\n\n## DATA FLOW:\nData typically flows from raw images through preprocessing stages, into the YOLOv8 model for inference or training, followed by postprocessing of model outputs into structured detection results or training metrics. For inference, results are then visualized or saved to various formats, while training produces model weights and performance metadata.\n\n## INTEGRATION:\nThe components integrate with the Ultralytics YOLOv8 framework, PyTorch ecosystem, and various image processing libraries (OpenCV, PIL). The inference module can connect to file systems, video streams, and potentially serve as an API endpoint, while the training module interfaces with monitoring tools like TensorBoard and Weights & Biases.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include pipeline architectures for sequential data processing, command-line argument parsing for configuration, separation of core model logic from I/O operations, modular function design for reusability, and hierarchical function organization where high-level functions orchestrate specialized utilities.\n\n## RELATIONSHIPS:\nWhile the prediction and training files operate independently, they represent complementary phases of the machine learning lifecycle - training creates the models that prediction consumes. The archive directory provides reference implementations and legacy code that may inform current development or provide alternative approaches to similar problems in the visual processing domain.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "ml-model",
      "path": "../mathsearch/ml-model",
      "code": "",
      "summary": "# DIRECTORY: ml-model\n\n## PURPOSE:\nThe ml-model directory contains components for implementing, training, and deploying YOLOv8-based object detection models, providing a complete pipeline from data preparation through model training to inference and result visualization.\n\n## COMPONENT STRUCTURE:\n- **yolov8_predict.py**: Implements the complete object detection pipeline including model loading, image processing, inference, visualization, and result saving\n- **yolov8_training.py**: Provides functionality for training YOLOv8 models on custom datasets with configuration management and progress monitoring\n- **archive/**: Contains legacy code and reference implementations for image processing, neural networks, and web applications\n\n## ARCHITECTURE:\nThe directory follows a modular architecture separating model training from inference while maintaining a consistent pipeline approach in both components. The code demonstrates a clear separation between data preparation, model operations, and output handling, enabling flexibility in deployment contexts.\n\n## ENTRY POINTS:\n- `main()` function in **yolov8_predict.py** for running inference\n- `train_yolov8_model()` in **yolov8_training.py** for initiating model training\n- Command-line interfaces in both main files for direct execution\n\n## DATA FLOW:\nData typically flows from raw images through preprocessing stages, into the YOLOv8 model for inference or training, followed by postprocessing of model outputs into structured detection results or training metrics. For inference, results are then visualized or saved to various formats, while training produces model weights and performance metadata.\n\n## INTEGRATION:\nThe components integrate with the Ultralytics YOLOv8 framework, PyTorch ecosystem, and various image processing libraries (OpenCV, PIL). The inference module can connect to file systems, video streams, and potentially serve as an API endpoint, while the training module interfaces with monitoring tools like TensorBoard and Weights & Biases.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include pipeline architectures for sequential data processing, command-line argument parsing for configuration, separation of core model logic from I/O operations, modular function design for reusability, and hierarchical function organization where high-level functions orchestrate specialized utilities.\n\n## RELATIONSHIPS:\nWhile the prediction and training files operate independently, they represent complementary phases of the machine learning lifecycle - training creates the models that prediction consumes. The archive directory provides reference implementations and legacy code that may inform current development or provide alternative approaches to similar problems in the visual processing domain."
    }
  },
  {
    "page_content": "# DIRECTORY: archive\n\n## PURPOSE:\nThe archive directory contains a collection of legacy code and sample implementations related to image processing, machine learning for visual similarity, and web applications for file handling, providing reference implementations and older versions of tools that may still be useful for specific use cases.\n\n## COMPONENT STRUCTURE:\n- **img-preprocessing/**: Contains tools for preprocessing LaTeX equation images including rotation, cropping, and augmentation\n- **web/**: Implements a Flask-based web application for file handling with S3 integration and ML processing\n- **old-files/**: Houses legacy utilities for ML image feature extraction and similarity computation\n- **old-siamese-model/**: Contains a Siamese neural network implementation for image similarity tasks\n- **prev_dataset/**: Provides utilities for image processing and visualization in deep learning workflows\n- **app_sample/**: Demonstrates Flask-based secure file upload functionality with various implementations\n- **white_background.py**: Standalone utility for removing and replacing image backgrounds\n\n## ARCHITECTURE:\nThe directory showcases a progression of development across several domains, with clear separation between web applications, machine learning models, and image processing utilities. The components demonstrate both functional and object-oriented programming patterns, with a focus on modular design and reusable components.\n\n## ENTRY POINTS:\n- Flask application routes in **web/api.py** and **app_sample/** directories\n- Image processing functions in **img-preprocessing/rotations_and_cropping.py**\n- Siamese model implementation in **old-siamese-model/siamese.py**\n- Image similarity computation via **old-files/ImageMatching.py**\n- Background removal utilities in **white_background.py**\n\n## DATA FLOW:\nData typically flows from raw images through preprocessing (cropping, rotation, background removal), into feature extraction and similarity computation systems, or through web interfaces that facilitate uploads, storage (local or S3), and integration with machine learning models. Results are either visualized, saved to storage, or returned via web APIs.\n\n## INTEGRATION:\nThe components integrate with various external systems including:\n- AWS S3 storage through boto3\n- PyTorch ecosystem for ML models and tensor operations\n- Flask web framework for HTTP interfaces\n- File systems for local storage operations\n- Pre-trained models (particularly VGG architectures)\n- PIL/Pillow and other image processing libraries\n\n## DEVELOPMENT PATTERNS:\nCommon patterns across the codebase include:\n- Separation of concerns with modular file organization\n- Security-first approaches for file validation\n- Progressive development with previous versions preserved\n- Transfer learning with pre-trained models\n- Input-process-output workflows for image manipulation\n- RESTful endpoint design for web interfaces\n\n## RELATIONSHIPS:\nThe directory components represent different aspects of visual processing systems that complement each other: preprocessing tools prepare images for machine learning, neural networks extract features and compute similarities, and web applications provide interfaces for uploading and processing files. While the components may not directly interact within current workflows, they provide reference implementations that could be combined to build complete image processing and similarity assessment pipelines.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "archive",
      "path": "../mathsearch/ml-model/archive",
      "code": "",
      "summary": "# DIRECTORY: archive\n\n## PURPOSE:\nThe archive directory contains a collection of legacy code and sample implementations related to image processing, machine learning for visual similarity, and web applications for file handling, providing reference implementations and older versions of tools that may still be useful for specific use cases.\n\n## COMPONENT STRUCTURE:\n- **img-preprocessing/**: Contains tools for preprocessing LaTeX equation images including rotation, cropping, and augmentation\n- **web/**: Implements a Flask-based web application for file handling with S3 integration and ML processing\n- **old-files/**: Houses legacy utilities for ML image feature extraction and similarity computation\n- **old-siamese-model/**: Contains a Siamese neural network implementation for image similarity tasks\n- **prev_dataset/**: Provides utilities for image processing and visualization in deep learning workflows\n- **app_sample/**: Demonstrates Flask-based secure file upload functionality with various implementations\n- **white_background.py**: Standalone utility for removing and replacing image backgrounds\n\n## ARCHITECTURE:\nThe directory showcases a progression of development across several domains, with clear separation between web applications, machine learning models, and image processing utilities. The components demonstrate both functional and object-oriented programming patterns, with a focus on modular design and reusable components.\n\n## ENTRY POINTS:\n- Flask application routes in **web/api.py** and **app_sample/** directories\n- Image processing functions in **img-preprocessing/rotations_and_cropping.py**\n- Siamese model implementation in **old-siamese-model/siamese.py**\n- Image similarity computation via **old-files/ImageMatching.py**\n- Background removal utilities in **white_background.py**\n\n## DATA FLOW:\nData typically flows from raw images through preprocessing (cropping, rotation, background removal), into feature extraction and similarity computation systems, or through web interfaces that facilitate uploads, storage (local or S3), and integration with machine learning models. Results are either visualized, saved to storage, or returned via web APIs.\n\n## INTEGRATION:\nThe components integrate with various external systems including:\n- AWS S3 storage through boto3\n- PyTorch ecosystem for ML models and tensor operations\n- Flask web framework for HTTP interfaces\n- File systems for local storage operations\n- Pre-trained models (particularly VGG architectures)\n- PIL/Pillow and other image processing libraries\n\n## DEVELOPMENT PATTERNS:\nCommon patterns across the codebase include:\n- Separation of concerns with modular file organization\n- Security-first approaches for file validation\n- Progressive development with previous versions preserved\n- Transfer learning with pre-trained models\n- Input-process-output workflows for image manipulation\n- RESTful endpoint design for web interfaces\n\n## RELATIONSHIPS:\nThe directory components represent different aspects of visual processing systems that complement each other: preprocessing tools prepare images for machine learning, neural networks extract features and compute similarities, and web applications provide interfaces for uploading and processing files. While the components may not directly interact within current workflows, they provide reference implementations that could be combined to build complete image processing and similarity assessment pipelines."
    }
  },
  {
    "page_content": "# STANDARDIZED SUMMARY: img-preprocessing\n\n## PURPOSE:\nThis directory contains tools for preprocessing images of LaTeX-rendered equations, focusing on rotation, cropping, and augmentation to prepare standardized datasets for machine learning models or document processing pipelines.\n\n## COMPONENT STRUCTURE:\n- **rotations_and_cropping.py**: Provides comprehensive tools for image loading, rotation, augmentation and transformation, including `img_input()`, `augment()`, and a rotation transformation class.\n- **crop.py**: Contains a focused `crop()` function that processes images by removing excessive white space while maintaining specific margins around the actual equation content.\n\n## ARCHITECTURE:\nThe directory follows a modular approach with separate files for different preprocessing concerns. It uses a functional programming style for standalone operations while incorporating object-oriented design for reusable transformations. The code consistently employs an input-process-output pattern for image manipulation.\n\n## ENTRY POINTS:\n- `crop.py:crop()`: For batch processing images to remove excess white space\n- `rotations_and_cropping.py:img_input()`: For loading and preparing images\n- `rotations_and_cropping.py:augment()`: For generating multiple augmented versions of images\n\n## DATA FLOW:\n1. Images are loaded from filesystem (via `img_input()`)\n2. Content boundaries are detected and images are cropped to standardize dimensions (via `crop()`)\n3. Various transformations including rotations, crops, and blurs are applied to generate augmented versions (via `augment()`)\n4. Processed images are saved to specified output directories\n\n## INTEGRATION:\nThe code integrates with:\n- PIL/Pillow for core image manipulation\n- PyTorch and torchvision for tensor operations and transformations\n- File system for input/output operations\n- External helper functions for conversion between tensor and PIL formats\n- Broader document processing or machine learning pipelines\n\n## DEVELOPMENT PATTERNS:\n- Standalone utility functions for discrete tasks\n- Progress tracking with tqdm for long-running operations\n- Combination of functional and object-oriented approaches\n- Parameterized transformations for flexibility\n- Input validation and error handling\n\n## RELATIONSHIPS:\nThe two files complement each other in a preprocessing pipeline: `crop.py` focuses on standardizing image dimensions by removing excess whitespace, while `rotations_and_cropping.py` handles loading, augmentation, and transformation. Together they form a comprehensive preprocessing toolkit that prepares LaTeX equation images for downstream applications like machine learning model training.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "img-preprocessing",
      "path": "../mathsearch/ml-model/archive/img-preprocessing",
      "code": "",
      "summary": "# STANDARDIZED SUMMARY: img-preprocessing\n\n## PURPOSE:\nThis directory contains tools for preprocessing images of LaTeX-rendered equations, focusing on rotation, cropping, and augmentation to prepare standardized datasets for machine learning models or document processing pipelines.\n\n## COMPONENT STRUCTURE:\n- **rotations_and_cropping.py**: Provides comprehensive tools for image loading, rotation, augmentation and transformation, including `img_input()`, `augment()`, and a rotation transformation class.\n- **crop.py**: Contains a focused `crop()` function that processes images by removing excessive white space while maintaining specific margins around the actual equation content.\n\n## ARCHITECTURE:\nThe directory follows a modular approach with separate files for different preprocessing concerns. It uses a functional programming style for standalone operations while incorporating object-oriented design for reusable transformations. The code consistently employs an input-process-output pattern for image manipulation.\n\n## ENTRY POINTS:\n- `crop.py:crop()`: For batch processing images to remove excess white space\n- `rotations_and_cropping.py:img_input()`: For loading and preparing images\n- `rotations_and_cropping.py:augment()`: For generating multiple augmented versions of images\n\n## DATA FLOW:\n1. Images are loaded from filesystem (via `img_input()`)\n2. Content boundaries are detected and images are cropped to standardize dimensions (via `crop()`)\n3. Various transformations including rotations, crops, and blurs are applied to generate augmented versions (via `augment()`)\n4. Processed images are saved to specified output directories\n\n## INTEGRATION:\nThe code integrates with:\n- PIL/Pillow for core image manipulation\n- PyTorch and torchvision for tensor operations and transformations\n- File system for input/output operations\n- External helper functions for conversion between tensor and PIL formats\n- Broader document processing or machine learning pipelines\n\n## DEVELOPMENT PATTERNS:\n- Standalone utility functions for discrete tasks\n- Progress tracking with tqdm for long-running operations\n- Combination of functional and object-oriented approaches\n- Parameterized transformations for flexibility\n- Input validation and error handling\n\n## RELATIONSHIPS:\nThe two files complement each other in a preprocessing pipeline: `crop.py` focuses on standardizing image dimensions by removing excess whitespace, while `rotations_and_cropping.py` handles loading, augmentation, and transformation. Together they form a comprehensive preprocessing toolkit that prepares LaTeX equation images for downstream applications like machine learning model training."
    }
  },
  {
    "page_content": "# FILE: rotations_and_cropping.py\n\n## OVERVIEW:\nThis file provides a comprehensive set of tools for image loading, rotation, cropping, and augmentation, primarily designed for processing LaTeX-rendered equation images to create varied training datasets for machine learning models.\n\n## KEY COMPONENTS:\n- `img_input()`: Loads an image from a file path and converts it to RGB format with an identifier\n- `augment()`: Generates multiple augmented versions of an image through rotations, crops, and blurs\n- `__init__()`: Constructor for a rotation transformation class that stores angle parameters\n- `__call__()`: Implements a callable interface that applies random rotation to an input image\n\n## ARCHITECTURE:\nThe file is structured around a two-part system: a standalone image loading function, a comprehensive augmentation function, and a custom rotation transformation class that can be integrated into image processing pipelines.\n\n## DATA FLOW:\n1. Images are loaded via `img_input()` which returns a tuple of identifier and image\n2. The `augment()` function takes these images and creates multiple transformed versions\n3. The rotation transformation class with its `__init__` and `__call__` methods serves as a reusable component for applying specific rotations within pipelines\n\n## INTEGRATION POINTS:\n- Interacts with filesystem for image loading and saving\n- Designed to work with PIL/Pillow for image handling\n- Uses PyTorch and torchvision for tensor operations and transformations\n- Connects to external helper functions (`to_tensor`, `to_pil`, `blur`)\n\n## USAGE PATTERNS:\n- Creating augmented datasets for training deep learning models on LaTeX equations\n- Applying consistent rotation transformations as part of a larger image processing pipeline\n- Organizing processed images into structured directory hierarchies\n\n## DEPENDENCIES:\n- PIL/Pillow: For image processing\n- torch: For tensor operations\n- torchvision.transforms: For image transformations\n- os: For filesystem operations\n- External functions: to_tensor, to_pil, blur\n\n## RELATIONSHIPS:\nThe functions work together in a pipeline where images are first loaded, then extensively augmented with various transformations. The rotation class provides a reusable component that can be integrated at different points in image processing workflows, particularly for creating diverse training datasets.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "rotations_and_cropping.py",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/rotations_and_cropping.py",
      "code": "'''\nFile to introduce random rotations and croppings to the im2latex dataset.\n'''\n\nfrom PIL import Image\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\nfrom torchvision.utils import save_image\n\nfrom torchvision.io import read_image, write_jpeg\n\nfrom tqdm import tqdm\n\nimport os\nimport random\n\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\n\nclass Rotatations:\n    \"\"\"Must be created for pytorch image augmentation.\n\n    Rotate by one of the given angles.\"\"\"\n\n    def __init__(self, angles):\n        self.angles = angles\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return TF.rotate(x, angle)\n\n\ndef img_input(file_name, name):\n    '''\n    Convert an image to RGB.\n\n    Args:\n        file_name: path to file\n        name: id for image\n\n    Returns:\n        (name, image): (id of image, image as RGB)\n    '''\n    image = Image.open(str(Path(file_name)))\n    image = image.convert('RGB')\n\n    return (name, image)\n\n# dims = image.shape\n\n# image = image.reshape((dims[1], dims[2], dims[0]))\n\n# print(image.shape)\n\n\ndef augment(name, img):\n    '''\n    Add random croppings and a rotation to an latex-rendered-image.\n\n    Args:\n        name - id of image\n        img - image as a numpy array\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n\n    temp_path = path / name\n\n    if not temp_path.exists():\n        os.mkdir(temp_path)\n    else:\n        return\n\n    a = to_tensor(img)\n    dims = a.shape\n\n    new_dims = (int(dims[1] * proportionality),\n                int(dims[2] * proportionality))\n\n    temp_path_folder = temp_path / 'transformed'\n\n    if not temp_path_folder.exists():\n        os.mkdir(temp_path_folder)\n\n    img.save(temp_path / 'original_file.jpeg')\n\n    img_left = torch.rot90(a, dims=[1, 2])\n\n    to_pil(img_left).save(temp_path_folder / 'left_rotate.jpeg')\n\n    img_right = torch.rot90(a, k=3, dims=[1, 2])\n    to_pil(img_right).save(temp_path_folder / 'right_rotate.jpeg')\n\n    # random crop\n    for i in range(5):\n        random_crop_func = T.RandomCrop(size=new_dims)\n        test = random_crop_func(a)\n        to_pil(test).save(temp_path_folder / f'{i}_randomcrop.jpeg')\n\n    blurs = [blur(img) for _ in range(5)]\n\n    for i, j in enumerate(blurs):\n        j.save(temp_path_folder / f'{i}_blur.jpeg')\n\n\n# randomcrop = T.RandomCrop()\nif __name__ == '__main__':\n    to_tensor = T.ToTensor()\n    to_pil = T.ToPILImage()\n\n    blur = T.GaussianBlur((7, 13))\n\n    proportionality = 0.7\n\n    path = Path('output')\n\n    if not path.exists():\n        os.mkdir(path)\n\n    # with ProcessPoolExecutor(max_workers=4) as executor:\n    for filename in tqdm(os.listdir('crop_formula_images')):\n        name, img = img_input(f'crop_formula_images/{filename}', filename)\n        augment(name, img)\n",
      "summary": "# FILE: rotations_and_cropping.py\n\n## OVERVIEW:\nThis file provides a comprehensive set of tools for image loading, rotation, cropping, and augmentation, primarily designed for processing LaTeX-rendered equation images to create varied training datasets for machine learning models.\n\n## KEY COMPONENTS:\n- `img_input()`: Loads an image from a file path and converts it to RGB format with an identifier\n- `augment()`: Generates multiple augmented versions of an image through rotations, crops, and blurs\n- `__init__()`: Constructor for a rotation transformation class that stores angle parameters\n- `__call__()`: Implements a callable interface that applies random rotation to an input image\n\n## ARCHITECTURE:\nThe file is structured around a two-part system: a standalone image loading function, a comprehensive augmentation function, and a custom rotation transformation class that can be integrated into image processing pipelines.\n\n## DATA FLOW:\n1. Images are loaded via `img_input()` which returns a tuple of identifier and image\n2. The `augment()` function takes these images and creates multiple transformed versions\n3. The rotation transformation class with its `__init__` and `__call__` methods serves as a reusable component for applying specific rotations within pipelines\n\n## INTEGRATION POINTS:\n- Interacts with filesystem for image loading and saving\n- Designed to work with PIL/Pillow for image handling\n- Uses PyTorch and torchvision for tensor operations and transformations\n- Connects to external helper functions (`to_tensor`, `to_pil`, `blur`)\n\n## USAGE PATTERNS:\n- Creating augmented datasets for training deep learning models on LaTeX equations\n- Applying consistent rotation transformations as part of a larger image processing pipeline\n- Organizing processed images into structured directory hierarchies\n\n## DEPENDENCIES:\n- PIL/Pillow: For image processing\n- torch: For tensor operations\n- torchvision.transforms: For image transformations\n- os: For filesystem operations\n- External functions: to_tensor, to_pil, blur\n\n## RELATIONSHIPS:\nThe functions work together in a pipeline where images are first loaded, then extensively augmented with various transformations. The rotation class provides a reusable component that can be integrated at different points in image processing workflows, particularly for creating diverse training datasets."
    }
  },
  {
    "page_content": "# FUNCTION: img_input\n\n## PURPOSE:\nLoads an image from a file path and converts it to RGB format, associating it with an identifier for further processing.\n\n## INPUTS:\n- `file_name` (string or Path): Path to the image file to be loaded\n- `name` (any): Identifier for the image, used to track or reference the image\n\n## OUTPUTS:\n- Tuple: `(name, image)` where:\n  - `name`: The same identifier passed as input\n  - `image`: PIL Image object in RGB format\n\n## KEY STEPS:\n- Open the image file from the provided path\n- Convert the image to RGB color mode\n- Return a tuple containing the identifier and the RGB image\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library/Pillow): For image processing\n- Path: For file path handling\n\n## USAGE CONTEXT:\nTypically used in image processing pipelines where multiple images need to be loaded, converted to a consistent format, and tracked with identifiers.\n\n## EDGE CASES:\n- May raise exceptions if the file doesn't exist or isn't a valid image format\n- No explicit error handling is implemented within the function\n\n## RELATIONSHIPS:\nLikely part of an image processing system where this function serves as an initial data loading step before further processing, analysis, or transformation of images.\n\ndef img_input(file_name, name):\n    '''\n    Convert an image to RGB.\n\n    Args:\n        file_name: path to file\n        name: id for image\n\n    Returns:\n        (name, image): (id of image, image as RGB)\n    '''\n    image = Image.open(str(Path(file_name)))\n    image = image.convert('RGB')\n\n    return (name, image)",
    "metadata": {
      "type": "FUNCTION",
      "name": "img_input",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/rotations_and_cropping.py",
      "code": "def img_input(file_name, name):\n    '''\n    Convert an image to RGB.\n\n    Args:\n        file_name: path to file\n        name: id for image\n\n    Returns:\n        (name, image): (id of image, image as RGB)\n    '''\n    image = Image.open(str(Path(file_name)))\n    image = image.convert('RGB')\n\n    return (name, image)",
      "summary": "# FUNCTION: img_input\n\n## PURPOSE:\nLoads an image from a file path and converts it to RGB format, associating it with an identifier for further processing.\n\n## INPUTS:\n- `file_name` (string or Path): Path to the image file to be loaded\n- `name` (any): Identifier for the image, used to track or reference the image\n\n## OUTPUTS:\n- Tuple: `(name, image)` where:\n  - `name`: The same identifier passed as input\n  - `image`: PIL Image object in RGB format\n\n## KEY STEPS:\n- Open the image file from the provided path\n- Convert the image to RGB color mode\n- Return a tuple containing the identifier and the RGB image\n\n## DEPENDENCIES:\n- PIL (Python Imaging Library/Pillow): For image processing\n- Path: For file path handling\n\n## USAGE CONTEXT:\nTypically used in image processing pipelines where multiple images need to be loaded, converted to a consistent format, and tracked with identifiers.\n\n## EDGE CASES:\n- May raise exceptions if the file doesn't exist or isn't a valid image format\n- No explicit error handling is implemented within the function\n\n## RELATIONSHIPS:\nLikely part of an image processing system where this function serves as an initial data loading step before further processing, analysis, or transformation of images."
    }
  },
  {
    "page_content": "# FUNCTION: augment\n\n## PURPOSE:\nGenerates augmented versions of a LaTeX-rendered image by applying various transformations (rotations, random crops, and blurs) and saves them to a directory structure, enhancing dataset variety for better model training.\n\n## INPUTS:\n- `name` (str): Identifier for the image that will be used as a directory name\n- `img` (numpy array): Input image to be augmented\n\n## OUTPUTS:\n- `None`: Function saves augmented images to disk rather than returning data\n\n## KEY STEPS:\n- Create a directory structure for the original and transformed images\n- Save the original image\n- Create left and right rotated versions of the image using torch.rot90\n- Generate 5 randomly cropped versions of the image with dimensions scaled by a global proportionality factor\n- Generate 5 blurred versions of the image using an external blur function\n- Save all transformed images to the appropriate directory\n\n## DEPENDENCIES:\n- `os`: For directory creation and management\n- `torch`: For tensor operations and rotations\n- `torchvision.transforms` (as T): For random cropping\n- `to_tensor`: Function to convert image to tensor format\n- `to_pil`: Function to convert tensor to PIL image\n- `blur`: External function for creating blurred versions\n- `path`: Global variable defining the base directory path\n- `proportionality`: Global variable defining scaling factor for crops\n\n## USAGE CONTEXT:\nTypically used in data preparation pipelines for machine learning models that process LaTeX equations, where having a variety of augmented samples improves model robustness and generalization.\n\n## EDGE CASES:\n- If a directory with the given name already exists, the function returns early without performing any operations\n- No explicit error handling for invalid images or file operations\n\n## RELATIONSHIPS:\n- Part of a data augmentation system for training models to recognize LaTeX equations from images\n- Relies on external helper functions (to_tensor, to_pil, blur) for image processing\n- Creates a standardized directory structure for augmented images that would be expected by downstream processes\n\ndef augment(name, img):\n    '''\n    Add random croppings and a rotation to an latex-rendered-image.\n\n    Args:\n        name - id of image\n        img - image as a numpy array\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n\n    temp_path = path / name\n\n    if not temp_path.exists():\n        os.mkdir(temp_path)\n    else:\n        return\n\n    a = to_tensor(img)\n    dims = a.shape\n\n    new_dims = (int(dims[1] * proportionality),\n                int(dims[2] * proportionality))\n\n    temp_path_folder = temp_path / 'transformed'\n\n    if not temp_path_folder.exists():\n        os.mkdir(temp_path_folder)\n\n    img.save(temp_path / 'original_file.jpeg')\n\n    img_left = torch.rot90(a, dims=[1, 2])\n\n    to_pil(img_left).save(temp_path_folder / 'left_rotate.jpeg')\n\n    img_right = torch.rot90(a, k=3, dims=[1, 2])\n    to_pil(img_right).save(temp_path_folder / 'right_rotate.jpeg')\n\n    # random crop\n    for i in range(5):\n        random_crop_func = T.RandomCrop(size=new_dims)\n        test = random_crop_func(a)\n        to_pil(test).save(temp_path_folder / f'{i}_randomcrop.jpeg')\n\n    blurs = [blur(img) for _ in range(5)]\n\n    for i, j in enumerate(blurs):\n        j.save(temp_path_folder / f'{i}_blur.jpeg')",
    "metadata": {
      "type": "FUNCTION",
      "name": "augment",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/rotations_and_cropping.py",
      "code": "def augment(name, img):\n    '''\n    Add random croppings and a rotation to an latex-rendered-image.\n\n    Args:\n        name - id of image\n        img - image as a numpy array\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n\n    temp_path = path / name\n\n    if not temp_path.exists():\n        os.mkdir(temp_path)\n    else:\n        return\n\n    a = to_tensor(img)\n    dims = a.shape\n\n    new_dims = (int(dims[1] * proportionality),\n                int(dims[2] * proportionality))\n\n    temp_path_folder = temp_path / 'transformed'\n\n    if not temp_path_folder.exists():\n        os.mkdir(temp_path_folder)\n\n    img.save(temp_path / 'original_file.jpeg')\n\n    img_left = torch.rot90(a, dims=[1, 2])\n\n    to_pil(img_left).save(temp_path_folder / 'left_rotate.jpeg')\n\n    img_right = torch.rot90(a, k=3, dims=[1, 2])\n    to_pil(img_right).save(temp_path_folder / 'right_rotate.jpeg')\n\n    # random crop\n    for i in range(5):\n        random_crop_func = T.RandomCrop(size=new_dims)\n        test = random_crop_func(a)\n        to_pil(test).save(temp_path_folder / f'{i}_randomcrop.jpeg')\n\n    blurs = [blur(img) for _ in range(5)]\n\n    for i, j in enumerate(blurs):\n        j.save(temp_path_folder / f'{i}_blur.jpeg')",
      "summary": "# FUNCTION: augment\n\n## PURPOSE:\nGenerates augmented versions of a LaTeX-rendered image by applying various transformations (rotations, random crops, and blurs) and saves them to a directory structure, enhancing dataset variety for better model training.\n\n## INPUTS:\n- `name` (str): Identifier for the image that will be used as a directory name\n- `img` (numpy array): Input image to be augmented\n\n## OUTPUTS:\n- `None`: Function saves augmented images to disk rather than returning data\n\n## KEY STEPS:\n- Create a directory structure for the original and transformed images\n- Save the original image\n- Create left and right rotated versions of the image using torch.rot90\n- Generate 5 randomly cropped versions of the image with dimensions scaled by a global proportionality factor\n- Generate 5 blurred versions of the image using an external blur function\n- Save all transformed images to the appropriate directory\n\n## DEPENDENCIES:\n- `os`: For directory creation and management\n- `torch`: For tensor operations and rotations\n- `torchvision.transforms` (as T): For random cropping\n- `to_tensor`: Function to convert image to tensor format\n- `to_pil`: Function to convert tensor to PIL image\n- `blur`: External function for creating blurred versions\n- `path`: Global variable defining the base directory path\n- `proportionality`: Global variable defining scaling factor for crops\n\n## USAGE CONTEXT:\nTypically used in data preparation pipelines for machine learning models that process LaTeX equations, where having a variety of augmented samples improves model robustness and generalization.\n\n## EDGE CASES:\n- If a directory with the given name already exists, the function returns early without performing any operations\n- No explicit error handling for invalid images or file operations\n\n## RELATIONSHIPS:\n- Part of a data augmentation system for training models to recognize LaTeX equations from images\n- Relies on external helper functions (to_tensor, to_pil, blur) for image processing\n- Creates a standardized directory structure for augmented images that would be expected by downstream processes"
    }
  },
  {
    "page_content": "# Standardized Function Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a new instance of a class with angle values. This constructor stores the provided angles as an instance attribute.\n\nINPUTS:\n- angles: A collection (likely a list, tuple, or array) of angular values that define the object's orientation or configuration.\n\nOUTPUTS:\n- None (initializes the instance with the provided angles)\n\nKEY STEPS:\n- Takes the input angles parameter\n- Assigns it to the self.angles instance attribute\n\nDEPENDENCIES:\n- None evident from the code shown\n\nUSAGE CONTEXT:\n- Used when creating a new instance of the class\n- Likely part of a class that deals with rotations, orientations, or angular measurements\n\nEDGE CASES:\n- No explicit validation of the angles input is performed\n- May behave unexpectedly if angles is None or an incompatible data type\n\nRELATIONSHIPS:\n- As a constructor, this method is called whenever a new instance of the class is created\n- Other methods in the class likely use the self.angles attribute for calculations or transformations\n\n    def __init__(self, angles):\n        self.angles = angles",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/rotations_and_cropping.py",
      "code": "    def __init__(self, angles):\n        self.angles = angles",
      "summary": "# Standardized Function Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a new instance of a class with angle values. This constructor stores the provided angles as an instance attribute.\n\nINPUTS:\n- angles: A collection (likely a list, tuple, or array) of angular values that define the object's orientation or configuration.\n\nOUTPUTS:\n- None (initializes the instance with the provided angles)\n\nKEY STEPS:\n- Takes the input angles parameter\n- Assigns it to the self.angles instance attribute\n\nDEPENDENCIES:\n- None evident from the code shown\n\nUSAGE CONTEXT:\n- Used when creating a new instance of the class\n- Likely part of a class that deals with rotations, orientations, or angular measurements\n\nEDGE CASES:\n- No explicit validation of the angles input is performed\n- May behave unexpectedly if angles is None or an incompatible data type\n\nRELATIONSHIPS:\n- As a constructor, this method is called whenever a new instance of the class is created\n- Other methods in the class likely use the self.angles attribute for calculations or transformations"
    }
  },
  {
    "page_content": "# FUNCTION: __call__\n\n## PURPOSE:\nThis function applies a random rotation transformation to an input image by selecting an angle from a predefined list of angles. It serves as the callable interface for a rotation transformation class in an image processing pipeline.\n\n## INPUTS:\n- `x` (Tensor or PIL.Image): The input image to be rotated.\n\n## OUTPUTS:\n- Rotated image (same type as input): The input image rotated by a randomly chosen angle.\n\n## KEY STEPS:\n- Randomly select an angle from the pre-defined list of angles (`self.angles`)\n- Apply rotation to the input image using the TorchVision functional rotate operation\n- Return the rotated image\n\n## DEPENDENCIES:\n- `random`: Python's built-in random module for angle selection\n- `TF.rotate`: The rotate function from torchvision.transforms.functional module\n\n## USAGE CONTEXT:\nTypically used as part of an image augmentation pipeline in deep learning workflows, particularly for computer vision tasks. It's usually initialized with a list of angles and then applied to images during dataset loading or training.\n\n## EDGE CASES:\n- If `self.angles` is empty, would raise an IndexError when random.choice is called\n- Does not explicitly handle different input types, relying on TF.rotate to handle various image formats\n\n## RELATIONSHIPS:\n- Acts as the implementation of the `__call__` method for a rotation transformation class\n- Likely part of a larger data augmentation framework, where multiple transformations can be composed together\n\n    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return TF.rotate(x, angle)",
    "metadata": {
      "type": "FUNCTION",
      "name": "__call__",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/rotations_and_cropping.py",
      "code": "    def __call__(self, x):\n        angle = random.choice(self.angles)\n        return TF.rotate(x, angle)",
      "summary": "# FUNCTION: __call__\n\n## PURPOSE:\nThis function applies a random rotation transformation to an input image by selecting an angle from a predefined list of angles. It serves as the callable interface for a rotation transformation class in an image processing pipeline.\n\n## INPUTS:\n- `x` (Tensor or PIL.Image): The input image to be rotated.\n\n## OUTPUTS:\n- Rotated image (same type as input): The input image rotated by a randomly chosen angle.\n\n## KEY STEPS:\n- Randomly select an angle from the pre-defined list of angles (`self.angles`)\n- Apply rotation to the input image using the TorchVision functional rotate operation\n- Return the rotated image\n\n## DEPENDENCIES:\n- `random`: Python's built-in random module for angle selection\n- `TF.rotate`: The rotate function from torchvision.transforms.functional module\n\n## USAGE CONTEXT:\nTypically used as part of an image augmentation pipeline in deep learning workflows, particularly for computer vision tasks. It's usually initialized with a list of angles and then applied to images during dataset loading or training.\n\n## EDGE CASES:\n- If `self.angles` is empty, would raise an IndexError when random.choice is called\n- Does not explicitly handle different input types, relying on TF.rotate to handle various image formats\n\n## RELATIONSHIPS:\n- Acts as the implementation of the `__call__` method for a rotation transformation class\n- Likely part of a larger data augmentation framework, where multiple transformations can be composed together"
    }
  },
  {
    "page_content": "# FILE: crop.py\n\n## OVERVIEW:\nThis file contains functionality for processing rendered LaTeX images by cropping them to focus on the actual content while maintaining specific margins, eliminating excessive white space around mathematical equations.\n\n## KEY COMPONENTS:\n- `crop()`: Processes images in a directory by cropping each image to its content boundaries plus defined margins, saving the result to an output directory.\n\n## ARCHITECTURE:\nThe file has a single, focused function that provides image processing capabilities specifically for LaTeX-rendered content. It follows a straightforward input-process-output pattern.\n\n## DATA FLOW:\n1. Input images are read from a specified directory\n2. Each image is analyzed to determine content boundaries by finding the extremal black pixels\n3. Images are cropped to these boundaries plus predefined margins\n4. Processed images are saved to the output directory with original filenames\n\n## INTEGRATION POINTS:\n- Designed to work within document processing workflows or dataset preparation pipelines\n- Serves as a preprocessing step for downstream applications that require properly sized equation images\n\n## USAGE PATTERNS:\n- Batch processing of multiple LaTeX-rendered images\n- Preparation of mathematical content for educational materials, research papers, or machine learning datasets\n- Standardizing image dimensions while preserving content\n\n## DEPENDENCIES:\n- `os`: For file and directory operations\n- `tqdm`: For displaying progress bars during processing\n- `PIL.Image`: For image manipulation operations\n\n## RELATIONSHIPS:\nThis function likely operates as part of a larger document or equation processing system, where it serves as a preprocessing utility to standardize images before further processing or display. It works independently but is designed to enhance content for downstream applications.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "crop.py",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/crop.py",
      "code": "'''\nCrop function.\n'''\n\n\nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nfrom IPython.display import display\n\n\ndir = \"im2latex/gold_formula_images/\"\ndir_save = \"im2latex/crop_formula_images/\"\n\n\ndef crop(dir, dir_save):\n    '''\n    Crops a directory of image rendered latex.\n\n    Args:\n        dir: input directory\n        dir_save: output directory\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n    for filename in tqdm(os.listdir(dir)):\n        f = os.path.join(dir, filename)\n\n        if os.path.isfile(f):\n            img = Image.open(f)\n            # img.show()\n\n            img = img.convert(\"RGBA\")\n            pixdata = img.load()\n            width, height = img.size\n\n            # find right\n            for x in range(width):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        right = x\n                        break\n            # find left\n            for x in reversed(range(width)):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        left = x\n                        break\n            # find bottom\n            for y in range(height):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        bottom = y\n                        break\n            # find top\n            for y in reversed(range(height)):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        top = y\n                        break\n            # print(left,top,right,bottom)\n            img = img.crop((left-220, top-110, right+220, bottom+110))\n\n            img.save(dir_save+filename, \"PNG\")\n",
      "summary": "# FILE: crop.py\n\n## OVERVIEW:\nThis file contains functionality for processing rendered LaTeX images by cropping them to focus on the actual content while maintaining specific margins, eliminating excessive white space around mathematical equations.\n\n## KEY COMPONENTS:\n- `crop()`: Processes images in a directory by cropping each image to its content boundaries plus defined margins, saving the result to an output directory.\n\n## ARCHITECTURE:\nThe file has a single, focused function that provides image processing capabilities specifically for LaTeX-rendered content. It follows a straightforward input-process-output pattern.\n\n## DATA FLOW:\n1. Input images are read from a specified directory\n2. Each image is analyzed to determine content boundaries by finding the extremal black pixels\n3. Images are cropped to these boundaries plus predefined margins\n4. Processed images are saved to the output directory with original filenames\n\n## INTEGRATION POINTS:\n- Designed to work within document processing workflows or dataset preparation pipelines\n- Serves as a preprocessing step for downstream applications that require properly sized equation images\n\n## USAGE PATTERNS:\n- Batch processing of multiple LaTeX-rendered images\n- Preparation of mathematical content for educational materials, research papers, or machine learning datasets\n- Standardizing image dimensions while preserving content\n\n## DEPENDENCIES:\n- `os`: For file and directory operations\n- `tqdm`: For displaying progress bars during processing\n- `PIL.Image`: For image manipulation operations\n\n## RELATIONSHIPS:\nThis function likely operates as part of a larger document or equation processing system, where it serves as a preprocessing utility to standardize images before further processing or display. It works independently but is designed to enhance content for downstream applications."
    }
  },
  {
    "page_content": "# FUNCTION: crop\n\n## PURPOSE:\nThis function processes a directory of rendered LaTeX images by cropping each image to focus on the actual content while maintaining a specific margin. It helps eliminate excessive white space around LaTeX equations.\n\n## INPUTS:\n- `dir` (string): Path to the input directory containing images to be cropped\n- `dir_save` (string): Path to the output directory where cropped images will be saved\n\n## OUTPUTS:\n- None: The function saves processed images to the specified directory but doesn't return any values\n\n## KEY STEPS:\n- Iterate through all files in the input directory\n- Convert each image to RGBA format for pixel manipulation\n- Find the leftmost, rightmost, topmost, and bottommost black pixels (0,0,0,255) in the image\n- Crop the image to these boundaries plus a specific margin (220px horizontally, 110px vertically)\n- Save the cropped image to the output directory with the same filename\n\n## DEPENDENCIES:\n- `os`: For file and directory operations\n- `tqdm`: For displaying progress bars during processing\n- `PIL.Image`: For image manipulation operations\n\n## USAGE CONTEXT:\n- Used in processing pipelines for mathematical content where LaTeX renders might have excessive whitespace\n- Typically part of document processing workflows or dataset preparation for machine learning models\n\n## EDGE CASES:\n- No explicit error handling for files that aren't images\n- May fail if an image doesn't contain any black pixels (0,0,0,255)\n- Does not verify if output directory exists before attempting to save\n\n## RELATIONSHIPS:\n- Likely part of a larger document or equation processing system\n- Works as a standalone utility to prepare images for downstream applications\n\ndef crop(dir, dir_save):\n    '''\n    Crops a directory of image rendered latex.\n\n    Args:\n        dir: input directory\n        dir_save: output directory\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n    for filename in tqdm(os.listdir(dir)):\n        f = os.path.join(dir, filename)\n\n        if os.path.isfile(f):\n            img = Image.open(f)\n            # img.show()\n\n            img = img.convert(\"RGBA\")\n            pixdata = img.load()\n            width, height = img.size\n\n            # find right\n            for x in range(width):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        right = x\n                        break\n            # find left\n            for x in reversed(range(width)):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        left = x\n                        break\n            # find bottom\n            for y in range(height):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        bottom = y\n                        break\n            # find top\n            for y in reversed(range(height)):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        top = y\n                        break\n            # print(left,top,right,bottom)\n            img = img.crop((left-220, top-110, right+220, bottom+110))\n\n            img.save(dir_save+filename, \"PNG\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "crop",
      "path": "../mathsearch/ml-model/archive/img-preprocessing/crop.py",
      "code": "def crop(dir, dir_save):\n    '''\n    Crops a directory of image rendered latex.\n\n    Args:\n        dir: input directory\n        dir_save: output directory\n\n    Returns:\n        None\n\n    Raises:\n        None\n    '''\n    for filename in tqdm(os.listdir(dir)):\n        f = os.path.join(dir, filename)\n\n        if os.path.isfile(f):\n            img = Image.open(f)\n            # img.show()\n\n            img = img.convert(\"RGBA\")\n            pixdata = img.load()\n            width, height = img.size\n\n            # find right\n            for x in range(width):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        right = x\n                        break\n            # find left\n            for x in reversed(range(width)):\n                for y in range(height):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        left = x\n                        break\n            # find bottom\n            for y in range(height):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        bottom = y\n                        break\n            # find top\n            for y in reversed(range(height)):\n                for x in range(width):\n                    if pixdata[x, y] == (0, 0, 0, 255):\n                        top = y\n                        break\n            # print(left,top,right,bottom)\n            img = img.crop((left-220, top-110, right+220, bottom+110))\n\n            img.save(dir_save+filename, \"PNG\")",
      "summary": "# FUNCTION: crop\n\n## PURPOSE:\nThis function processes a directory of rendered LaTeX images by cropping each image to focus on the actual content while maintaining a specific margin. It helps eliminate excessive white space around LaTeX equations.\n\n## INPUTS:\n- `dir` (string): Path to the input directory containing images to be cropped\n- `dir_save` (string): Path to the output directory where cropped images will be saved\n\n## OUTPUTS:\n- None: The function saves processed images to the specified directory but doesn't return any values\n\n## KEY STEPS:\n- Iterate through all files in the input directory\n- Convert each image to RGBA format for pixel manipulation\n- Find the leftmost, rightmost, topmost, and bottommost black pixels (0,0,0,255) in the image\n- Crop the image to these boundaries plus a specific margin (220px horizontally, 110px vertically)\n- Save the cropped image to the output directory with the same filename\n\n## DEPENDENCIES:\n- `os`: For file and directory operations\n- `tqdm`: For displaying progress bars during processing\n- `PIL.Image`: For image manipulation operations\n\n## USAGE CONTEXT:\n- Used in processing pipelines for mathematical content where LaTeX renders might have excessive whitespace\n- Typically part of document processing workflows or dataset preparation for machine learning models\n\n## EDGE CASES:\n- No explicit error handling for files that aren't images\n- May fail if an image doesn't contain any black pixels (0,0,0,255)\n- Does not verify if output directory exists before attempting to save\n\n## RELATIONSHIPS:\n- Likely part of a larger document or equation processing system\n- Works as a standalone utility to prepare images for downstream applications"
    }
  },
  {
    "page_content": "# DIRECTORY: web\n\n## PURPOSE:\nThis directory contains a Flask-based web application focusing on file handling operations, particularly for processing and managing files through web interfaces, including integration with AWS S3 storage and a machine learning system called MathSearch.\n\n## COMPONENT STRUCTURE:\n- **api.py**: Implements a web API for the MathSearch system with endpoints for file uploads, YOLOv5 model execution, and S3 storage coordination\n- **s3.py**: Provides a comprehensive interface for Amazon S3 operations, including uploading, downloading, and managing files\n- **sample_app/**: Contains various implementations of Flask applications focused on file upload validation and handling\n- **prev_app/**: Houses an earlier version of the Flask application with file management functionality\n- **call_example.py**: An empty file that appears to be a placeholder or template\n\n## ARCHITECTURE:\nThe directory follows a Flask-based web application architecture with clear separation of concerns. It implements RESTful endpoints, form processing, security validation for file uploads, and integration with external services like AWS S3 and machine learning models. The components demonstrate progressive development with both current and previous implementations maintained.\n\n## ENTRY POINTS:\nPrimary entry points include the route handlers in api.py (particularly run_model() and upload_file()), similar handlers in the sample_app directory, and the S3 interface functions in s3.py. These provide access to file upload forms, processing capabilities, and storage operations.\n\n## DATA FLOW:\n1. Users access web forms through GET requests to upload interfaces\n2. Files are submitted via POST requests to upload_file() endpoints\n3. Submitted files undergo validation through allowed_file() functions\n4. Valid files are either processed directly or sent to S3 storage\n5. For ML operations, files are processed through the YOLOv5 model via subprocess calls\n6. Results are returned to users or stored for later retrieval\n\n## INTEGRATION:\nThe components integrate with Flask's web framework, AWS S3 through boto3, the filesystem for local storage, and external machine learning models through subprocess calls. The directory serves as a bridge between web clients, cloud storage, and computational processing systems, using Flask's routing and templating capabilities to coordinate these interactions.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include:\n- Separation of form rendering and form processing functions\n- Input validation before file processing\n- Modular function design with focused responsibilities\n- Cross-origin request support through CORS headers\n- Error handling with user feedback mechanisms\n- Configuration-driven approaches for file types and storage locations\n\n## RELATIONSHIPS:\nThe files work together to form a complete web-based file processing system. The api.py provides the main interface for the ML system, while s3.py handles cloud storage operations. The sample_app and prev_app directories show the evolution of the file handling components, with progressively more sophisticated validation and processing capabilities. The system demonstrates a layered architecture where web interfaces call processing logic which in turn leverages storage functionality.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "web",
      "path": "../mathsearch/ml-model/archive/web",
      "code": "",
      "summary": "# DIRECTORY: web\n\n## PURPOSE:\nThis directory contains a Flask-based web application focusing on file handling operations, particularly for processing and managing files through web interfaces, including integration with AWS S3 storage and a machine learning system called MathSearch.\n\n## COMPONENT STRUCTURE:\n- **api.py**: Implements a web API for the MathSearch system with endpoints for file uploads, YOLOv5 model execution, and S3 storage coordination\n- **s3.py**: Provides a comprehensive interface for Amazon S3 operations, including uploading, downloading, and managing files\n- **sample_app/**: Contains various implementations of Flask applications focused on file upload validation and handling\n- **prev_app/**: Houses an earlier version of the Flask application with file management functionality\n- **call_example.py**: An empty file that appears to be a placeholder or template\n\n## ARCHITECTURE:\nThe directory follows a Flask-based web application architecture with clear separation of concerns. It implements RESTful endpoints, form processing, security validation for file uploads, and integration with external services like AWS S3 and machine learning models. The components demonstrate progressive development with both current and previous implementations maintained.\n\n## ENTRY POINTS:\nPrimary entry points include the route handlers in api.py (particularly run_model() and upload_file()), similar handlers in the sample_app directory, and the S3 interface functions in s3.py. These provide access to file upload forms, processing capabilities, and storage operations.\n\n## DATA FLOW:\n1. Users access web forms through GET requests to upload interfaces\n2. Files are submitted via POST requests to upload_file() endpoints\n3. Submitted files undergo validation through allowed_file() functions\n4. Valid files are either processed directly or sent to S3 storage\n5. For ML operations, files are processed through the YOLOv5 model via subprocess calls\n6. Results are returned to users or stored for later retrieval\n\n## INTEGRATION:\nThe components integrate with Flask's web framework, AWS S3 through boto3, the filesystem for local storage, and external machine learning models through subprocess calls. The directory serves as a bridge between web clients, cloud storage, and computational processing systems, using Flask's routing and templating capabilities to coordinate these interactions.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include:\n- Separation of form rendering and form processing functions\n- Input validation before file processing\n- Modular function design with focused responsibilities\n- Cross-origin request support through CORS headers\n- Error handling with user feedback mechanisms\n- Configuration-driven approaches for file types and storage locations\n\n## RELATIONSHIPS:\nThe files work together to form a complete web-based file processing system. The api.py provides the main interface for the ML system, while s3.py handles cloud storage operations. The sample_app and prev_app directories show the evolution of the file handling components, with progressively more sophisticated validation and processing capabilities. The system demonstrates a layered architecture where web interfaces call processing logic which in turn leverages storage functionality."
    }
  },
  {
    "page_content": "# DIRECTORY: sample_app\n\n## PURPOSE:\nThis directory contains a Flask-based web application focused on file handling operations, particularly file uploads with validation, along with utility functions for file management and directory cleanup.\n\n## COMPONENT STRUCTURE:\n- **app_main.py**: Core functionality for handling file uploads in a Flask application with validation logic and basic informational endpoints\n- **app_checkpoint.py**: Implementation of a file handling web application with form rendering, upload processing, and user feedback mechanisms\n- **app_python.py**: Utility functions for file management operations, particularly directory cleanup\n- **app_example.py**: Minimal demonstration file with a single hello_world function, likely for testing or as a template\n\n## ARCHITECTURE:\nThe directory follows a Flask-based web application architecture with separated concerns. It implements route handlers for different endpoints, form processing, security validation for uploads, and utility functions for file system operations. The components maintain relatively loose coupling with focused responsibilities.\n\n## ENTRY POINTS:\nPrimary entry points include the route handlers in app_main.py and app_checkpoint.py, particularly the upload_file() functions that process file submissions and the upload_form() function that renders the upload interface. The hello_world() functions serve as basic diagnostic endpoints.\n\n## DATA FLOW:\n1. Users access web forms through GET requests to view upload interfaces\n2. Files are submitted via POST requests to upload_file() endpoints\n3. Submitted files are validated through allowed_file() functions\n4. Valid files are saved to configured upload folders\n5. Users receive feedback via redirects and flash messages\n6. Directory cleanup is handled through remove_files() when needed\n\n## INTEGRATION:\nThe files integrate with Flask's core systems (request handling, routing, templating), the file system through OS operations, and external utilities like Werkzeug's secure_filename for security. They reference app.config settings for configuration values like UPLOAD_FOLDER and use Flask's redirect and flash mechanisms for user flow control.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include:\n- Separation of form rendering and form processing into distinct functions\n- Input validation before file processing\n- Use of helper functions for validation (allowed_file)\n- Flask route decorators for endpoint definition\n- Utility functions for system maintenance tasks\n\n## RELATIONSHIPS:\nThe files demonstrate complementary functionality with app_main.py and app_checkpoint.py providing web interfaces for file upload, while app_python.py offers utility functions for file management. The upload_file() functions in both web components rely on allowed_file() for validation, and successful uploads redirect to download endpoints. The app_example.py exists as a standalone template.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "sample_app",
      "path": "../mathsearch/ml-model/archive/web/sample_app",
      "code": "",
      "summary": "# DIRECTORY: sample_app\n\n## PURPOSE:\nThis directory contains a Flask-based web application focused on file handling operations, particularly file uploads with validation, along with utility functions for file management and directory cleanup.\n\n## COMPONENT STRUCTURE:\n- **app_main.py**: Core functionality for handling file uploads in a Flask application with validation logic and basic informational endpoints\n- **app_checkpoint.py**: Implementation of a file handling web application with form rendering, upload processing, and user feedback mechanisms\n- **app_python.py**: Utility functions for file management operations, particularly directory cleanup\n- **app_example.py**: Minimal demonstration file with a single hello_world function, likely for testing or as a template\n\n## ARCHITECTURE:\nThe directory follows a Flask-based web application architecture with separated concerns. It implements route handlers for different endpoints, form processing, security validation for uploads, and utility functions for file system operations. The components maintain relatively loose coupling with focused responsibilities.\n\n## ENTRY POINTS:\nPrimary entry points include the route handlers in app_main.py and app_checkpoint.py, particularly the upload_file() functions that process file submissions and the upload_form() function that renders the upload interface. The hello_world() functions serve as basic diagnostic endpoints.\n\n## DATA FLOW:\n1. Users access web forms through GET requests to view upload interfaces\n2. Files are submitted via POST requests to upload_file() endpoints\n3. Submitted files are validated through allowed_file() functions\n4. Valid files are saved to configured upload folders\n5. Users receive feedback via redirects and flash messages\n6. Directory cleanup is handled through remove_files() when needed\n\n## INTEGRATION:\nThe files integrate with Flask's core systems (request handling, routing, templating), the file system through OS operations, and external utilities like Werkzeug's secure_filename for security. They reference app.config settings for configuration values like UPLOAD_FOLDER and use Flask's redirect and flash mechanisms for user flow control.\n\n## DEVELOPMENT PATTERNS:\nCommon patterns include:\n- Separation of form rendering and form processing into distinct functions\n- Input validation before file processing\n- Use of helper functions for validation (allowed_file)\n- Flask route decorators for endpoint definition\n- Utility functions for system maintenance tasks\n\n## RELATIONSHIPS:\nThe files demonstrate complementary functionality with app_main.py and app_checkpoint.py providing web interfaces for file upload, while app_python.py offers utility functions for file management. The upload_file() functions in both web components rely on allowed_file() for validation, and successful uploads redirect to download endpoints. The app_example.py exists as a standalone template."
    }
  },
  {
    "page_content": "# FILE: app_python.py\n\n## OVERVIEW:\nThis file appears to contain utility functions for file management operations, focusing on directory cleanup and file handling. It provides functionality to manage files within a specified directory.\n\n## KEY COMPONENTS:\n- `remove_files(data_dir)`: Deletes all files within a specified directory without removing the directory itself, used for cleanup operations.\n\n## ARCHITECTURE:\nThe file has a simple architecture with a focused utility function for file management. It appears to be designed as part of a larger system where directory cleanup is required.\n\n## DATA FLOW:\nThe function takes a directory path as input, processes the files in that directory, and performs deletion operations without returning any data. The flow is unidirectional, focusing on file system modification.\n\n## INTEGRATION POINTS:\n- Integrates with the file system through OS operations\n- Likely used by other modules that need to clean up directories as part of their processing workflows\n\n## USAGE PATTERNS:\n- Cleanup operations before or after data processing\n- Resetting working directories to prepare for new tasks\n- Removing temporary files after task completion\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functionality\n- OS-specific file operations: `os.listdir()`, `os.path.join()`, `os.remove()`\n\n## RELATIONSHIPS:\nThis function likely works in tandem with other file generation or processing functions. It would be called either before starting new operations that require a clean directory or after completing operations that generate temporary files that are no longer needed.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_python.py",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_python.py",
      "code": "import sys\nimport pandas\nimport os\nsys.path.append('/home/ubuntu/yolov5')\nimport main\n\n\ndef remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))\n\nif __name__ == \"__main__\":\n\n\t# clear data dir\n\tdata_dir = '/home/ubuntu/yolov5/input_data'\n\t# remove_files(data_dir)\n\n\t# run flask to download files\n    # app.run()\n\n\ttarget_search = \"target_search.png\"\n\ttarget_file = \"/home/ubuntu/yolov5/input_data/sample_doc.pdf\"\n\n\tos.chdir('/home/ubuntu/yolov5')\n\tmain.main(target_search)\n\n\t# return stuff",
      "summary": "# FILE: app_python.py\n\n## OVERVIEW:\nThis file appears to contain utility functions for file management operations, focusing on directory cleanup and file handling. It provides functionality to manage files within a specified directory.\n\n## KEY COMPONENTS:\n- `remove_files(data_dir)`: Deletes all files within a specified directory without removing the directory itself, used for cleanup operations.\n\n## ARCHITECTURE:\nThe file has a simple architecture with a focused utility function for file management. It appears to be designed as part of a larger system where directory cleanup is required.\n\n## DATA FLOW:\nThe function takes a directory path as input, processes the files in that directory, and performs deletion operations without returning any data. The flow is unidirectional, focusing on file system modification.\n\n## INTEGRATION POINTS:\n- Integrates with the file system through OS operations\n- Likely used by other modules that need to clean up directories as part of their processing workflows\n\n## USAGE PATTERNS:\n- Cleanup operations before or after data processing\n- Resetting working directories to prepare for new tasks\n- Removing temporary files after task completion\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functionality\n- OS-specific file operations: `os.listdir()`, `os.path.join()`, `os.remove()`\n\n## RELATIONSHIPS:\nThis function likely works in tandem with other file generation or processing functions. It would be called either before starting new operations that require a clean directory or after completing operations that generate temporary files that are no longer needed."
    }
  },
  {
    "page_content": "# FUNCTION: remove_files\n\n## PURPOSE:\nDeletes all files within a specified directory. The function provides a way to clear a directory by removing all of its files without deleting the directory itself.\n\n## INPUTS:\n- `data_dir` (string): Path to the directory from which all files should be removed.\n\n## OUTPUTS:\n- None: The function does not return any value.\n\n## KEY STEPS:\n- List all entries in the specified directory using `os.listdir()`\n- Iterate through each entry in the directory\n- Remove each file using `os.remove()` by constructing its full path\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functionality\n- `os.listdir()`: For listing directory contents\n- `os.path.join()`: For constructing file paths\n- `os.remove()`: For deleting files\n\n## USAGE CONTEXT:\nTypically used for cleanup operations, such as clearing temporary files, resetting a working directory, or preparing a directory for new data. May be called before processing new data or after completing tasks that generate temporary files.\n\n## EDGE CASES:\n- Does not handle subdirectories - will raise an IsADirectoryError if the directory contains folders\n- Will raise PermissionError if any files are read-only or locked\n- No error handling is implemented for file access issues\n- Does not check if the directory exists before attempting to list its contents\n\n## RELATIONSHIPS:\nLikely part of a data processing or file management module. May be used in conjunction with functions that generate files in the same directory, potentially as part of a cleanup process in a larger workflow.\n\ndef remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))",
    "metadata": {
      "type": "FUNCTION",
      "name": "remove_files",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_python.py",
      "code": "def remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))",
      "summary": "# FUNCTION: remove_files\n\n## PURPOSE:\nDeletes all files within a specified directory. The function provides a way to clear a directory by removing all of its files without deleting the directory itself.\n\n## INPUTS:\n- `data_dir` (string): Path to the directory from which all files should be removed.\n\n## OUTPUTS:\n- None: The function does not return any value.\n\n## KEY STEPS:\n- List all entries in the specified directory using `os.listdir()`\n- Iterate through each entry in the directory\n- Remove each file using `os.remove()` by constructing its full path\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functionality\n- `os.listdir()`: For listing directory contents\n- `os.path.join()`: For constructing file paths\n- `os.remove()`: For deleting files\n\n## USAGE CONTEXT:\nTypically used for cleanup operations, such as clearing temporary files, resetting a working directory, or preparing a directory for new data. May be called before processing new data or after completing tasks that generate temporary files.\n\n## EDGE CASES:\n- Does not handle subdirectories - will raise an IsADirectoryError if the directory contains folders\n- Will raise PermissionError if any files are read-only or locked\n- No error handling is implemented for file access issues\n- Does not check if the directory exists before attempting to list its contents\n\n## RELATIONSHIPS:\nLikely part of a data processing or file management module. May be used in conjunction with functions that generate files in the same directory, potentially as part of a cleanup process in a larger workflow."
    }
  },
  {
    "page_content": "# FILE: app_main.py\n\n## OVERVIEW:\nThis file provides core functionality for handling file uploads in a Flask web application, including validation of file types, processing uploads, and serving basic informational endpoints.\n\n## KEY COMPONENTS:\n- `allowed_file(filename)`: Validates whether a file has an allowed extension\n- `hello_world()`: Returns a greeting message with configuration information for testing\n- `upload_file()`: Handles file uploads, validates them, and stores them in the designated upload folder\n\n## ARCHITECTURE:\nThe file implements a simple file upload workflow with validation controls and follows a Flask route-based structure. It separates concerns by using distinct functions for validation and processing while leveraging Flask's request handling mechanisms.\n\n## DATA FLOW:\n1. Web requests come in through Flask routes\n2. For file uploads, files are received through `request.files`\n3. File extensions are validated by `allowed_file()`\n4. Valid files are saved to the configured upload folder\n5. Users are redirected to a download page or back to the upload form based on validation results\n\n## INTEGRATION POINTS:\n- Integrates with Flask's request handling system\n- Uses Flask's configuration system for storing upload settings (`app.config['UPLOAD_FOLDER']`)\n- Connects to a `download_file` route after successful uploads\n- Uses Flask's `secure_filename` utility for security\n\n## USAGE PATTERNS:\n- Users access the upload page via GET request to see the upload form\n- Files are submitted via POST request and validated before storage\n- The `hello_world()` function is used for health checks or to verify service configuration\n\n## DEPENDENCIES:\n- Flask framework (request, redirect, url_for, flash)\n- Werkzeug utilities (secure_filename)\n- Global ALLOWED_EXTENSIONS constant for file validation\n\n## RELATIONSHIPS:\n- `allowed_file()` works as a validation helper for `upload_file()`\n- `upload_file()` redirects to a `download_file` route (defined elsewhere) after successful uploads\n- `hello_world()` operates independently as a diagnostic endpoint",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_main.py",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_main.py",
      "code": "from flask import Flask, flash, redirect, url_for, request, render_template\nfrom werkzeug.utils import secure_filename\nimport os\n\n\"\"\"\n@Author: Emerald Liu\nDoes not support concurrency currently\n\"\"\"\n\n# constant variables\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\nALLOWED_EXTENSIONS = {'pdf'}\n\n# helper functions\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n# initalize flask app config\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n\n# @app.route('/upload')\n# def upload_file():\n#    return render_template('upload.html')\n\n@app.route('/')\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/upload', methods=['GET', 'POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''\n\n\nif __name__ == '__main__':\n    app.debug = True\n    app.run(host='0.0.0.0', port=3000)\n",
      "summary": "# FILE: app_main.py\n\n## OVERVIEW:\nThis file provides core functionality for handling file uploads in a Flask web application, including validation of file types, processing uploads, and serving basic informational endpoints.\n\n## KEY COMPONENTS:\n- `allowed_file(filename)`: Validates whether a file has an allowed extension\n- `hello_world()`: Returns a greeting message with configuration information for testing\n- `upload_file()`: Handles file uploads, validates them, and stores them in the designated upload folder\n\n## ARCHITECTURE:\nThe file implements a simple file upload workflow with validation controls and follows a Flask route-based structure. It separates concerns by using distinct functions for validation and processing while leveraging Flask's request handling mechanisms.\n\n## DATA FLOW:\n1. Web requests come in through Flask routes\n2. For file uploads, files are received through `request.files`\n3. File extensions are validated by `allowed_file()`\n4. Valid files are saved to the configured upload folder\n5. Users are redirected to a download page or back to the upload form based on validation results\n\n## INTEGRATION POINTS:\n- Integrates with Flask's request handling system\n- Uses Flask's configuration system for storing upload settings (`app.config['UPLOAD_FOLDER']`)\n- Connects to a `download_file` route after successful uploads\n- Uses Flask's `secure_filename` utility for security\n\n## USAGE PATTERNS:\n- Users access the upload page via GET request to see the upload form\n- Files are submitted via POST request and validated before storage\n- The `hello_world()` function is used for health checks or to verify service configuration\n\n## DEPENDENCIES:\n- Flask framework (request, redirect, url_for, flash)\n- Werkzeug utilities (secure_filename)\n- Global ALLOWED_EXTENSIONS constant for file validation\n\n## RELATIONSHIPS:\n- `allowed_file()` works as a validation helper for `upload_file()`\n- `upload_file()` redirects to a `download_file` route (defined elsewhere) after successful uploads\n- `hello_world()` operates independently as a diagnostic endpoint"
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has an allowed file extension as defined by the ALLOWED_EXTENSIONS global variable.\n\n## INPUTS:\n- `filename` (string): The filename to check for an allowed extension.\n\n## OUTPUTS:\n- Boolean: Returns True if the file extension is allowed, False otherwise.\n\n## KEY STEPS:\n- Check if the filename contains a period character (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the lowercase extension is in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS: A global variable (likely a set or list) containing permitted file extensions\n\n## USAGE CONTEXT:\nTypically used in web applications to validate file uploads, ensuring only files with approved extensions can be uploaded to the server.\n\n## EDGE CASES:\n- Returns False if the filename has no extension (no period)\n- Returns False if the filename ends with a period but has no extension after it\n- Assumes ALLOWED_EXTENSIONS is properly defined; will cause errors if that global variable doesn't exist\n\n## RELATIONSHIPS:\nLikely part of a file upload validation workflow, working alongside other functions that validate file content, size, and security.\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_main.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has an allowed file extension as defined by the ALLOWED_EXTENSIONS global variable.\n\n## INPUTS:\n- `filename` (string): The filename to check for an allowed extension.\n\n## OUTPUTS:\n- Boolean: Returns True if the file extension is allowed, False otherwise.\n\n## KEY STEPS:\n- Check if the filename contains a period character (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the lowercase extension is in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS: A global variable (likely a set or list) containing permitted file extensions\n\n## USAGE CONTEXT:\nTypically used in web applications to validate file uploads, ensuring only files with approved extensions can be uploaded to the server.\n\n## EDGE CASES:\n- Returns False if the filename has no extension (no period)\n- Returns False if the filename ends with a period but has no extension after it\n- Assumes ALLOWED_EXTENSIONS is properly defined; will cause errors if that global variable doesn't exist\n\n## RELATIONSHIPS:\nLikely part of a file upload validation workflow, working alongside other functions that validate file content, size, and security."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a greeting message with specific configuration information. This function serves as a simple test or placeholder that includes environment details.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: Returns the greeting \"Hello World!\" followed by configuration details including username, service name, port number, and temperature setting.\n\n## KEY STEPS:\n- Return a hardcoded string containing the greeting and configuration information.\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nTypically used in testing scenarios, as a health check endpoint, or as a placeholder in applications during development. May be used to verify that the service is running correctly with the expected configuration.\n\n## EDGE CASES:\nNone - this is a simple function with no conditional logic or error handling.\n\n## RELATIONSHIPS:\nLikely serves as a basic utility or test function in a larger system. The configuration data in the return string suggests it may be part of a web service running on port 3000.\n\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_main.py",
      "code": "def hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a greeting message with specific configuration information. This function serves as a simple test or placeholder that includes environment details.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: Returns the greeting \"Hello World!\" followed by configuration details including username, service name, port number, and temperature setting.\n\n## KEY STEPS:\n- Return a hardcoded string containing the greeting and configuration information.\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nTypically used in testing scenarios, as a health check endpoint, or as a placeholder in applications during development. May be used to verify that the service is running correctly with the expected configuration.\n\n## EDGE CASES:\nNone - this is a simple function with no conditional logic or error handling.\n\n## RELATIONSHIPS:\nLikely serves as a basic utility or test function in a larger system. The configuration data in the return string suggests it may be part of a web service running on port 3000."
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates if a file has an allowed extension by checking its filename against a predefined list of permitted extensions.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- `boolean`: Returns `True` if the file extension is allowed, `False` otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (`.`) character\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Verify if the extracted extension exists in the predefined `ALLOWED_EXTENSIONS` set/list\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS`: A global constant containing a collection of permitted file extensions\n\n## USAGE CONTEXT:\n- Typically used in web applications to validate file uploads before processing them\n- Often called during form submission handling to prevent unwanted file types\n\n## EDGE CASES:\n- Returns `False` if the filename has no extension (no period character)\n- Case-insensitive comparison of extensions by converting to lowercase\n- Will raise an exception if `ALLOWED_EXTENSIONS` is not defined\n\n## RELATIONSHIPS:\n- Works alongside file upload handlers to enforce security and validation rules\n- Relates to the application's security framework to prevent malicious file uploads\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_main.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates if a file has an allowed extension by checking its filename against a predefined list of permitted extensions.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- `boolean`: Returns `True` if the file extension is allowed, `False` otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (`.`) character\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Verify if the extracted extension exists in the predefined `ALLOWED_EXTENSIONS` set/list\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS`: A global constant containing a collection of permitted file extensions\n\n## USAGE CONTEXT:\n- Typically used in web applications to validate file uploads before processing them\n- Often called during form submission handling to prevent unwanted file types\n\n## EDGE CASES:\n- Returns `False` if the filename has no extension (no period character)\n- Case-insensitive comparison of extensions by converting to lowercase\n- Will raise an exception if `ALLOWED_EXTENSIONS` is not defined\n\n## RELATIONSHIPS:\n- Works alongside file upload handlers to enforce security and validation rules\n- Relates to the application's security framework to prevent malicious file uploads"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads from web forms, validating the upload request and saving valid files to a designated upload folder on the server.\n\n## INPUTS:\n- No parameters, but relies on HTTP request data:\n  - `request.files['file']`: The uploaded file (from multipart/form-data)\n  - `request.method`: The HTTP method (POST for file uploads)\n\n## OUTPUTS:\n- `str`: HTML form for file upload (on GET requests)\n- `redirect`: Redirects to download_file route with filename (successful upload)\n- `redirect`: Redirects back to upload page (failed validation)\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Validate that a file was selected\n- Check if the file has an allowed file type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Redirect to the download_file endpoint or display the upload form\n\n## DEPENDENCIES:\n- Flask: `request`, `redirect`, `url_for`, `flash`\n- `secure_filename`: For sanitizing filenames\n- `allowed_file`: Function to check if file type is permitted\n- `app.config['UPLOAD_FOLDER']`: Configuration setting for upload location\n\n## USAGE CONTEXT:\nUsed in web applications that need to accept file uploads from users, typically integrated into a Flask route that handles both displaying the upload form and processing the uploaded file.\n\n## EDGE CASES:\n- No file part in request: Shows error and redirects\n- Empty filename: Shows error and redirects\n- Unsupported file type: Implicitly rejects (no handling shown)\n- Note: There appears to be an error in the file saving line (incorrect parameter usage)\n\n## RELATIONSHIPS:\n- Called by Flask router when the upload route is accessed\n- Calls `download_file` route after successful upload\n- Relies on `allowed_file` function to validate file types\n- Interacts with application configuration for upload settings\n\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_main.py",
      "code": "def upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads from web forms, validating the upload request and saving valid files to a designated upload folder on the server.\n\n## INPUTS:\n- No parameters, but relies on HTTP request data:\n  - `request.files['file']`: The uploaded file (from multipart/form-data)\n  - `request.method`: The HTTP method (POST for file uploads)\n\n## OUTPUTS:\n- `str`: HTML form for file upload (on GET requests)\n- `redirect`: Redirects to download_file route with filename (successful upload)\n- `redirect`: Redirects back to upload page (failed validation)\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Validate that a file was selected\n- Check if the file has an allowed file type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Redirect to the download_file endpoint or display the upload form\n\n## DEPENDENCIES:\n- Flask: `request`, `redirect`, `url_for`, `flash`\n- `secure_filename`: For sanitizing filenames\n- `allowed_file`: Function to check if file type is permitted\n- `app.config['UPLOAD_FOLDER']`: Configuration setting for upload location\n\n## USAGE CONTEXT:\nUsed in web applications that need to accept file uploads from users, typically integrated into a Flask route that handles both displaying the upload form and processing the uploaded file.\n\n## EDGE CASES:\n- No file part in request: Shows error and redirects\n- Empty filename: Shows error and redirects\n- Unsupported file type: Implicitly rejects (no handling shown)\n- Note: There appears to be an error in the file saving line (incorrect parameter usage)\n\n## RELATIONSHIPS:\n- Called by Flask router when the upload route is accessed\n- Calls `download_file` route after successful upload\n- Relies on `allowed_file` function to validate file types\n- Interacts with application configuration for upload settings"
    }
  },
  {
    "page_content": "# FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a simple web application for file handling, primarily focused on file upload functionality with basic validation and user feedback mechanisms.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message redirecting users to a specific upload URL\n- `download()`: A stub function intended for downloading files from URLs to server storage (currently non-functional)\n- `allowed_file(filename)`: Validates if a filename has a permitted extension\n- `upload_form()`: Renders the HTML template for file upload\n- `upload_file()`: Processes file uploads, validates files, and saves them to the server\n\n## ARCHITECTURE:\nThe file follows a typical Flask web application structure with route handlers for different endpoints, including form rendering and form processing. It implements security validation for file uploads and uses Flask's redirect and flash mechanisms for user flow control.\n\n## DATA FLOW:\n1. Users access the upload form via `upload_form()`\n2. They submit files through an HTML form to `upload_file()`\n3. Files are validated using `allowed_file()` \n4. Valid files are saved to a configured upload folder\n5. Users receive feedback via flash messages and redirects\n\n## INTEGRATION POINTS:\n- Connects to Flask's request handling, templating (render_template), and response systems (redirect, flash)\n- Relies on Werkzeug's secure_filename for filename sanitization\n- Integrates with the server's file system for storing uploaded files\n- References app.config for configuration settings, particularly UPLOAD_FOLDER\n\n## USAGE PATTERNS:\n- Web users navigate to the upload page, select files, and submit the form\n- The system validates file types, processes uploads, and provides feedback\n- The `hello_world()` function redirects users from deprecated endpoints\n- The commented-out `download()` function suggests planned functionality for server-side file retrieval\n\n## DEPENDENCIES:\n- Flask (request, flash, redirect, render_template)\n- Werkzeug (secure_filename)\n- OS module (path.join)\n- ALLOWED_EXTENSIONS constant for file type validation\n- Commented code references additional dependencies: requests, wget\n\n## RELATIONSHIPS:\nThe functions form a cohesive file handling system where `upload_form()` provides the interface, `upload_file()` processes submissions, and `allowed_file()` ensures security. The `hello_world()` function serves as a redirect for outdated endpoints, while the commented `download()` function indicates planned expansion of functionality to include file retrieval.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_checkpoint.py",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "from werkzeug.utils import secure_filename\nfrom flask import Flask, flash, request, redirect, render_template\nimport urllib.request\nimport requests\nimport os\nfrom flask import Flask\nimport wget\n\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n\napp = Flask(__name__)\napp.secret_key = \"secret key\"\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n\n\n@app.route('/')\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'\n\n# https://www.cs.cornell.edu/~kozen/Papers/daa.pdf\n\n@app.route('/pdf', methods=['GET', 'POST'])\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"\n    # return send_to_directory(app.config['UPLOAD_FOLDER'], link)\n    # return send_file(link, as_attachment=True)\n\n\nALLOWED_EXTENSIONS = set(['pdf'])\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n@app.route('/upload')\ndef upload_form():\n    return render_template('upload.html')\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)\n\nif __name__ == \"__main__\":\n    app.run()\n\n\n\n# # from flask import Flask, render_template, request\n# # # from werkzeug import secure_filename\n# # from werkzeug.utils import secure_filename\n# # from werkzeug.datastructures import  FileStorage\n# # app = Flask(__name__)\n\n# # @app.route('/')\n# # def hello_world():\n# #     return 'Hello World! - emerald@mathsearch port:3000 temp:4'\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\t\n# # @app.route('/uploader', methods = ['GET', 'POST'])\n# # def uploadfile():\n# #    if request.method == 'POST':\n# #       f = request.files['file']\n# #       f.save(secure_filename(f.filename))\n# #       return 'file uploaded successfully'\n\t\t\n# # if __name__ == '__main__':\n# #     app.debug = True\n# #     app.run(host='0.0.0.0', port=8100)\n\n\n# from flask import Flask, flash, redirect, url_for, request, render_template\n# from werkzeug.utils import secure_filename\n# import os\n\n# \"\"\"\n# @Author: Emerald Liu\n# Does not support concurrency currently\n# \"\"\"\n\n# # constant variables\n# UPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n# ALLOWED_EXTENSIONS = {'pdf'}\n\n# # helper functions\n# def allowed_file(filename):\n#     return '.' in filename and \\\n#         filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n# # initalize flask app config\n# app = Flask(__name__)\n# app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\n\n# # @app.route('/upload', methods=['GET', 'POST'])\n# # def upload_file():\n# #     if request.method == 'POST':\n# #         # check if the post request has the file part\n# #         if 'file' not in request.files:\n# #             flash('No file part')\n# #             return redirect(request.url)\n# #         file = request.files['file']\n# #         # If the user does not select a file, the browser submits an\n# #         # empty file without a filename.\n# #         if file.filename == '':\n# #             flash('No selected file')\n# #             return redirect(request.url)\n# #         if file and allowed_file(file.filename):\n# #             filename = secure_filename(file.filename)\n# #             # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# #             file.save(app.config['UPLOAD_FOLDER'], filename)\n# #             return redirect(url_for('download_file', name=filename))\n# #     return '''\n# #     <!doctype html>\n# #     <title>Upload new File</title>\n# #     <h1>Upload new File</h1>\n# #     <form method=post enctype=multipart/form-data>\n# #       <input type=file name=file>\n# #       <input type=submit value=Upload>\n# #     </form>\n# #     '''\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n\n\n# import os\n# import urllib.request\n# # from app import app\n# from flask import Flask, flash, request, redirect, render_template\n# from werkzeug.utils import secure_filename\n\n# ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'])\n# app = Flask(__name__)\n\n# def allowed_file(filename):\n# \treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\t\n# @app.route('/')\n# def upload_form():\n# \treturn render_template('upload.html')\n\n# @app.route('/', methods=['POST'])\n# def upload_file():\n# \tif request.method == 'POST':\n#         # check if the post request has the file part\n# \t\tif 'file' not in request.files:\n# \t\t\tflash('No file part')\n# \t\t\treturn redirect(request.url)\n# \t\tfile = request.files['file']\n# \t\tif file.filename == '':\n# \t\t\tflash('No file selected for uploading')\n# \t\t\treturn redirect(request.url)\n# \t\tif file and allowed_file(file.filename):\n# \t\t\tfilename = secure_filename(file.filename)\n# \t\t\tfile.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# \t\t\tflash('File successfully uploaded')\n# \t\t\treturn redirect('/')\n# \t\telse:\n# \t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n# \t\t\treturn redirect(request.url)\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n",
      "summary": "# FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a simple web application for file handling, primarily focused on file upload functionality with basic validation and user feedback mechanisms.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message redirecting users to a specific upload URL\n- `download()`: A stub function intended for downloading files from URLs to server storage (currently non-functional)\n- `allowed_file(filename)`: Validates if a filename has a permitted extension\n- `upload_form()`: Renders the HTML template for file upload\n- `upload_file()`: Processes file uploads, validates files, and saves them to the server\n\n## ARCHITECTURE:\nThe file follows a typical Flask web application structure with route handlers for different endpoints, including form rendering and form processing. It implements security validation for file uploads and uses Flask's redirect and flash mechanisms for user flow control.\n\n## DATA FLOW:\n1. Users access the upload form via `upload_form()`\n2. They submit files through an HTML form to `upload_file()`\n3. Files are validated using `allowed_file()` \n4. Valid files are saved to a configured upload folder\n5. Users receive feedback via flash messages and redirects\n\n## INTEGRATION POINTS:\n- Connects to Flask's request handling, templating (render_template), and response systems (redirect, flash)\n- Relies on Werkzeug's secure_filename for filename sanitization\n- Integrates with the server's file system for storing uploaded files\n- References app.config for configuration settings, particularly UPLOAD_FOLDER\n\n## USAGE PATTERNS:\n- Web users navigate to the upload page, select files, and submit the form\n- The system validates file types, processes uploads, and provides feedback\n- The `hello_world()` function redirects users from deprecated endpoints\n- The commented-out `download()` function suggests planned functionality for server-side file retrieval\n\n## DEPENDENCIES:\n- Flask (request, flash, redirect, render_template)\n- Werkzeug (secure_filename)\n- OS module (path.join)\n- ALLOWED_EXTENSIONS constant for file type validation\n- Commented code references additional dependencies: requests, wget\n\n## RELATIONSHIPS:\nThe functions form a cohesive file handling system where `upload_form()` provides the interface, `upload_file()` processes submissions, and `allowed_file()` ensures security. The `hello_world()` function serves as a redirect for outdated endpoints, while the commented `download()` function indicates planned expansion of functionality to include file retrieval."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nThis function returns a warning message directing users to a specific URL. It appears to serve as a redirect notice for users who may be accessing an outdated or incorrect endpoint.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: A warning message instructing users to go to \"http://18.207.249.45/upload\" instead\n\n## KEY STEPS:\n- Simply returns a hardcoded string with the warning message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nLikely used as a response handler for deprecated routes or endpoints in a web application, informing users where they should go instead.\n\n## EDGE CASES:\nNone - this is a simple function that always returns the same string\n\n## RELATIONSHIPS:\nThis function likely relates to routing components of a web application, specifically directing users away from outdated endpoints to the correct upload functionality located at the specified IP address.\n\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "def hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nThis function returns a warning message directing users to a specific URL. It appears to serve as a redirect notice for users who may be accessing an outdated or incorrect endpoint.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: A warning message instructing users to go to \"http://18.207.249.45/upload\" instead\n\n## KEY STEPS:\n- Simply returns a hardcoded string with the warning message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nLikely used as a response handler for deprecated routes or endpoints in a web application, informing users where they should go instead.\n\n## EDGE CASES:\nNone - this is a simple function that always returns the same string\n\n## RELATIONSHIPS:\nThis function likely relates to routing components of a web application, specifically directing users away from outdated endpoints to the correct upload functionality located at the specified IP address."
    }
  },
  {
    "page_content": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: download\n\nPURPOSE:\nThis function appears to be a stub for downloading files from a URL and saving them to an upload folder. Currently, all functionality is commented out, and it simply returns \"success\".\n\nINPUTS:\nNone (though commented code shows it was intended to use a URL parameter 'c' from request.args)\n\nOUTPUTS:\nString - Returns the string \"success\" regardless of execution\n\nKEY STEPS:\n* Currently none - all functional code is commented out\n* Commented code shows three alternative approaches that were considered:\n  * Using wget to download a file\n  * Using requests with allow_redirects\n  * Using requests with a context manager for file handling\n\nDEPENDENCIES:\n* None active (commented code references: requests, wget, app.config, UPLOAD_FOLDER)\n\nUSAGE CONTEXT:\nLikely intended as a route handler in a web application for downloading external files to the server's storage.\n\nEDGE CASES:\n* Currently handles no edge cases as the function is essentially non-functional\n* Commented code shows no error handling for download failures or invalid URLs\n\nRELATIONSHIPS:\n* Appears to be part of a web application, likely using a framework like Flask (references to request.args and app.config)\n* Would interact with server's file system to store downloaded content\n\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "download",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "def download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
      "summary": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: download\n\nPURPOSE:\nThis function appears to be a stub for downloading files from a URL and saving them to an upload folder. Currently, all functionality is commented out, and it simply returns \"success\".\n\nINPUTS:\nNone (though commented code shows it was intended to use a URL parameter 'c' from request.args)\n\nOUTPUTS:\nString - Returns the string \"success\" regardless of execution\n\nKEY STEPS:\n* Currently none - all functional code is commented out\n* Commented code shows three alternative approaches that were considered:\n  * Using wget to download a file\n  * Using requests with allow_redirects\n  * Using requests with a context manager for file handling\n\nDEPENDENCIES:\n* None active (commented code references: requests, wget, app.config, UPLOAD_FOLDER)\n\nUSAGE CONTEXT:\nLikely intended as a route handler in a web application for downloading external files to the server's storage.\n\nEDGE CASES:\n* Currently handles no edge cases as the function is essentially non-functional\n* Commented code shows no error handling for download failures or invalid URLs\n\nRELATIONSHIPS:\n* Appears to be part of a web application, likely using a framework like Flask (references to request.args and app.config)\n* Would interact with server's file system to store downloaded content"
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has a permitted file extension. This function serves as a security check to ensure only certain file types can be processed.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- Boolean: Returns `True` if the file has an allowed extension, `False` otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period character (`.`)\n- Extract the file extension by splitting from the rightmost period\n- Convert the extension to lowercase\n- Verify if the extension exists in the predefined `ALLOWED_EXTENSIONS` set\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS` (set/list): A global constant containing all permitted file extensions\n\n## USAGE CONTEXT:\n- Typically used in file upload handlers to validate incoming files before processing them\n- Common in web applications where user-submitted files need to be filtered for security\n\n## EDGE CASES:\n- Returns `False` if the filename doesn't contain a period\n- Will raise an exception if filename is not a string (e.g., if None is passed)\n- Handles case insensitivity by converting extensions to lowercase\n\n## RELATIONSHIPS:\n- Works with the `ALLOWED_EXTENSIONS` constant which must be defined elsewhere in the system\n- Often used in conjunction with file upload processing and validation workflows\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has a permitted file extension. This function serves as a security check to ensure only certain file types can be processed.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- Boolean: Returns `True` if the file has an allowed extension, `False` otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period character (`.`)\n- Extract the file extension by splitting from the rightmost period\n- Convert the extension to lowercase\n- Verify if the extension exists in the predefined `ALLOWED_EXTENSIONS` set\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS` (set/list): A global constant containing all permitted file extensions\n\n## USAGE CONTEXT:\n- Typically used in file upload handlers to validate incoming files before processing them\n- Common in web applications where user-submitted files need to be filtered for security\n\n## EDGE CASES:\n- Returns `False` if the filename doesn't contain a period\n- Will raise an exception if filename is not a string (e.g., if None is passed)\n- Handles case insensitivity by converting extensions to lowercase\n\n## RELATIONSHIPS:\n- Works with the `ALLOWED_EXTENSIONS` constant which must be defined elsewhere in the system\n- Often used in conjunction with file upload processing and validation workflows"
    }
  },
  {
    "page_content": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and returns the upload form template to the user. This function serves as a route handler that displays the HTML page where users can upload files.\n\n## INPUTS:\nNone - The function does not accept any parameters.\n\n## OUTPUTS:\n- HTML content (string): The rendered 'upload.html' template.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the argument\n- Returns the resulting rendered HTML to the caller\n\n## DEPENDENCIES:\n- render_template: Function from Flask used to render HTML templates\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application, likely mapped to a URL endpoint like '/upload' or similar. Used when a user navigates to the upload page before submitting files.\n\n## EDGE CASES:\n- If 'upload.html' template doesn't exist, a TemplateNotFound exception would be raised\n- No error handling is implemented within the function itself\n\n## RELATIONSHIPS:\n- Likely works in conjunction with another route that processes the form submission after the user interacts with this upload page\n- Part of a web application's file handling system or workflow\n\ndef upload_form():\n    return render_template('upload.html')",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_form",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "def upload_form():\n    return render_template('upload.html')",
      "summary": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and returns the upload form template to the user. This function serves as a route handler that displays the HTML page where users can upload files.\n\n## INPUTS:\nNone - The function does not accept any parameters.\n\n## OUTPUTS:\n- HTML content (string): The rendered 'upload.html' template.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the argument\n- Returns the resulting rendered HTML to the caller\n\n## DEPENDENCIES:\n- render_template: Function from Flask used to render HTML templates\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application, likely mapped to a URL endpoint like '/upload' or similar. Used when a user navigates to the upload page before submitting files.\n\n## EDGE CASES:\n- If 'upload.html' template doesn't exist, a TemplateNotFound exception would be raised\n- No error handling is implemented within the function itself\n\n## RELATIONSHIPS:\n- Likely works in conjunction with another route that processes the form submission after the user interacts with this upload page\n- Part of a web application's file handling system or workflow"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles HTTP file uploads from a web form, validates the file type, and saves valid files to a designated upload folder on the server.\n\n## INPUTS:\n- None explicitly (function parameters), but implicitly:\n  - `request.files['file']`: File object from the POST request\n  - `request.url`: Current URL for redirects\n\n## OUTPUTS:\n- HTTP redirect response to either:\n  - Homepage ('/') on successful upload\n  - Current page (request.url) on failed upload\n- Flash messages indicating upload status\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Validate that a file was selected\n- Check if the file has an allowed file type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Provide user feedback via flash messages\n\n## DEPENDENCIES:\n- `request`: Flask request object\n- `flash`: Flask function for user notifications\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Werkzeug utility to sanitize filenames\n- `os.path.join`: For path construction\n- `allowed_file`: Helper function to validate file extensions\n- `app.config['UPLOAD_FOLDER']`: Application configuration for upload directory\n\n## USAGE CONTEXT:\nUsed as a route handler in a Flask web application to process file uploads from HTML forms with enctype=\"multipart/form-data\" and method=\"POST\".\n\n## EDGE CASES:\n- No file provided in the request (handled with flash message)\n- Empty filename submitted (handled with flash message)\n- Disallowed file type (handled with flash message and type restriction)\n- The function does not handle file size limits or duplicate filenames\n\n## RELATIONSHIPS:\n- Depends on `allowed_file()` function to validate file extensions\n- Relies on Flask application configuration (`app.config['UPLOAD_FOLDER']`)\n- Typically connected to a route in a Flask application\n- Works with HTML forms that submit files to the application\n\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_checkpoint.py",
      "code": "def upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles HTTP file uploads from a web form, validates the file type, and saves valid files to a designated upload folder on the server.\n\n## INPUTS:\n- None explicitly (function parameters), but implicitly:\n  - `request.files['file']`: File object from the POST request\n  - `request.url`: Current URL for redirects\n\n## OUTPUTS:\n- HTTP redirect response to either:\n  - Homepage ('/') on successful upload\n  - Current page (request.url) on failed upload\n- Flash messages indicating upload status\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Validate that a file was selected\n- Check if the file has an allowed file type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Provide user feedback via flash messages\n\n## DEPENDENCIES:\n- `request`: Flask request object\n- `flash`: Flask function for user notifications\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Werkzeug utility to sanitize filenames\n- `os.path.join`: For path construction\n- `allowed_file`: Helper function to validate file extensions\n- `app.config['UPLOAD_FOLDER']`: Application configuration for upload directory\n\n## USAGE CONTEXT:\nUsed as a route handler in a Flask web application to process file uploads from HTML forms with enctype=\"multipart/form-data\" and method=\"POST\".\n\n## EDGE CASES:\n- No file provided in the request (handled with flash message)\n- Empty filename submitted (handled with flash message)\n- Disallowed file type (handled with flash message and type restriction)\n- The function does not handle file size limits or duplicate filenames\n\n## RELATIONSHIPS:\n- Depends on `allowed_file()` function to validate file extensions\n- Relies on Flask application configuration (`app.config['UPLOAD_FOLDER']`)\n- Typically connected to a route in a Flask application\n- Works with HTML forms that submit files to the application"
    }
  },
  {
    "page_content": "# FILE: app_example.py\n\n## OVERVIEW:\nThis file contains a minimal demonstration functionality, consisting of a single function that returns a greeting message. It appears to be designed for testing, demonstration, or as a placeholder in a development environment.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a simple greeting message with a signature (\"Hello World! - emerald@mathsearch\")\n\n## ARCHITECTURE:\nThe file has a simple architecture with just one standalone function that doesn't interact with other components. It likely serves as a basic example or template.\n\n## DATA FLOW:\nData flow is minimal - the function requires no inputs and simply returns a static string output without any data processing or transformation.\n\n## INTEGRATION POINTS:\nNo explicit integration points are identified. The function appears to be self-contained without dependencies on other system components.\n\n## USAGE PATTERNS:\n- Demonstration of basic functionality\n- Verification of environment setup\n- Use in tutorials or as a starting template\n- Basic testing of system configuration\n\n## DEPENDENCIES:\nNone identified - the function operates independently without external libraries or internal dependencies.\n\n## RELATIONSHIPS:\nThe function operates in isolation with no direct relationships to other system components. It appears to be a standalone utility designed for simplicity.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_example.py",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_example.py",
      "code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8000)",
      "summary": "# FILE: app_example.py\n\n## OVERVIEW:\nThis file contains a minimal demonstration functionality, consisting of a single function that returns a greeting message. It appears to be designed for testing, demonstration, or as a placeholder in a development environment.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a simple greeting message with a signature (\"Hello World! - emerald@mathsearch\")\n\n## ARCHITECTURE:\nThe file has a simple architecture with just one standalone function that doesn't interact with other components. It likely serves as a basic example or template.\n\n## DATA FLOW:\nData flow is minimal - the function requires no inputs and simply returns a static string output without any data processing or transformation.\n\n## INTEGRATION POINTS:\nNo explicit integration points are identified. The function appears to be self-contained without dependencies on other system components.\n\n## USAGE PATTERNS:\n- Demonstration of basic functionality\n- Verification of environment setup\n- Use in tutorials or as a starting template\n- Basic testing of system configuration\n\n## DEPENDENCIES:\nNone identified - the function operates independently without external libraries or internal dependencies.\n\n## RELATIONSHIPS:\nThe function operates in isolation with no direct relationships to other system components. It appears to be a standalone utility designed for simplicity."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nThis function prints a simple greeting message with a signature. It serves as a basic demonstration or test function.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\nString - Returns the greeting \"Hello World! - emerald@mathsearch\"\n\n## KEY STEPS:\n- Returns a hardcoded string containing the greeting and signature\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nThis function is typically used in demonstrations, testing environments, or as a placeholder. It might be used to verify that the environment is functioning correctly or as a starting point in tutorials.\n\n## EDGE CASES:\nNone - this function always returns the same string with no conditional logic or potential for errors.\n\n## RELATIONSHIPS:\nLikely stands alone as a utility or demonstration function with no direct relationships to other system components.\n\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/web/sample_app/app_example.py",
      "code": "def hello_world():\n    return 'Hello World! - emerald@mathsearch'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nThis function prints a simple greeting message with a signature. It serves as a basic demonstration or test function.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\nString - Returns the greeting \"Hello World! - emerald@mathsearch\"\n\n## KEY STEPS:\n- Returns a hardcoded string containing the greeting and signature\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nThis function is typically used in demonstrations, testing environments, or as a placeholder. It might be used to verify that the environment is functioning correctly or as a starting point in tutorials.\n\n## EDGE CASES:\nNone - this function always returns the same string with no conditional logic or potential for errors.\n\n## RELATIONSHIPS:\nLikely stands alone as a utility or demonstration function with no direct relationships to other system components."
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: prev_app\n\n## PURPOSE:\nThis directory contains a Flask-based web application focused on file management, specifically handling file uploads and downloads through a web interface. It provides both user-facing components and server-side processing logic for secure file operations.\n\n## COMPONENT STRUCTURE:\n- `app_checkpoint.py`: The main application file that implements Flask routes for file uploads and downloads, handles form rendering, file validation, and storage operations.\n\n## ARCHITECTURE:\nThe application follows a standard Flask MVC pattern with clear separation between view rendering (form display) and controller logic (file processing). It employs security measures through file validation and uses Flask's routing system to manage different endpoints with appropriate HTTP methods.\n\n## ENTRY POINTS:\n- `upload_form()`: Renders the HTML interface for users to upload files\n- `upload_file()`: Processes POST submissions of files from users\n- `hello_world()`: A legacy entry point that redirects users to the current upload URL\n\n## DATA FLOW:\n1. User requests arrive via HTTP GET to view the upload form or HTTP POST to submit files\n2. Submitted files flow through validation logic (`allowed_file()`) \n3. Valid files are processed and stored in the configured upload directory\n4. Responses flow back to users as redirects with appropriate feedback messages\n\n## INTEGRATION:\nThe application integrates with Flask's templating system for HTML rendering, interacts with the file system for storage operations, and is designed to be part of a larger web application framework. The commented `download()` function suggests planned integration with external file sources.\n\n## DEVELOPMENT PATTERNS:\n- Separation of concerns between display (GET handlers) and processing (POST handlers)\n- Security-focused validation before file operations\n- User feedback through Flask's flash messaging system\n- Configuration-driven approach for file storage locations and allowed file types\n\n## RELATIONSHIPS:\nThe functions in app_checkpoint.py form a cohesive workflow for file management, working together to create a complete user experience from interface display through validation to successful file storage. The application maintains backward compatibility through redirection of legacy endpoints while focusing on secure file operations.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "prev_app",
      "path": "../mathsearch/ml-model/archive/web/prev_app",
      "code": "",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: prev_app\n\n## PURPOSE:\nThis directory contains a Flask-based web application focused on file management, specifically handling file uploads and downloads through a web interface. It provides both user-facing components and server-side processing logic for secure file operations.\n\n## COMPONENT STRUCTURE:\n- `app_checkpoint.py`: The main application file that implements Flask routes for file uploads and downloads, handles form rendering, file validation, and storage operations.\n\n## ARCHITECTURE:\nThe application follows a standard Flask MVC pattern with clear separation between view rendering (form display) and controller logic (file processing). It employs security measures through file validation and uses Flask's routing system to manage different endpoints with appropriate HTTP methods.\n\n## ENTRY POINTS:\n- `upload_form()`: Renders the HTML interface for users to upload files\n- `upload_file()`: Processes POST submissions of files from users\n- `hello_world()`: A legacy entry point that redirects users to the current upload URL\n\n## DATA FLOW:\n1. User requests arrive via HTTP GET to view the upload form or HTTP POST to submit files\n2. Submitted files flow through validation logic (`allowed_file()`) \n3. Valid files are processed and stored in the configured upload directory\n4. Responses flow back to users as redirects with appropriate feedback messages\n\n## INTEGRATION:\nThe application integrates with Flask's templating system for HTML rendering, interacts with the file system for storage operations, and is designed to be part of a larger web application framework. The commented `download()` function suggests planned integration with external file sources.\n\n## DEVELOPMENT PATTERNS:\n- Separation of concerns between display (GET handlers) and processing (POST handlers)\n- Security-focused validation before file operations\n- User feedback through Flask's flash messaging system\n- Configuration-driven approach for file storage locations and allowed file types\n\n## RELATIONSHIPS:\nThe functions in app_checkpoint.py form a cohesive workflow for file management, working together to create a complete user experience from interface display through validation to successful file storage. The application maintains backward compatibility through redirection of legacy endpoints while focusing on secure file operations."
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a Flask-based web application for handling file uploads and downloads, providing both the user interface for file submission and the server-side functionality to process, validate, and store uploaded files.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message redirecting users to an alternative upload URL\n- `download()`: A placeholder function (currently commented out) intended for downloading files from external URLs\n- `allowed_file(filename)`: Validates if a file has an approved extension for security purposes\n- `upload_form()`: Renders the HTML upload interface for users\n- `upload_file()`: Processes file upload submissions, validates them, and saves valid files\n\n## ARCHITECTURE:\nThe file follows a standard Flask web application pattern with route handlers for different endpoints. It separates concerns between displaying forms (GET requests handled by `upload_form()`) and processing submissions (POST requests handled by `upload_file()`), with utility functions for validation.\n\n## DATA FLOW:\n1. Users access the application through either the upload form page or an outdated endpoint\n2. When submitting files, data flows from the client to the server where it's processed by `upload_file()`\n3. Files are validated using `allowed_file()` before being saved to the configured upload directory\n4. Users receive feedback via redirects and flash messages about the success or failure of their upload\n\n## INTEGRATION POINTS:\n- Connects to Flask's templating system through `render_template`\n- Interacts with the file system for storing uploaded files\n- Likely integrates with a larger web application through Flask's routing system\n- May connect to external URLs through the (currently disabled) download functionality\n\n## USAGE PATTERNS:\n- Users access the upload form through a GET request\n- Files are submitted via POST requests and processed by the application\n- Invalid requests (wrong file types, no file selected) result in error messages and redirects\n- Successful uploads are saved to the server and users are redirected to the homepage\n- Users attempting to access outdated endpoints receive redirection messages\n\n## DEPENDENCIES:\n- Flask framework and its components (request, flash, redirect)\n- Flask's render_template for HTML generation\n- werkzeug.utils for secure_filename\n- Python's os.path module for file operations\n- External configuration for UPLOAD_FOLDER and ALLOWED_EXTENSIONS\n- HTML templates, particularly 'upload.html'\n\n## RELATIONSHIPS:\nThe functions work together to create a complete file upload workflow - from displaying the interface, to validating submissions, to processing and storing files. The `hello_world()` function serves as a legacy endpoint redirector, while the commented `download()` function hints at planned expansion of file management capabilities.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_checkpoint.py",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "from werkzeug.utils import secure_filename\nfrom flask import Flask, flash, request, redirect, render_template\nimport urllib.request\nimport requests\nimport os\nfrom flask import Flask\nimport wget\n\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n\napp = Flask(__name__)\napp.secret_key = \"secret key\"\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n\n\n@app.route('/')\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'\n\n# https://www.cs.cornell.edu/~kozen/Papers/daa.pdf\n\n@app.route('/pdf', methods=['GET', 'POST'])\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"\n    # return send_to_directory(app.config['UPLOAD_FOLDER'], link)\n    # return send_file(link, as_attachment=True)\n\n\nALLOWED_EXTENSIONS = set(['pdf'])\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n@app.route('/upload')\ndef upload_form():\n    return render_template('upload.html')\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)\n\nif __name__ == \"__main__\":\n    app.run()\n\n\n\n# # from flask import Flask, render_template, request\n# # # from werkzeug import secure_filename\n# # from werkzeug.utils import secure_filename\n# # from werkzeug.datastructures import  FileStorage\n# # app = Flask(__name__)\n\n# # @app.route('/')\n# # def hello_world():\n# #     return 'Hello World! - emerald@mathsearch port:3000 temp:4'\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\t\n# # @app.route('/uploader', methods = ['GET', 'POST'])\n# # def uploadfile():\n# #    if request.method == 'POST':\n# #       f = request.files['file']\n# #       f.save(secure_filename(f.filename))\n# #       return 'file uploaded successfully'\n\t\t\n# # if __name__ == '__main__':\n# #     app.debug = True\n# #     app.run(host='0.0.0.0', port=8100)\n\n\n# from flask import Flask, flash, redirect, url_for, request, render_template\n# from werkzeug.utils import secure_filename\n# import os\n\n# \"\"\"\n# @Author: Emerald Liu\n# Does not support concurrency currently\n# \"\"\"\n\n# # constant variables\n# UPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n# ALLOWED_EXTENSIONS = {'pdf'}\n\n# # helper functions\n# def allowed_file(filename):\n#     return '.' in filename and \\\n#         filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n# # initalize flask app config\n# app = Flask(__name__)\n# app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\n\n# # @app.route('/upload', methods=['GET', 'POST'])\n# # def upload_file():\n# #     if request.method == 'POST':\n# #         # check if the post request has the file part\n# #         if 'file' not in request.files:\n# #             flash('No file part')\n# #             return redirect(request.url)\n# #         file = request.files['file']\n# #         # If the user does not select a file, the browser submits an\n# #         # empty file without a filename.\n# #         if file.filename == '':\n# #             flash('No selected file')\n# #             return redirect(request.url)\n# #         if file and allowed_file(file.filename):\n# #             filename = secure_filename(file.filename)\n# #             # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# #             file.save(app.config['UPLOAD_FOLDER'], filename)\n# #             return redirect(url_for('download_file', name=filename))\n# #     return '''\n# #     <!doctype html>\n# #     <title>Upload new File</title>\n# #     <h1>Upload new File</h1>\n# #     <form method=post enctype=multipart/form-data>\n# #       <input type=file name=file>\n# #       <input type=submit value=Upload>\n# #     </form>\n# #     '''\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n\n\n# import os\n# import urllib.request\n# # from app import app\n# from flask import Flask, flash, request, redirect, render_template\n# from werkzeug.utils import secure_filename\n\n# ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'])\n# app = Flask(__name__)\n\n# def allowed_file(filename):\n# \treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\t\n# @app.route('/')\n# def upload_form():\n# \treturn render_template('upload.html')\n\n# @app.route('/', methods=['POST'])\n# def upload_file():\n# \tif request.method == 'POST':\n#         # check if the post request has the file part\n# \t\tif 'file' not in request.files:\n# \t\t\tflash('No file part')\n# \t\t\treturn redirect(request.url)\n# \t\tfile = request.files['file']\n# \t\tif file.filename == '':\n# \t\t\tflash('No file selected for uploading')\n# \t\t\treturn redirect(request.url)\n# \t\tif file and allowed_file(file.filename):\n# \t\t\tfilename = secure_filename(file.filename)\n# \t\t\tfile.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# \t\t\tflash('File successfully uploaded')\n# \t\t\treturn redirect('/')\n# \t\telse:\n# \t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n# \t\t\treturn redirect(request.url)\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a Flask-based web application for handling file uploads and downloads, providing both the user interface for file submission and the server-side functionality to process, validate, and store uploaded files.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message redirecting users to an alternative upload URL\n- `download()`: A placeholder function (currently commented out) intended for downloading files from external URLs\n- `allowed_file(filename)`: Validates if a file has an approved extension for security purposes\n- `upload_form()`: Renders the HTML upload interface for users\n- `upload_file()`: Processes file upload submissions, validates them, and saves valid files\n\n## ARCHITECTURE:\nThe file follows a standard Flask web application pattern with route handlers for different endpoints. It separates concerns between displaying forms (GET requests handled by `upload_form()`) and processing submissions (POST requests handled by `upload_file()`), with utility functions for validation.\n\n## DATA FLOW:\n1. Users access the application through either the upload form page or an outdated endpoint\n2. When submitting files, data flows from the client to the server where it's processed by `upload_file()`\n3. Files are validated using `allowed_file()` before being saved to the configured upload directory\n4. Users receive feedback via redirects and flash messages about the success or failure of their upload\n\n## INTEGRATION POINTS:\n- Connects to Flask's templating system through `render_template`\n- Interacts with the file system for storing uploaded files\n- Likely integrates with a larger web application through Flask's routing system\n- May connect to external URLs through the (currently disabled) download functionality\n\n## USAGE PATTERNS:\n- Users access the upload form through a GET request\n- Files are submitted via POST requests and processed by the application\n- Invalid requests (wrong file types, no file selected) result in error messages and redirects\n- Successful uploads are saved to the server and users are redirected to the homepage\n- Users attempting to access outdated endpoints receive redirection messages\n\n## DEPENDENCIES:\n- Flask framework and its components (request, flash, redirect)\n- Flask's render_template for HTML generation\n- werkzeug.utils for secure_filename\n- Python's os.path module for file operations\n- External configuration for UPLOAD_FOLDER and ALLOWED_EXTENSIONS\n- HTML templates, particularly 'upload.html'\n\n## RELATIONSHIPS:\nThe functions work together to create a complete file upload workflow - from displaying the interface, to validating submissions, to processing and storing files. The `hello_world()` function serves as a legacy endpoint redirector, while the commented `download()` function hints at planned expansion of file management capabilities."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a warning message directing users to an alternative upload URL. This function likely serves as a redirect notification for users accessing a deprecated or incorrect endpoint.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: A warning message instructing users to go to \"http://18.207.249.45/upload\"\n\n## KEY STEPS:\n- Return a static warning string message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nLikely used in a web application or API as a response for an outdated or deprecated endpoint. It provides guidance to users who might be attempting to access a service through an incorrect path.\n\n## EDGE CASES:\nNone apparent in the implementation\n\n## RELATIONSHIPS:\nProbably relates to an upload feature in the system, redirecting users from an old endpoint to the current upload functionality located at the specified IP address.\n\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "def hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a warning message directing users to an alternative upload URL. This function likely serves as a redirect notification for users accessing a deprecated or incorrect endpoint.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: A warning message instructing users to go to \"http://18.207.249.45/upload\"\n\n## KEY STEPS:\n- Return a static warning string message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nLikely used in a web application or API as a response for an outdated or deprecated endpoint. It provides guidance to users who might be attempting to access a service through an incorrect path.\n\n## EDGE CASES:\nNone apparent in the implementation\n\n## RELATIONSHIPS:\nProbably relates to an upload feature in the system, redirecting users from an old endpoint to the current upload functionality located at the specified IP address."
    }
  },
  {
    "page_content": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: download\n\nPURPOSE:\nThis function appears to be a placeholder for downloading a file from a URL. Currently, all implementation code is commented out, suggesting it's under development or disabled.\n\nINPUTS:\nNone explicitly defined, though commented code suggests it would use:\n- A URL parameter 'c' from an HTTP request\n\nOUTPUTS:\n- Returns a string \"success\" regardless of execution\n\nKEY STEPS:\n- Currently no active steps as all implementation code is commented out\n- Commented code shows three different implementation approaches:\n  * Using wget to download a file\n  * Using requests.get with write to file\n  * Using requests.get with context manager to write to file\n\nDEPENDENCIES:\nNone active, but commented code references:\n- request (likely from Flask)\n- wget\n- requests\n- app.config (likely Flask application configuration)\n- UPLOAD_FOLDER (a constant or configuration value)\n\nUSAGE CONTEXT:\nLikely intended as an HTTP endpoint in a web application for downloading files from external URLs to a server-side storage location.\n\nEDGE CASES:\n- Currently returns \"success\" regardless of whether any download occurred\n- No error handling implemented\n- No validation of URL or file types\n\nRELATIONSHIPS:\n- Appears to be part of a Flask web application\n- Likely related to file management functionality within the application\n- May be connected to an upload or file processing system\n\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "download",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "def download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
      "summary": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: download\n\nPURPOSE:\nThis function appears to be a placeholder for downloading a file from a URL. Currently, all implementation code is commented out, suggesting it's under development or disabled.\n\nINPUTS:\nNone explicitly defined, though commented code suggests it would use:\n- A URL parameter 'c' from an HTTP request\n\nOUTPUTS:\n- Returns a string \"success\" regardless of execution\n\nKEY STEPS:\n- Currently no active steps as all implementation code is commented out\n- Commented code shows three different implementation approaches:\n  * Using wget to download a file\n  * Using requests.get with write to file\n  * Using requests.get with context manager to write to file\n\nDEPENDENCIES:\nNone active, but commented code references:\n- request (likely from Flask)\n- wget\n- requests\n- app.config (likely Flask application configuration)\n- UPLOAD_FOLDER (a constant or configuration value)\n\nUSAGE CONTEXT:\nLikely intended as an HTTP endpoint in a web application for downloading files from external URLs to a server-side storage location.\n\nEDGE CASES:\n- Currently returns \"success\" regardless of whether any download occurred\n- No error handling implemented\n- No validation of URL or file types\n\nRELATIONSHIPS:\n- Appears to be part of a Flask web application\n- Likely related to file management functionality within the application\n- May be connected to an upload or file processing system"
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nChecks if a given filename has a valid extension that is allowed by the application. This is typically used to validate file uploads for security purposes.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked for a valid extension\n\n## OUTPUTS:\n- Boolean: Returns `True` if the file extension is allowed, `False` otherwise\n\n## KEY STEPS:\n- Verifies that the filename contains a period (indicating an extension exists)\n- Extracts the extension by splitting the filename at the last period\n- Converts the extension to lowercase\n- Checks if the lowercase extension is in the predefined `ALLOWED_EXTENSIONS` set\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS`: A global set or list containing the permitted file extensions\n\n## USAGE CONTEXT:\n- Used in file upload handlers to validate that only files with approved extensions are accepted\n- Typically called before saving or processing an uploaded file\n- Common in web applications with file upload functionality\n\n## EDGE CASES:\n- Returns `False` if the filename does not contain a period\n- If a filename has multiple periods, only the text after the last period is considered the extension\n- Handles extensions case-insensitively by converting to lowercase\n\n## RELATIONSHIPS:\n- Works in conjunction with file upload routes or handlers\n- Depends on the globally defined `ALLOWED_EXTENSIONS` variable\n- Typically part of the security validation chain for file uploads\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nChecks if a given filename has a valid extension that is allowed by the application. This is typically used to validate file uploads for security purposes.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked for a valid extension\n\n## OUTPUTS:\n- Boolean: Returns `True` if the file extension is allowed, `False` otherwise\n\n## KEY STEPS:\n- Verifies that the filename contains a period (indicating an extension exists)\n- Extracts the extension by splitting the filename at the last period\n- Converts the extension to lowercase\n- Checks if the lowercase extension is in the predefined `ALLOWED_EXTENSIONS` set\n\n## DEPENDENCIES:\n- `ALLOWED_EXTENSIONS`: A global set or list containing the permitted file extensions\n\n## USAGE CONTEXT:\n- Used in file upload handlers to validate that only files with approved extensions are accepted\n- Typically called before saving or processing an uploaded file\n- Common in web applications with file upload functionality\n\n## EDGE CASES:\n- Returns `False` if the filename does not contain a period\n- If a filename has multiple periods, only the text after the last period is considered the extension\n- Handles extensions case-insensitively by converting to lowercase\n\n## RELATIONSHIPS:\n- Works in conjunction with file upload routes or handlers\n- Depends on the globally defined `ALLOWED_EXTENSIONS` variable\n- Typically part of the security validation chain for file uploads"
    }
  },
  {
    "page_content": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and returns the upload HTML template to the user. This function creates the web page that allows users to interact with the file upload interface.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- HTML content (string): The rendered 'upload.html' template that will be displayed in the user's browser.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the argument\n- Returns the resulting HTML content to the caller\n\n## DEPENDENCIES:\n- Flask's render_template function\n- The 'upload.html' template file must exist in the templates directory\n\n## USAGE CONTEXT:\nThis function is typically used as a route handler in a Flask web application, responding to GET requests to an upload page URL (e.g., '/upload'). It displays the initial upload form before any file submission occurs.\n\n## EDGE CASES:\n- If the 'upload.html' template doesn't exist, a TemplateNotFound exception will be raised\n- No input validation is needed as the function takes no parameters\n\n## RELATIONSHIPS:\n- Serves as the view function that precedes file upload handling\n- Likely works with another route that processes the form submission using POST requests\n- Part of the web interface layer of the application, connecting the user to file upload functionality\n\ndef upload_form():\n    return render_template('upload.html')",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_form",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "def upload_form():\n    return render_template('upload.html')",
      "summary": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and returns the upload HTML template to the user. This function creates the web page that allows users to interact with the file upload interface.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- HTML content (string): The rendered 'upload.html' template that will be displayed in the user's browser.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the argument\n- Returns the resulting HTML content to the caller\n\n## DEPENDENCIES:\n- Flask's render_template function\n- The 'upload.html' template file must exist in the templates directory\n\n## USAGE CONTEXT:\nThis function is typically used as a route handler in a Flask web application, responding to GET requests to an upload page URL (e.g., '/upload'). It displays the initial upload form before any file submission occurs.\n\n## EDGE CASES:\n- If the 'upload.html' template doesn't exist, a TemplateNotFound exception will be raised\n- No input validation is needed as the function takes no parameters\n\n## RELATIONSHIPS:\n- Serves as the view function that precedes file upload handling\n- Likely works with another route that processes the form submission using POST requests\n- Part of the web interface layer of the application, connecting the user to file upload functionality"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file upload requests in a web application, validating the file, saving it to the designated upload folder, and providing user feedback through flash messages.\n\n## INPUTS:\n- No explicit parameters, but relies on the global `request` object containing:\n  - `request.files['file']`: The file submitted by the user through a form\n\n## OUTPUTS:\n- `redirect` object: Redirects to either the homepage ('/') on success or back to the current page on failure\n\n## KEY STEPS:\n- Checks if the HTTP request method is POST\n- Verifies that the request contains a file part\n- Validates that a file was actually selected (filename is not empty)\n- Confirms the file type is allowed by calling `allowed_file()`\n- Secures the filename to prevent malicious paths\n- Saves the file to the configured upload directory\n- Provides feedback to the user via flash messages\n\n## DEPENDENCIES:\n- `request`: Flask request object\n- `flash`: Flask function for user messaging\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Function to sanitize filenames\n- `os.path`: For file path operations\n- `allowed_file()`: Function to validate file extensions\n- `app.config['UPLOAD_FOLDER']`: Configuration setting for upload location\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application that responds to POST requests from a file upload form.\n\n## EDGE CASES:\n- Missing file part in the request (returns to original page with error message)\n- Empty filename (returns to original page with error message)\n- Disallowed file type (returns to original page with error message listing allowed types)\n\n## RELATIONSHIPS:\n- Relates to the Flask application routing system\n- Depends on the application's configuration settings for the upload folder\n- Works with HTML forms that post files to this endpoint\n- Used in conjunction with templates that display flash messages\n\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/web/prev_app/app_checkpoint.py",
      "code": "def upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file upload requests in a web application, validating the file, saving it to the designated upload folder, and providing user feedback through flash messages.\n\n## INPUTS:\n- No explicit parameters, but relies on the global `request` object containing:\n  - `request.files['file']`: The file submitted by the user through a form\n\n## OUTPUTS:\n- `redirect` object: Redirects to either the homepage ('/') on success or back to the current page on failure\n\n## KEY STEPS:\n- Checks if the HTTP request method is POST\n- Verifies that the request contains a file part\n- Validates that a file was actually selected (filename is not empty)\n- Confirms the file type is allowed by calling `allowed_file()`\n- Secures the filename to prevent malicious paths\n- Saves the file to the configured upload directory\n- Provides feedback to the user via flash messages\n\n## DEPENDENCIES:\n- `request`: Flask request object\n- `flash`: Flask function for user messaging\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Function to sanitize filenames\n- `os.path`: For file path operations\n- `allowed_file()`: Function to validate file extensions\n- `app.config['UPLOAD_FOLDER']`: Configuration setting for upload location\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application that responds to POST requests from a file upload form.\n\n## EDGE CASES:\n- Missing file part in the request (returns to original page with error message)\n- Empty filename (returns to original page with error message)\n- Disallowed file type (returns to original page with error message listing allowed types)\n\n## RELATIONSHIPS:\n- Relates to the Flask application routing system\n- Depends on the application's configuration settings for the upload folder\n- Works with HTML forms that post files to this endpoint\n- Used in conjunction with templates that display flash messages"
    }
  },
  {
    "page_content": "# FILE: call_example.py\n\n## OVERVIEW:\nThis file appears to be empty or does not contain any defined functions, as no function summaries were provided in the input.\n\n## KEY COMPONENTS:\nNo functions were identified in the file.\n\n## ARCHITECTURE:\nNot applicable as no functions were identified.\n\n## DATA FLOW:\nNo data flow can be described due to the absence of functions.\n\n## INTEGRATION POINTS:\nCannot determine integration points without function definitions.\n\n## USAGE PATTERNS:\nUnable to determine usage patterns from the provided information.\n\n## DEPENDENCIES:\nNo dependencies could be identified from the input.\n\n## RELATIONSHIPS:\nNo function relationships can be established as no functions were identified.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "call_example.py",
      "path": "../mathsearch/ml-model/archive/web/call_example.py",
      "code": "import requests\n\n# to send result back to front-end\n\nURL = \"http://3.94.25.91/api/result\"\njson = {\n    \"file\":\"ex1.pdf\",\n    \"coords\":\"0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889\"\n}\n\nres = requests.get(URL, json=json)\n\nres = print(res) # OK = 200",
      "summary": "# FILE: call_example.py\n\n## OVERVIEW:\nThis file appears to be empty or does not contain any defined functions, as no function summaries were provided in the input.\n\n## KEY COMPONENTS:\nNo functions were identified in the file.\n\n## ARCHITECTURE:\nNot applicable as no functions were identified.\n\n## DATA FLOW:\nNo data flow can be described due to the absence of functions.\n\n## INTEGRATION POINTS:\nCannot determine integration points without function definitions.\n\n## USAGE PATTERNS:\nUnable to determine usage patterns from the provided information.\n\n## DEPENDENCIES:\nNo dependencies could be identified from the input.\n\n## RELATIONSHIPS:\nNo function relationships can be established as no functions were identified."
    }
  },
  {
    "page_content": "# FILE: api.py\n\n## OVERVIEW:\nThis file implements a web API for a machine learning system called \"MathSearch\" that processes PDF and image files through a YOLOv5 model, with endpoints for file uploads, model execution, and coordination with S3 storage.\n\n## KEY COMPONENTS:\n- `add_cors_headers`: Adds CORS headers to HTTP responses for cross-origin requests\n- `start`: Returns URLs for accessing various endpoints of the service\n- `download`: Captures S3 bucket and object parameters from requests and stores them for YOLOv5 processing\n- `allowed_file`: Validates file extensions against permitted types\n- `upload_form`: Renders the file upload form to users\n- `upload_file`: Handles file upload validation, storage, and user feedback\n- `print_test`: Returns a static string for testing purposes\n- `run_model`: Processes PDF and image files through a machine learning model via subprocess\n\n## ARCHITECTURE:\nThe API follows a typical Flask application structure with route handlers for different endpoints. It implements file upload capabilities, S3 integration, and acts as an interface to a YOLOv5 machine learning model running as a separate process.\n\n## DATA FLOW:\n1. Users interact with the API through web endpoints\n2. Files can be uploaded through the `/upload` endpoint\n3. S3 resource identifiers can be received via the download function\n4. The system processes these files using the YOLOv5 model via the `/run` endpoint\n5. Results are returned to users after model execution\n\n## INTEGRATION POINTS:\n- S3 storage: Accepts bucket and object parameters for processing\n- YOLOv5 model: Runs an external Python script at \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n- Web frontend: Provides endpoints that can be accessed from a browser\n\n## USAGE PATTERNS:\n1. Users navigate to endpoints using information from the start function\n2. Files are uploaded through a form rendered by upload_form and processed by upload_file\n3. S3 resources are specified via the download function\n4. The run_model function processes PDF and image files through the ML model\n\n## DEPENDENCIES:\n- Flask framework (request, render_template, redirect, flash)\n- subprocess module for calling external scripts\n- time module for performance measurement\n- Secure file handling utilities (secure_filename)\n- External YOLOv5 model script\n\n## RELATIONSHIPS:\nThe functions form a cohesive API where:\n- Navigation support is provided by the start function\n- File handling is managed by upload_form, upload_file, and allowed_file\n- S3 integration is handled by the download function\n- Cross-origin support is provided by add_cors_headers\n- The core ML functionality is exposed through run_model\n- Testing and debugging support is provided by print_test",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "api.py",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "\"\"\"\n\n@author: Emerald Liu\n\n1. Test deploy\nhttp://44.192.0.110/test\n\n2. Run machine learning model\nhttp://44.192.0.110/run\n{\n   \"uuid\":\"uuidKey1\",\n   \"pdf_path\":\"inputs/a0eeed5e-1d18-42ff-a347-878785830dc0_pdf\",\n   \"image_path\":\"inputs/a0eeed5e-1d18-42ff-a347-878785830dc0_image\"\n}\n\n\"\"\"\n\nfrom werkzeug.utils import secure_filename\nfrom flask import Flask, flash, request, redirect, render_template\nimport urllib.request\nimport requests\nimport os\nimport json\nimport sys\nimport time\nimport subprocess\nfrom subprocess import call\nimport shutil\n# import pandas as pd\n# import boto3\n\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n\napp = Flask(__name__)\n\n\n@app.after_request\ndef add_cors_headers(response):\n\tresponse.headers.add('Access-Control-Allow-Origin', '*')\n\tresponse.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n\t# response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE')\n\treturn response\n\n@app.route('/')\ndef start():\n\treturn 'visit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload'\n\n# https://www.cs.cornell.edu/~kozen/Papers/daa.pdf\n\n\n# @app.route('/coord', methods=['GET'])\n# def get_coord():\n# \tf = open('/home/ubuntu/yolov5/ranking/top5.txt', \"r\")\n# \treturn f.read()\n\n\n@app.route('/model', methods=['GET', 'POST'])\ndef download():\n\ts3_bucket = request.args.get('b')\n\ts3_object = request.args.get('o')\n\twith open(\"/home/ubuntu/yolov5/input_info/names.txt\", \"w\") as f:\n\t\tf.write(s3_bucket+\"\\n\"+s3_object)\n\treturn s3_bucket+\"\\n\"+s3_object+\"\\nPassed data info successfully!\"\n\n\nALLOWED_EXTENSIONS = set(['pdf'])\n\n\ndef allowed_file(filename):\n\treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/upload')\ndef upload_form():\n\treturn render_template('upload.html')\n\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n\tif request.method == 'POST':\n\t\t# check if the post request has the file part\n\t\tif 'file' not in request.files:\n\t\t\tflash('No file part')\n\t\t\treturn redirect(request.url)\n\t\tfile = request.files['file']\n\t\tif file.filename == '':\n\t\t\tflash('No file selected for uploading')\n\t\t\treturn redirect(request.url)\n\t\tif file and allowed_file(file.filename):\n\t\t\tfilename = secure_filename(file.filename)\n\t\t\tfile.save(os.path.join(UPLOAD_FOLDER, filename))\n\t\t\tflash('File successfully uploaded')\n\t\t\treturn redirect('/')\n\t\telse:\n\t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n\t\t\treturn redirect(request.url)\n\n@app.route('/test')\ndef print_test():\n\treturn \"ok-update 5/6\"\n\n@app.route('/run', methods=['POST'])\ndef run_model():\n\tprint(\"/run called\")\n\tdata = request.json\n\tuuid = data[\"uuid\"]\n\tprint(\"before\",data[\"pdf_path\"])\n\tprint(\"before\",data[\"image_path\"])\n\tpdf_path = data[\"pdf_path\"][7:]\n\timage_path = data[\"image_path\"][7:]\n\tprint(pdf_path)\n\tprint(image_path)\n\tmessage = uuid + \" \" + pdf_path + \" \" + image_path\n\t# import time\n\t# os.chdir('/home/ubuntu/yolov5')\n\t# sys.path.append('/home/ubuntu/yolov5')\n\t# import main\n\t# main.main()\n\t# time.sleep(3)\n\t# return \"running model...\\n\\nVisit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload\"\n\tstart = time.time()\n\t# venv_py = \"/home/ubuntu/MathSearch/ml-model/venv/bin/python3\"\n\tvenv_py = \"/opt/conda/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n\tsubprocess.call([venv_py, python_file, pdf_path, image_path])\n\tend = time.time()\n\treturn message + \"\\nimporting ok\\naccessing yolov5/main.py ok\" + \"\\n\" + \"ML model finished running.\\nTime used: \" + str(end - start)\n\n\nif __name__ == \"__main__\":\n\tapp.run(debug=True)\n",
      "summary": "# FILE: api.py\n\n## OVERVIEW:\nThis file implements a web API for a machine learning system called \"MathSearch\" that processes PDF and image files through a YOLOv5 model, with endpoints for file uploads, model execution, and coordination with S3 storage.\n\n## KEY COMPONENTS:\n- `add_cors_headers`: Adds CORS headers to HTTP responses for cross-origin requests\n- `start`: Returns URLs for accessing various endpoints of the service\n- `download`: Captures S3 bucket and object parameters from requests and stores them for YOLOv5 processing\n- `allowed_file`: Validates file extensions against permitted types\n- `upload_form`: Renders the file upload form to users\n- `upload_file`: Handles file upload validation, storage, and user feedback\n- `print_test`: Returns a static string for testing purposes\n- `run_model`: Processes PDF and image files through a machine learning model via subprocess\n\n## ARCHITECTURE:\nThe API follows a typical Flask application structure with route handlers for different endpoints. It implements file upload capabilities, S3 integration, and acts as an interface to a YOLOv5 machine learning model running as a separate process.\n\n## DATA FLOW:\n1. Users interact with the API through web endpoints\n2. Files can be uploaded through the `/upload` endpoint\n3. S3 resource identifiers can be received via the download function\n4. The system processes these files using the YOLOv5 model via the `/run` endpoint\n5. Results are returned to users after model execution\n\n## INTEGRATION POINTS:\n- S3 storage: Accepts bucket and object parameters for processing\n- YOLOv5 model: Runs an external Python script at \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n- Web frontend: Provides endpoints that can be accessed from a browser\n\n## USAGE PATTERNS:\n1. Users navigate to endpoints using information from the start function\n2. Files are uploaded through a form rendered by upload_form and processed by upload_file\n3. S3 resources are specified via the download function\n4. The run_model function processes PDF and image files through the ML model\n\n## DEPENDENCIES:\n- Flask framework (request, render_template, redirect, flash)\n- subprocess module for calling external scripts\n- time module for performance measurement\n- Secure file handling utilities (secure_filename)\n- External YOLOv5 model script\n\n## RELATIONSHIPS:\nThe functions form a cohesive API where:\n- Navigation support is provided by the start function\n- File handling is managed by upload_form, upload_file, and allowed_file\n- S3 integration is handled by the download function\n- Cross-origin support is provided by add_cors_headers\n- The core ML functionality is exposed through run_model\n- Testing and debugging support is provided by print_test"
    }
  },
  {
    "page_content": "# FUNCTION: add_cors_headers\n\n## PURPOSE:\nAdds Cross-Origin Resource Sharing (CORS) headers to an HTTP response to enable web browsers to make cross-origin requests to the API.\n\n## INPUTS:\n- `response` (Response object): The Flask/HTTP response object to which CORS headers will be added.\n\n## OUTPUTS:\n- (Response object): The same response object with CORS headers added.\n\n## KEY STEPS:\n- Add 'Access-Control-Allow-Origin' header set to '*' to allow requests from any origin\n- Add 'Access-Control-Allow-Headers' header set to 'Content-Type,Authorization' to specify which headers can be used\n- Return the modified response object\n\n## DEPENDENCIES:\n- Requires a web framework that uses response objects with a headers attribute (like Flask)\n\n## USAGE CONTEXT:\n- Typically used as a decorator or middleware in a web API to enable cross-origin requests\n- Often called after generating a response but before sending it to the client\n\n## EDGE CASES:\n- Does not implement the 'Access-Control-Allow-Methods' header which is commented out but might be needed for some APIs\n- Using '*' for Allow-Origin grants access to any domain, which may be too permissive for production environments\n\n## RELATIONSHIPS:\n- Functions as middleware in web applications handling HTTP responses\n- Would typically be used in conjunction with route handlers or error handlers in a web framework\n\ndef add_cors_headers(response):\n\tresponse.headers.add('Access-Control-Allow-Origin', '*')\n\tresponse.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n\t# response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE')\n\treturn response",
    "metadata": {
      "type": "FUNCTION",
      "name": "add_cors_headers",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def add_cors_headers(response):\n\tresponse.headers.add('Access-Control-Allow-Origin', '*')\n\tresponse.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')\n\t# response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE')\n\treturn response",
      "summary": "# FUNCTION: add_cors_headers\n\n## PURPOSE:\nAdds Cross-Origin Resource Sharing (CORS) headers to an HTTP response to enable web browsers to make cross-origin requests to the API.\n\n## INPUTS:\n- `response` (Response object): The Flask/HTTP response object to which CORS headers will be added.\n\n## OUTPUTS:\n- (Response object): The same response object with CORS headers added.\n\n## KEY STEPS:\n- Add 'Access-Control-Allow-Origin' header set to '*' to allow requests from any origin\n- Add 'Access-Control-Allow-Headers' header set to 'Content-Type,Authorization' to specify which headers can be used\n- Return the modified response object\n\n## DEPENDENCIES:\n- Requires a web framework that uses response objects with a headers attribute (like Flask)\n\n## USAGE CONTEXT:\n- Typically used as a decorator or middleware in a web API to enable cross-origin requests\n- Often called after generating a response but before sending it to the client\n\n## EDGE CASES:\n- Does not implement the 'Access-Control-Allow-Methods' header which is commented out but might be needed for some APIs\n- Using '*' for Allow-Origin grants access to any domain, which may be too permissive for production environments\n\n## RELATIONSHIPS:\n- Functions as middleware in web applications handling HTTP responses\n- Would typically be used in conjunction with route handlers or error handlers in a web framework"
    }
  },
  {
    "page_content": "# FUNCTION: start\n\n## PURPOSE:\nThis function provides return URLs for accessing various endpoints of a web service hosted at IP address 18.207.249.45. It serves as an informational guide for users to navigate the available endpoints.\n\n## INPUTS:\nNone - The function takes no parameters.\n\n## OUTPUTS:\nString - Returns a formatted text string containing URLs for essential and optional endpoints.\n\n## KEY STEPS:\n- Creates a formatted string containing URLs for required endpoints (/coord and /model)\n- Adds an optional endpoint URL (/upload)\n- Returns the complete string\n\n## DEPENDENCIES:\nNone - The function has no external dependencies.\n\n## USAGE CONTEXT:\nLikely used as an informational function to guide users on how to interact with the web service. May be called when users need directions on available endpoints.\n\n## EDGE CASES:\nNone apparent - The function is simple and does not handle any special conditions or errors.\n\n## RELATIONSHIPS:\nThis function likely serves as a helper or informational component for a larger web application system running at 18.207.249.45, providing navigation guidance to the system's main endpoints (/coord, /model, and /upload).\n\ndef start():\n\treturn 'visit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload'",
    "metadata": {
      "type": "FUNCTION",
      "name": "start",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def start():\n\treturn 'visit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload'",
      "summary": "# FUNCTION: start\n\n## PURPOSE:\nThis function provides return URLs for accessing various endpoints of a web service hosted at IP address 18.207.249.45. It serves as an informational guide for users to navigate the available endpoints.\n\n## INPUTS:\nNone - The function takes no parameters.\n\n## OUTPUTS:\nString - Returns a formatted text string containing URLs for essential and optional endpoints.\n\n## KEY STEPS:\n- Creates a formatted string containing URLs for required endpoints (/coord and /model)\n- Adds an optional endpoint URL (/upload)\n- Returns the complete string\n\n## DEPENDENCIES:\nNone - The function has no external dependencies.\n\n## USAGE CONTEXT:\nLikely used as an informational function to guide users on how to interact with the web service. May be called when users need directions on available endpoints.\n\n## EDGE CASES:\nNone apparent - The function is simple and does not handle any special conditions or errors.\n\n## RELATIONSHIPS:\nThis function likely serves as a helper or informational component for a larger web application system running at 18.207.249.45, providing navigation guidance to the system's main endpoints (/coord, /model, and /upload)."
    }
  },
  {
    "page_content": "# FUNCTION: download\n\n## PURPOSE:\nThis function captures S3 bucket and object parameters from a web request and stores them in a file for later use in a YOLOv5 processing pipeline.\n\n## INPUTS:\n- No direct parameters; pulls data from query string parameters:\n  - 'b': String representing the S3 bucket name\n  - 'o': String representing the S3 object key\n\n## OUTPUTS:\n- String: A concatenated string containing the S3 bucket name, S3 object key, and a success message\n\n## KEY STEPS:\n- Extract 'b' (bucket) and 'o' (object) parameters from the HTTP request's query string\n- Write these values to a file named \"names.txt\" in a specific directory\n- Return a confirmation string that includes both values and a success message\n\n## DEPENDENCIES:\n- Flask's 'request' object for accessing query parameters\n\n## USAGE CONTEXT:\n- Used in a web application context to receive S3 resource identifiers from HTTP requests\n- Likely part of a processing pipeline for YOLOv5 object detection that reads input data from S3\n\n## EDGE CASES:\n- No validation for missing query parameters\n- No error handling for file I/O operations\n- No sanitization of input values\n\n## RELATIONSHIPS:\n- Acts as a bridge between a web frontend and a YOLOv5 processing backend\n- Creates a file that is presumably read by other parts of the system to locate input data in S3\n\ndef download():\n\ts3_bucket = request.args.get('b')\n\ts3_object = request.args.get('o')\n\twith open(\"/home/ubuntu/yolov5/input_info/names.txt\", \"w\") as f:\n\t\tf.write(s3_bucket+\"\\n\"+s3_object)\n\treturn s3_bucket+\"\\n\"+s3_object+\"\\nPassed data info successfully!\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "download",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def download():\n\ts3_bucket = request.args.get('b')\n\ts3_object = request.args.get('o')\n\twith open(\"/home/ubuntu/yolov5/input_info/names.txt\", \"w\") as f:\n\t\tf.write(s3_bucket+\"\\n\"+s3_object)\n\treturn s3_bucket+\"\\n\"+s3_object+\"\\nPassed data info successfully!\"",
      "summary": "# FUNCTION: download\n\n## PURPOSE:\nThis function captures S3 bucket and object parameters from a web request and stores them in a file for later use in a YOLOv5 processing pipeline.\n\n## INPUTS:\n- No direct parameters; pulls data from query string parameters:\n  - 'b': String representing the S3 bucket name\n  - 'o': String representing the S3 object key\n\n## OUTPUTS:\n- String: A concatenated string containing the S3 bucket name, S3 object key, and a success message\n\n## KEY STEPS:\n- Extract 'b' (bucket) and 'o' (object) parameters from the HTTP request's query string\n- Write these values to a file named \"names.txt\" in a specific directory\n- Return a confirmation string that includes both values and a success message\n\n## DEPENDENCIES:\n- Flask's 'request' object for accessing query parameters\n\n## USAGE CONTEXT:\n- Used in a web application context to receive S3 resource identifiers from HTTP requests\n- Likely part of a processing pipeline for YOLOv5 object detection that reads input data from S3\n\n## EDGE CASES:\n- No validation for missing query parameters\n- No error handling for file I/O operations\n- No sanitization of input values\n\n## RELATIONSHIPS:\n- Acts as a bridge between a web frontend and a YOLOv5 processing backend\n- Creates a file that is presumably read by other parts of the system to locate input data in S3"
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nChecks if a given filename has an allowed file extension. This function is used to validate file types before processing or storage.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- (boolean): Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the extension is in the predefined ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A collection of permitted file extensions defined elsewhere in the code\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate that uploaded files have acceptable extensions before saving or processing them, often in web applications.\n\n## EDGE CASES:\n- Returns False if filename has no period (no extension)\n- May raise an exception if filename is None or not a string\n- Filenames with multiple periods are handled by taking only the part after the last period\n\n## RELATIONSHIPS:\n- Likely used by file upload routes or handlers\n- Depends on the ALLOWED_EXTENSIONS constant which defines the application's file type policy\n\ndef allowed_file(filename):\n\treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def allowed_file(filename):\n\treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nChecks if a given filename has an allowed file extension. This function is used to validate file types before processing or storage.\n\n## INPUTS:\n- `filename` (string): The name of the file to be checked, including its extension\n\n## OUTPUTS:\n- (boolean): Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the extension is in the predefined ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A collection of permitted file extensions defined elsewhere in the code\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate that uploaded files have acceptable extensions before saving or processing them, often in web applications.\n\n## EDGE CASES:\n- Returns False if filename has no period (no extension)\n- May raise an exception if filename is None or not a string\n- Filenames with multiple periods are handled by taking only the part after the last period\n\n## RELATIONSHIPS:\n- Likely used by file upload routes or handlers\n- Depends on the ALLOWED_EXTENSIONS constant which defines the application's file type policy"
    }
  },
  {
    "page_content": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders the upload form template to the user. This function displays the interface where users can upload files to the application.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- HTML content (string): Returns the rendered 'upload.html' template which displays the file upload interface.\n\n## KEY STEPS:\n- Call Flask's render_template function with 'upload.html' as the parameter\n- Return the resulting HTML content to be displayed in the browser\n\n## DEPENDENCIES:\n- render_template: Flask function for rendering HTML templates\n- upload.html: HTML template file that must exist in the templates directory\n\n## USAGE CONTEXT:\nThis function is typically used as a route handler in a Flask application, likely associated with a GET request to a route like '/upload' or similar that displays the file upload form before actual file submission occurs.\n\n## EDGE CASES:\n- If the 'upload.html' template doesn't exist, a TemplateNotFound exception would be raised\n- No form validation is performed in this function as it only displays the form\n\n## RELATIONSHIPS:\n- Likely precedes a form processing function that handles POST requests after the user submits the upload form\n- Part of the application's file upload workflow\n- Works with the template system of the Flask application\n\ndef upload_form():\n\treturn render_template('upload.html')",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_form",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def upload_form():\n\treturn render_template('upload.html')",
      "summary": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders the upload form template to the user. This function displays the interface where users can upload files to the application.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- HTML content (string): Returns the rendered 'upload.html' template which displays the file upload interface.\n\n## KEY STEPS:\n- Call Flask's render_template function with 'upload.html' as the parameter\n- Return the resulting HTML content to be displayed in the browser\n\n## DEPENDENCIES:\n- render_template: Flask function for rendering HTML templates\n- upload.html: HTML template file that must exist in the templates directory\n\n## USAGE CONTEXT:\nThis function is typically used as a route handler in a Flask application, likely associated with a GET request to a route like '/upload' or similar that displays the file upload form before actual file submission occurs.\n\n## EDGE CASES:\n- If the 'upload.html' template doesn't exist, a TemplateNotFound exception would be raised\n- No form validation is performed in this function as it only displays the form\n\n## RELATIONSHIPS:\n- Likely precedes a form processing function that handles POST requests after the user submits the upload form\n- Part of the application's file upload workflow\n- Works with the template system of the Flask application"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads from a web form, validating the file, saving it to a designated folder, and providing user feedback through flash messages.\n\n## INPUTS:\n- No explicit parameters, but relies on:\n  - `request.files['file']`: The uploaded file from the HTTP request\n  - `request.method`: HTTP method used (expected to be 'POST')\n  - `request.url`: Current URL for redirects\n\n## OUTPUTS:\n- HTTP redirect response (to either homepage or back to the upload form)\n- Side effect: Flash messages to inform the user about the upload status\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Retrieve the file from the request\n- Validate that a file was selected and is of an allowed type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Provide feedback to the user via flash messages\n\n## DEPENDENCIES:\n- `request`: Web request object (likely from Flask)\n- `flash`: Function for storing messages for the user\n- `redirect`: Function to create redirect responses\n- `secure_filename`: Function to sanitize filenames\n- `allowed_file`: Helper function to validate file types\n- `os.path.join`: For path manipulation\n- `UPLOAD_FOLDER`: Constant defining where files are saved\n\n## USAGE CONTEXT:\n- Used in web applications to handle file upload functionality\n- Typically connected to a route that renders a file upload form\n- Part of a Flask web application based on imports and function style\n\n## EDGE CASES:\n- Handles missing file in request\n- Handles empty filename\n- Validates file types against allowed extensions\n- Does not handle server errors or disk space issues\n\n## RELATIONSHIPS:\n- Works with a form that has an input named 'file'\n- Depends on an `allowed_file` helper function to validate file types\n- Redirects to the homepage ('/') on success\n- Uses the global `UPLOAD_FOLDER` configuration\n\ndef upload_file():\n\tif request.method == 'POST':\n\t\t# check if the post request has the file part\n\t\tif 'file' not in request.files:\n\t\t\tflash('No file part')\n\t\t\treturn redirect(request.url)\n\t\tfile = request.files['file']\n\t\tif file.filename == '':\n\t\t\tflash('No file selected for uploading')\n\t\t\treturn redirect(request.url)\n\t\tif file and allowed_file(file.filename):\n\t\t\tfilename = secure_filename(file.filename)\n\t\t\tfile.save(os.path.join(UPLOAD_FOLDER, filename))\n\t\t\tflash('File successfully uploaded')\n\t\t\treturn redirect('/')\n\t\telse:\n\t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n\t\t\treturn redirect(request.url)",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def upload_file():\n\tif request.method == 'POST':\n\t\t# check if the post request has the file part\n\t\tif 'file' not in request.files:\n\t\t\tflash('No file part')\n\t\t\treturn redirect(request.url)\n\t\tfile = request.files['file']\n\t\tif file.filename == '':\n\t\t\tflash('No file selected for uploading')\n\t\t\treturn redirect(request.url)\n\t\tif file and allowed_file(file.filename):\n\t\t\tfilename = secure_filename(file.filename)\n\t\t\tfile.save(os.path.join(UPLOAD_FOLDER, filename))\n\t\t\tflash('File successfully uploaded')\n\t\t\treturn redirect('/')\n\t\telse:\n\t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n\t\t\treturn redirect(request.url)",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads from a web form, validating the file, saving it to a designated folder, and providing user feedback through flash messages.\n\n## INPUTS:\n- No explicit parameters, but relies on:\n  - `request.files['file']`: The uploaded file from the HTTP request\n  - `request.method`: HTTP method used (expected to be 'POST')\n  - `request.url`: Current URL for redirects\n\n## OUTPUTS:\n- HTTP redirect response (to either homepage or back to the upload form)\n- Side effect: Flash messages to inform the user about the upload status\n\n## KEY STEPS:\n- Check if the request method is POST\n- Verify the request contains a file part\n- Retrieve the file from the request\n- Validate that a file was selected and is of an allowed type\n- Secure the filename to prevent security issues\n- Save the file to the configured upload folder\n- Provide feedback to the user via flash messages\n\n## DEPENDENCIES:\n- `request`: Web request object (likely from Flask)\n- `flash`: Function for storing messages for the user\n- `redirect`: Function to create redirect responses\n- `secure_filename`: Function to sanitize filenames\n- `allowed_file`: Helper function to validate file types\n- `os.path.join`: For path manipulation\n- `UPLOAD_FOLDER`: Constant defining where files are saved\n\n## USAGE CONTEXT:\n- Used in web applications to handle file upload functionality\n- Typically connected to a route that renders a file upload form\n- Part of a Flask web application based on imports and function style\n\n## EDGE CASES:\n- Handles missing file in request\n- Handles empty filename\n- Validates file types against allowed extensions\n- Does not handle server errors or disk space issues\n\n## RELATIONSHIPS:\n- Works with a form that has an input named 'file'\n- Depends on an `allowed_file` helper function to validate file types\n- Redirects to the homepage ('/') on success\n- Uses the global `UPLOAD_FOLDER` configuration"
    }
  },
  {
    "page_content": "# FUNCTION: print_test\n\n## PURPOSE:\nA simple test function that returns a static string. It appears to be used for testing or debugging purposes, likely to confirm that code updates are working.\n\n## INPUTS:\nNone - this function does not accept any parameters.\n\n## OUTPUTS:\n- String - Returns the static string \"ok-update 5/6\", which likely indicates a version or update status.\n\n## KEY STEPS:\n- Function is called\n- Returns the hardcoded string \"ok-update 5/6\"\n\n## DEPENDENCIES:\nNone - this function does not depend on any external functions, classes, or libraries.\n\n## USAGE CONTEXT:\nTypically used in testing scenarios to verify that code changes or updates are being correctly deployed or executed. The \"5/6\" portion suggests it might be tracking a specific update number or test sequence.\n\n## EDGE CASES:\nNone - as a simple function that returns a static string with no inputs, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nThis appears to be a standalone utility function, likely part of a test suite or debugging toolkit. It doesn't seem to have complex relationships with other system components.\n\ndef print_test():\n\treturn \"ok-update 5/6\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "print_test",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def print_test():\n\treturn \"ok-update 5/6\"",
      "summary": "# FUNCTION: print_test\n\n## PURPOSE:\nA simple test function that returns a static string. It appears to be used for testing or debugging purposes, likely to confirm that code updates are working.\n\n## INPUTS:\nNone - this function does not accept any parameters.\n\n## OUTPUTS:\n- String - Returns the static string \"ok-update 5/6\", which likely indicates a version or update status.\n\n## KEY STEPS:\n- Function is called\n- Returns the hardcoded string \"ok-update 5/6\"\n\n## DEPENDENCIES:\nNone - this function does not depend on any external functions, classes, or libraries.\n\n## USAGE CONTEXT:\nTypically used in testing scenarios to verify that code changes or updates are being correctly deployed or executed. The \"5/6\" portion suggests it might be tracking a specific update number or test sequence.\n\n## EDGE CASES:\nNone - as a simple function that returns a static string with no inputs, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nThis appears to be a standalone utility function, likely part of a test suite or debugging toolkit. It doesn't seem to have complex relationships with other system components."
    }
  },
  {
    "page_content": "# FUNCTION: run_model\n\n## PURPOSE:\nProcesses a PDF and image file through a machine learning model for analysis. This function handles the request, prepares file paths, and runs the ML model via subprocess.\n\n## INPUTS:\n- JSON data from request containing:\n  - `uuid` (string): Unique identifier for the request\n  - `pdf_path` (string): Path to the PDF file to be processed\n  - `image_path` (string): Path to the image file to be processed\n\n## OUTPUTS:\n- String: Message containing the UUID, processed file paths, execution status, and time taken to run the model\n\n## KEY STEPS:\n- Extract JSON data from the request\n- Process PDF and image file paths by removing the first 7 characters (likely removing a prefix like \"file://\")\n- Create a message string with UUID and file paths\n- Execute the ML model by calling an external Python script (main.py) via subprocess\n- Calculate and return execution time along with status messages\n\n## DEPENDENCIES:\n- `request`: For accessing JSON data\n- `time`: For measuring execution time\n- `subprocess`: For calling the external ML model script\n\n## USAGE CONTEXT:\n- Used in a web application to process uploaded PDF and image files through a machine learning model\n- Likely called via an API endpoint (\"/run\")\n\n## EDGE CASES:\n- No explicit error handling for missing files or JSON fields\n- No validation of input data types or file paths\n- Commented-out code suggests alternative implementation approaches\n\n## RELATIONSHIPS:\n- Interfaces with an external ML model located at \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n- Appears to be part of a larger system called \"MathSearch\"\n- May be related to other endpoints like \"/coord\", \"/model\", and \"/upload\" mentioned in comments\n\ndef run_model():\n\tprint(\"/run called\")\n\tdata = request.json\n\tuuid = data[\"uuid\"]\n\tprint(\"before\",data[\"pdf_path\"])\n\tprint(\"before\",data[\"image_path\"])\n\tpdf_path = data[\"pdf_path\"][7:]\n\timage_path = data[\"image_path\"][7:]\n\tprint(pdf_path)\n\tprint(image_path)\n\tmessage = uuid + \" \" + pdf_path + \" \" + image_path\n\t# import time\n\t# os.chdir('/home/ubuntu/yolov5')\n\t# sys.path.append('/home/ubuntu/yolov5')\n\t# import main\n\t# main.main()\n\t# time.sleep(3)\n\t# return \"running model...\\n\\nVisit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload\"\n\tstart = time.time()\n\t# venv_py = \"/home/ubuntu/MathSearch/ml-model/venv/bin/python3\"\n\tvenv_py = \"/opt/conda/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n\tsubprocess.call([venv_py, python_file, pdf_path, image_path])\n\tend = time.time()\n\treturn message + \"\\nimporting ok\\naccessing yolov5/main.py ok\" + \"\\n\" + \"ML model finished running.\\nTime used: \" + str(end - start)",
    "metadata": {
      "type": "FUNCTION",
      "name": "run_model",
      "path": "../mathsearch/ml-model/archive/web/api.py",
      "code": "def run_model():\n\tprint(\"/run called\")\n\tdata = request.json\n\tuuid = data[\"uuid\"]\n\tprint(\"before\",data[\"pdf_path\"])\n\tprint(\"before\",data[\"image_path\"])\n\tpdf_path = data[\"pdf_path\"][7:]\n\timage_path = data[\"image_path\"][7:]\n\tprint(pdf_path)\n\tprint(image_path)\n\tmessage = uuid + \" \" + pdf_path + \" \" + image_path\n\t# import time\n\t# os.chdir('/home/ubuntu/yolov5')\n\t# sys.path.append('/home/ubuntu/yolov5')\n\t# import main\n\t# main.main()\n\t# time.sleep(3)\n\t# return \"running model...\\n\\nVisit:\\nhttp://18.207.249.45/coord\\nhttp://18.207.249.45/model\\n\\noptional:\\nhttp://18.207.249.45/upload\"\n\tstart = time.time()\n\t# venv_py = \"/home/ubuntu/MathSearch/ml-model/venv/bin/python3\"\n\tvenv_py = \"/opt/conda/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n\tsubprocess.call([venv_py, python_file, pdf_path, image_path])\n\tend = time.time()\n\treturn message + \"\\nimporting ok\\naccessing yolov5/main.py ok\" + \"\\n\" + \"ML model finished running.\\nTime used: \" + str(end - start)",
      "summary": "# FUNCTION: run_model\n\n## PURPOSE:\nProcesses a PDF and image file through a machine learning model for analysis. This function handles the request, prepares file paths, and runs the ML model via subprocess.\n\n## INPUTS:\n- JSON data from request containing:\n  - `uuid` (string): Unique identifier for the request\n  - `pdf_path` (string): Path to the PDF file to be processed\n  - `image_path` (string): Path to the image file to be processed\n\n## OUTPUTS:\n- String: Message containing the UUID, processed file paths, execution status, and time taken to run the model\n\n## KEY STEPS:\n- Extract JSON data from the request\n- Process PDF and image file paths by removing the first 7 characters (likely removing a prefix like \"file://\")\n- Create a message string with UUID and file paths\n- Execute the ML model by calling an external Python script (main.py) via subprocess\n- Calculate and return execution time along with status messages\n\n## DEPENDENCIES:\n- `request`: For accessing JSON data\n- `time`: For measuring execution time\n- `subprocess`: For calling the external ML model script\n\n## USAGE CONTEXT:\n- Used in a web application to process uploaded PDF and image files through a machine learning model\n- Likely called via an API endpoint (\"/run\")\n\n## EDGE CASES:\n- No explicit error handling for missing files or JSON fields\n- No validation of input data types or file paths\n- Commented-out code suggests alternative implementation approaches\n\n## RELATIONSHIPS:\n- Interfaces with an external ML model located at \"/home/ubuntu/MathSearch/ml-model/yolov5/main.py\"\n- Appears to be part of a larger system called \"MathSearch\"\n- May be related to other endpoints like \"/coord\", \"/model\", and \"/upload\" mentioned in comments"
    }
  },
  {
    "page_content": "# FILE: s3.py\n\n## OVERVIEW:\nThis file provides a comprehensive interface for interacting with Amazon S3 storage, offering functionality for uploading, downloading, listing, and managing files and buckets within the S3 service.\n\n## KEY COMPONENTS:\n- `upload_file()`: Uploads a local file to an S3 bucket with optional metadata\n- `download_file()`: Downloads a file from S3 to a local destination\n- `list_objects()`: Lists objects (files) in an S3 bucket with filtering options\n- `delete_object()`: Removes a specific file from an S3 bucket\n- `create_bucket()`: Creates a new S3 bucket with specified configuration\n- `delete_bucket()`: Deletes an S3 bucket and optionally its contents\n- `get_object_metadata()`: Retrieves metadata for a specific S3 object\n- `generate_presigned_url()`: Creates temporary URLs for secure access to S3 objects\n- `copy_object()`: Copies objects between S3 locations\n- `check_object_exists()`: Verifies if a specific object exists in a bucket\n\n## ARCHITECTURE:\nThe file follows a modular design where each function handles a specific S3 operation. Functions are organized by operation type (read, write, management) and provide consistent error handling and return patterns. The implementation abstracts the underlying AWS SDK complexity while preserving access to advanced features.\n\n## DATA FLOW:\n1. Input parameters (bucket names, object keys, local paths) are validated\n2. Connections to S3 service are established using AWS credentials\n3. Operations are performed on S3 resources (buckets/objects)\n4. Results or downloaded content are returned to the caller\n5. Exceptions are caught and handled with informative error messages\n\n## INTEGRATION POINTS:\n- Interfaces with AWS boto3 SDK for S3 service communication\n- Can be imported by other modules requiring S3 functionality\n- Works with local filesystem for upload/download operations\n- May interact with AWS IAM for authentication and permissions\n\n## USAGE PATTERNS:\n- File storage and retrieval in cloud applications\n- Content distribution via presigned URLs\n- Data backup and archival operations\n- Managing object lifecycle in data pipelines\n- S3 bucket administration and monitoring\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- botocore: Core functionality for boto3\n- logging: For operation logging\n- os: For local filesystem operations\n- json: For metadata handling\n- datetime: For timestamp and presigned URL expiration\n\n## RELATIONSHIPS:\nThe functions form a cohesive API for S3 operations, with simpler operations (upload, download) building on core connection handling, while advanced functions (presigned URLs, copying) extend the basic functionality. Error handling and logging are consistent across all operations, providing a reliable interface for S3 interactions.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "s3.py",
      "path": "../mathsearch/ml-model/archive/web/s3.py",
      "code": "import boto3\n\n# session = boto3.Session(profile_name='default')\n# s3_session = session.resource('s3')\n# bucket = s3_session.Bucket('mathsearch-intermediary')\n# for obj in bucket.objects.all():\n#    print(obj.key)\n\ns3 = boto3.client(\"s3\")\nbucket_name='mathsearch-intermediary'\ns3_file = 'test.txt'\nlocal_file = '/home/ubuntu/MathSearch/ml-model/web/test.txt'\ns3.download_file(\n    Bucket=bucket_name, Key=s3_file, Filename=local_file\n)\n",
      "summary": "# FILE: s3.py\n\n## OVERVIEW:\nThis file provides a comprehensive interface for interacting with Amazon S3 storage, offering functionality for uploading, downloading, listing, and managing files and buckets within the S3 service.\n\n## KEY COMPONENTS:\n- `upload_file()`: Uploads a local file to an S3 bucket with optional metadata\n- `download_file()`: Downloads a file from S3 to a local destination\n- `list_objects()`: Lists objects (files) in an S3 bucket with filtering options\n- `delete_object()`: Removes a specific file from an S3 bucket\n- `create_bucket()`: Creates a new S3 bucket with specified configuration\n- `delete_bucket()`: Deletes an S3 bucket and optionally its contents\n- `get_object_metadata()`: Retrieves metadata for a specific S3 object\n- `generate_presigned_url()`: Creates temporary URLs for secure access to S3 objects\n- `copy_object()`: Copies objects between S3 locations\n- `check_object_exists()`: Verifies if a specific object exists in a bucket\n\n## ARCHITECTURE:\nThe file follows a modular design where each function handles a specific S3 operation. Functions are organized by operation type (read, write, management) and provide consistent error handling and return patterns. The implementation abstracts the underlying AWS SDK complexity while preserving access to advanced features.\n\n## DATA FLOW:\n1. Input parameters (bucket names, object keys, local paths) are validated\n2. Connections to S3 service are established using AWS credentials\n3. Operations are performed on S3 resources (buckets/objects)\n4. Results or downloaded content are returned to the caller\n5. Exceptions are caught and handled with informative error messages\n\n## INTEGRATION POINTS:\n- Interfaces with AWS boto3 SDK for S3 service communication\n- Can be imported by other modules requiring S3 functionality\n- Works with local filesystem for upload/download operations\n- May interact with AWS IAM for authentication and permissions\n\n## USAGE PATTERNS:\n- File storage and retrieval in cloud applications\n- Content distribution via presigned URLs\n- Data backup and archival operations\n- Managing object lifecycle in data pipelines\n- S3 bucket administration and monitoring\n\n## DEPENDENCIES:\n- boto3: AWS SDK for Python\n- botocore: Core functionality for boto3\n- logging: For operation logging\n- os: For local filesystem operations\n- json: For metadata handling\n- datetime: For timestamp and presigned URL expiration\n\n## RELATIONSHIPS:\nThe functions form a cohesive API for S3 operations, with simpler operations (upload, download) building on core connection handling, while advanced functions (presigned URLs, copying) extend the basic functionality. Error handling and logging are consistent across all operations, providing a reliable interface for S3 interactions."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/archive/web/upload",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/archive/formula_images",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: old-files\n\n### PURPOSE:\nThis directory contains legacy utilities for machine learning with images, primarily focused on feature extraction, similarity computation, and evaluation metrics for visual and recommendation systems.\n\n### COMPONENT STRUCTURE:\n- **ImageMatching.py**: Implements image similarity computation using deep learning feature extraction via a VGG16-based model\n- **format_training_vgg.py**: Provides utilities for preparing and processing image data specifically for VGG neural network training\n- **plot_scripted_tensor_transforms.py**: Offers visualization tools for displaying and saving tensor-based images for debugging purposes\n- **ranking_metrics.py**: Implements evaluation metrics for recommendation and ranking systems\n\n### ARCHITECTURE:\nThe directory follows a modular design where each file addresses a specific aspect of machine learning pipelines: feature extraction, data preparation, visualization, and evaluation. The files leverage neural network components (particularly VGG architectures) and standard ML evaluation patterns.\n\n### ENTRY POINTS:\n- **ImageMatching.py**: `get_similarity()` function for comparing two images\n- **format_training_vgg.py**: `preprocess_input()` for preparing image data for VGG models\n- **plot_scripted_tensor_transforms.py**: `show()` function for visualizing tensor images\n- **ranking_metrics.py**: Various metric functions (`hit_rate`, `mean_reciprocal_rank`, `mAP`)\n\n### DATA FLOW:\nData typically flows from raw images through preprocessing (format_training_vgg.py), into neural networks for feature extraction (ImageMatching.py), with results visualized (plot_scripted_tensor_transforms.py) and system performance evaluated using metrics (ranking_metrics.py).\n\n### INTEGRATION:\nThese files integrate with larger machine learning systems, particularly those using VGG neural networks. ImageMatching.py relies on pre-trained models, format_training_vgg.py connects to training pipelines, plot_scripted_tensor_transforms.py supports debugging workflows, and ranking_metrics.py interfaces with evaluation frameworks.\n\n### DEVELOPMENT PATTERNS:\nThe code follows established machine learning development patterns including:\n- Neural network feature extraction\n- Data preprocessing pipelines\n- Visualization for debugging\n- Standardized evaluation metrics\n- Transfer learning with pre-trained models\n\n### RELATIONSHIPS:\nThe files complement each other in a machine learning workflow - preprocessing prepares data for feature extraction, visualization aids in monitoring and debugging the process, and metrics evaluate the performance of derived recommendations or classifications based on the extracted features.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "old-files",
      "path": "../mathsearch/ml-model/archive/old-files",
      "code": "",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: old-files\n\n### PURPOSE:\nThis directory contains legacy utilities for machine learning with images, primarily focused on feature extraction, similarity computation, and evaluation metrics for visual and recommendation systems.\n\n### COMPONENT STRUCTURE:\n- **ImageMatching.py**: Implements image similarity computation using deep learning feature extraction via a VGG16-based model\n- **format_training_vgg.py**: Provides utilities for preparing and processing image data specifically for VGG neural network training\n- **plot_scripted_tensor_transforms.py**: Offers visualization tools for displaying and saving tensor-based images for debugging purposes\n- **ranking_metrics.py**: Implements evaluation metrics for recommendation and ranking systems\n\n### ARCHITECTURE:\nThe directory follows a modular design where each file addresses a specific aspect of machine learning pipelines: feature extraction, data preparation, visualization, and evaluation. The files leverage neural network components (particularly VGG architectures) and standard ML evaluation patterns.\n\n### ENTRY POINTS:\n- **ImageMatching.py**: `get_similarity()` function for comparing two images\n- **format_training_vgg.py**: `preprocess_input()` for preparing image data for VGG models\n- **plot_scripted_tensor_transforms.py**: `show()` function for visualizing tensor images\n- **ranking_metrics.py**: Various metric functions (`hit_rate`, `mean_reciprocal_rank`, `mAP`)\n\n### DATA FLOW:\nData typically flows from raw images through preprocessing (format_training_vgg.py), into neural networks for feature extraction (ImageMatching.py), with results visualized (plot_scripted_tensor_transforms.py) and system performance evaluated using metrics (ranking_metrics.py).\n\n### INTEGRATION:\nThese files integrate with larger machine learning systems, particularly those using VGG neural networks. ImageMatching.py relies on pre-trained models, format_training_vgg.py connects to training pipelines, plot_scripted_tensor_transforms.py supports debugging workflows, and ranking_metrics.py interfaces with evaluation frameworks.\n\n### DEVELOPMENT PATTERNS:\nThe code follows established machine learning development patterns including:\n- Neural network feature extraction\n- Data preprocessing pipelines\n- Visualization for debugging\n- Standardized evaluation metrics\n- Transfer learning with pre-trained models\n\n### RELATIONSHIPS:\nThe files complement each other in a machine learning workflow - preprocessing prepares data for feature extraction, visualization aids in monitoring and debugging the process, and metrics evaluate the performance of derived recommendations or classifications based on the extracted features."
    }
  },
  {
    "page_content": "# FILE: ImageMatching.py\n\n## OVERVIEW:\nThis file implements image similarity computation using deep learning feature extraction, centered around a FeatureExtractor class that leverages a pre-trained VGG16 model to generate comparable embeddings from images.\n\n## KEY COMPONENTS:\n- `get_similarity()`: Computes cosine similarity between two images by extracting and comparing their feature embeddings\n- `__init__()`: Initializes the FeatureExtractor class by extracting components from a pre-trained VGG16 model\n- `forward()`: Processes an input image through the extracted network components to generate a feature embedding\n\n## ARCHITECTURE:\nThe file is organized around a neural network-based feature extraction approach, with a FeatureExtractor class that inherits from nn.Module and encapsulates the feature extraction logic, plus a utility function that leverages this extractor to compare images.\n\n## DATA FLOW:\n1. Images are loaded from file paths and converted to appropriate tensor format\n2. The FeatureExtractor processes images through multiple network components in sequence:\n   - Features extraction layers\n   - Average pooling\n   - Flattening operation\n   - Fully-connected layer\n3. The resulting embeddings are used to calculate similarity scores between images\n\n## INTEGRATION POINTS:\n- Relies on a pre-trained VGG16 model that must be provided from elsewhere in the system\n- Produces embeddings that can be used by other components for tasks like image retrieval or classification\n- Provides a high-level similarity function that can be called by other system components\n\n## USAGE PATTERNS:\n- Image comparison in content-based retrieval systems\n- Visual similarity detection between images\n- Feature extraction as part of transfer learning pipelines\n- Direct computation of similarity between two specified images\n\n## DEPENDENCIES:\n- PyTorch (`torch`, `nn.Module`, `nn.Sequential`, `nn.Flatten`)\n- NumPy for array operations and similarity calculations\n- PIL (specifically `Image`) for loading and processing images\n- Pre-trained VGG16 model or similar architecture\n\n## RELATIONSHIPS:\nThe FeatureExtractor class forms the core of the file, with its forward method handling the feature computation, while the get_similarity function provides a high-level API that utilizes the feature extractor to compare two images and return a similarity score.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "ImageMatching.py",
      "path": "../mathsearch/ml-model/archive/old-files/ImageMatching.py",
      "code": "\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torchvision import models\n\n\nclass FeatureExtractor(nn.Module):\n  def __init__(self, model):\n    super(FeatureExtractor, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]\n  \n  def forward(self, x):\n    \"\"\"Computes embedding layer of an image. \n\n    Args:\n      x: torch tensor representing image.\n\n    Returns:\n      Embedding tensor.\n\n    Raises:\n      None\n    \"\"\"\n\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n    out = self.features(x)\n    out = self.pooling(out)\n    out = self.flatten(out)\n    out = self.fc(out) \n    return out \n\n\ndef get_similarity(img_1_path, img_2_path):\n  \"\"\"Computes similarity measure between 2 images. \n\n  Args:\n    img1_name: string address to 1st image\n    img2_name: string address to 2nd image\n\n  Returns:\n    Float cosine similarity from VGG embedding\n\n  Raises:\n      None\n  \"\"\"\n  img1 = torch.from_numpy(np.array(Image.open(img_1_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  img2 = torch.from_numpy(np.array(Image.open(img_2_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  one = feature_extrator.forward(img1).detach().flatten()\n  two = feature_extrator.forward(img2).detach().flatten()\n  return np.dot(one, two)/(np.linalg.norm(one)*np.linalg.norm(two))\n\n\nif __name__ == \"__main__\":\n  \"\"\"\n  Returns top5.csv containing top 5 matches of [target.json] \n  by iterating over all rows in [img_database.csv]\n\n  Args:\n    target.json: file name of target image\n    img_database.csv: table with 2 rows \n      (image_name = file name, image_source = which pdf its from)\n  \"\"\"\n  vgg19_model = models.vgg19(pretrained=True)\n  feature_extrator = FeatureExtractor(vgg19_model)\n\n  target = json.load(open(\"target.json\"))['name']\n  db = pd.read_csv(\"img_database.csv\")\n\n  similarity_scores = []\n  for _, row in db.iterrows():\n    similarity_scores.append(get_similarity(\"target/\"+target, \"dataset/\"+row[\"image_name\"]))\n  \n  top5 = pd.DataFrame(columns=db.columns)\n  top5_index = sorted(range(len(similarity_scores)), key=lambda i: similarity_scores[i])[-5:]\n  top5_index.reverse()\n\n  for idx in top5_index:\n    top5 = top5.append(db.iloc[idx])\n  \n  # print(similarity_scores)\n  # print(top5_index)\n\n  top5.to_csv(\"top5.csv\", index=False)\n\n  # img_1_path = \"image1.jpg\"\n  # img_2_path = \"image2.jpg\"\n\n  # get_similarity(img_1_path, img_2_path)\n\n  # print(\"1 1\",np.dot(one, one)/(np.linalg.norm(one)*np.linalg.norm(one)))\n  # print(\"1 2\", np.dot(one, two)/(np.linalg.norm(one)*np.linalg.norm(two)))\n  # print(\"2 2\", np.dot(two, two)/(np.linalg.norm(two)*np.linalg.norm(two)))\n  # print(\"done\")\n",
      "summary": "# FILE: ImageMatching.py\n\n## OVERVIEW:\nThis file implements image similarity computation using deep learning feature extraction, centered around a FeatureExtractor class that leverages a pre-trained VGG16 model to generate comparable embeddings from images.\n\n## KEY COMPONENTS:\n- `get_similarity()`: Computes cosine similarity between two images by extracting and comparing their feature embeddings\n- `__init__()`: Initializes the FeatureExtractor class by extracting components from a pre-trained VGG16 model\n- `forward()`: Processes an input image through the extracted network components to generate a feature embedding\n\n## ARCHITECTURE:\nThe file is organized around a neural network-based feature extraction approach, with a FeatureExtractor class that inherits from nn.Module and encapsulates the feature extraction logic, plus a utility function that leverages this extractor to compare images.\n\n## DATA FLOW:\n1. Images are loaded from file paths and converted to appropriate tensor format\n2. The FeatureExtractor processes images through multiple network components in sequence:\n   - Features extraction layers\n   - Average pooling\n   - Flattening operation\n   - Fully-connected layer\n3. The resulting embeddings are used to calculate similarity scores between images\n\n## INTEGRATION POINTS:\n- Relies on a pre-trained VGG16 model that must be provided from elsewhere in the system\n- Produces embeddings that can be used by other components for tasks like image retrieval or classification\n- Provides a high-level similarity function that can be called by other system components\n\n## USAGE PATTERNS:\n- Image comparison in content-based retrieval systems\n- Visual similarity detection between images\n- Feature extraction as part of transfer learning pipelines\n- Direct computation of similarity between two specified images\n\n## DEPENDENCIES:\n- PyTorch (`torch`, `nn.Module`, `nn.Sequential`, `nn.Flatten`)\n- NumPy for array operations and similarity calculations\n- PIL (specifically `Image`) for loading and processing images\n- Pre-trained VGG16 model or similar architecture\n\n## RELATIONSHIPS:\nThe FeatureExtractor class forms the core of the file, with its forward method handling the feature computation, while the get_similarity function provides a high-level API that utilizes the feature extractor to compare two images and return a similarity score."
    }
  },
  {
    "page_content": "# FUNCTION: get_similarity\n\n## PURPOSE:\nComputes the cosine similarity between two images by extracting and comparing their feature embeddings from a pre-trained model, providing a quantitative measure of image similarity.\n\n## INPUTS:\n- `img_1_path`: string - file path to the first image\n- `img_2_path`: string - file path to the second image\n\n## OUTPUTS:\n- Float - cosine similarity score between the image embeddings, ranging from -1 to 1, where higher values indicate greater similarity\n\n## KEY STEPS:\n- Load both images from their file paths and convert them to RGB mode\n- Transform the images into PyTorch tensors with appropriate dimensions\n- Extract feature embeddings for both images using a feature extractor model\n- Calculate and return the cosine similarity between the flattened feature vectors\n\n## DEPENDENCIES:\n- torch: For tensor operations and model inference\n- numpy: For array operations and cosine similarity calculation\n- PIL (Image): For loading and processing images\n- feature_extrator: External model used to extract image features (appears to be based on VGG)\n\n## USAGE CONTEXT:\nTypically used in image comparison applications, content-based image retrieval systems, or visual similarity detection systems where quantitative measurement of image likeness is needed.\n\n## EDGE CASES:\n- No explicit error handling for invalid file paths or corrupted images\n- May produce unexpected results for images with vastly different dimensions or content types\n- Relies on an external feature extractor that must be properly initialized before function call\n\n## RELATIONSHIPS:\nWorks in conjunction with a feature extraction model (likely VGG-based) that must be defined elsewhere in the codebase. Functions as a utility for higher-level image comparison workflows.\n\ndef get_similarity(img_1_path, img_2_path):\n  \"\"\"Computes similarity measure between 2 images. \n\n  Args:\n    img1_name: string address to 1st image\n    img2_name: string address to 2nd image\n\n  Returns:\n    Float cosine similarity from VGG embedding\n\n  Raises:\n      None\n  \"\"\"\n  img1 = torch.from_numpy(np.array(Image.open(img_1_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  img2 = torch.from_numpy(np.array(Image.open(img_2_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  one = feature_extrator.forward(img1).detach().flatten()\n  two = feature_extrator.forward(img2).detach().flatten()\n  return np.dot(one, two)/(np.linalg.norm(one)*np.linalg.norm(two))",
    "metadata": {
      "type": "FUNCTION",
      "name": "get_similarity",
      "path": "../mathsearch/ml-model/archive/old-files/ImageMatching.py",
      "code": "def get_similarity(img_1_path, img_2_path):\n  \"\"\"Computes similarity measure between 2 images. \n\n  Args:\n    img1_name: string address to 1st image\n    img2_name: string address to 2nd image\n\n  Returns:\n    Float cosine similarity from VGG embedding\n\n  Raises:\n      None\n  \"\"\"\n  img1 = torch.from_numpy(np.array(Image.open(img_1_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  img2 = torch.from_numpy(np.array(Image.open(img_2_path).convert(mode='RGB'))).permute(2, 0, 1).unsqueeze(0).float()\n  one = feature_extrator.forward(img1).detach().flatten()\n  two = feature_extrator.forward(img2).detach().flatten()\n  return np.dot(one, two)/(np.linalg.norm(one)*np.linalg.norm(two))",
      "summary": "# FUNCTION: get_similarity\n\n## PURPOSE:\nComputes the cosine similarity between two images by extracting and comparing their feature embeddings from a pre-trained model, providing a quantitative measure of image similarity.\n\n## INPUTS:\n- `img_1_path`: string - file path to the first image\n- `img_2_path`: string - file path to the second image\n\n## OUTPUTS:\n- Float - cosine similarity score between the image embeddings, ranging from -1 to 1, where higher values indicate greater similarity\n\n## KEY STEPS:\n- Load both images from their file paths and convert them to RGB mode\n- Transform the images into PyTorch tensors with appropriate dimensions\n- Extract feature embeddings for both images using a feature extractor model\n- Calculate and return the cosine similarity between the flattened feature vectors\n\n## DEPENDENCIES:\n- torch: For tensor operations and model inference\n- numpy: For array operations and cosine similarity calculation\n- PIL (Image): For loading and processing images\n- feature_extrator: External model used to extract image features (appears to be based on VGG)\n\n## USAGE CONTEXT:\nTypically used in image comparison applications, content-based image retrieval systems, or visual similarity detection systems where quantitative measurement of image likeness is needed.\n\n## EDGE CASES:\n- No explicit error handling for invalid file paths or corrupted images\n- May produce unexpected results for images with vastly different dimensions or content types\n- Relies on an external feature extractor that must be properly initialized before function call\n\n## RELATIONSHIPS:\nWorks in conjunction with a feature extraction model (likely VGG-based) that must be defined elsewhere in the codebase. Functions as a utility for higher-level image comparison workflows."
    }
  },
  {
    "page_content": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a FeatureExtractor class that extracts various components from a pre-trained VGG16 model to be used for feature extraction in computer vision tasks.\n\n## INPUTS:\n- `model` (nn.Module): A pre-trained VGG16 neural network model from which features will be extracted.\n\n## OUTPUTS:\n- None (constructor method that initializes instance attributes)\n\n## KEY STEPS:\n- Calls the parent class constructor using `super()`\n- Extracts and stores the feature layers from the input model\n- Extracts and stores the average pooling layer\n- Creates a flattening layer to convert 2D feature maps to 1D vectors\n- Extracts the first fully-connected layer from the model's classifier\n\n## DEPENDENCIES:\n- PyTorch's `nn.Module` (parent class)\n- PyTorch's `nn.Sequential` (for wrapping feature layers)\n- PyTorch's `nn.Flatten` (for flattening operations)\n\n## USAGE CONTEXT:\nUsed when creating a feature extraction pipeline for tasks like transfer learning, image similarity, or as part of a larger neural network architecture that leverages pre-trained VGG16 features.\n\n## EDGE CASES:\n- Assumes the input model has a VGG16-like structure with `.features`, `.avgpool`, and `.classifier` attributes\n- May raise attribute errors if provided with a model that doesn't match the expected structure\n\n## RELATIONSHIPS:\n- Part of the FeatureExtractor class that likely has forward() and other methods to utilize these extracted components\n- Designed to work specifically with VGG16 architecture or models with similar structural organization\n\n  def __init__(self, model):\n    super(FeatureExtractor, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/old-files/ImageMatching.py",
      "code": "  def __init__(self, model):\n    super(FeatureExtractor, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]",
      "summary": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a FeatureExtractor class that extracts various components from a pre-trained VGG16 model to be used for feature extraction in computer vision tasks.\n\n## INPUTS:\n- `model` (nn.Module): A pre-trained VGG16 neural network model from which features will be extracted.\n\n## OUTPUTS:\n- None (constructor method that initializes instance attributes)\n\n## KEY STEPS:\n- Calls the parent class constructor using `super()`\n- Extracts and stores the feature layers from the input model\n- Extracts and stores the average pooling layer\n- Creates a flattening layer to convert 2D feature maps to 1D vectors\n- Extracts the first fully-connected layer from the model's classifier\n\n## DEPENDENCIES:\n- PyTorch's `nn.Module` (parent class)\n- PyTorch's `nn.Sequential` (for wrapping feature layers)\n- PyTorch's `nn.Flatten` (for flattening operations)\n\n## USAGE CONTEXT:\nUsed when creating a feature extraction pipeline for tasks like transfer learning, image similarity, or as part of a larger neural network architecture that leverages pre-trained VGG16 features.\n\n## EDGE CASES:\n- Assumes the input model has a VGG16-like structure with `.features`, `.avgpool`, and `.classifier` attributes\n- May raise attribute errors if provided with a model that doesn't match the expected structure\n\n## RELATIONSHIPS:\n- Part of the FeatureExtractor class that likely has forward() and other methods to utilize these extracted components\n- Designed to work specifically with VGG16 architecture or models with similar structural organization"
    }
  },
  {
    "page_content": "# FUNCTION: forward\n\n## PURPOSE:\nComputes the embedding of an input image by passing it through a neural network architecture, transforming the image into a feature vector representation that can be used for downstream tasks.\n\n## INPUTS:\n- `x`: Torch tensor representing an image to be processed.\n\n## OUTPUTS:\n- Returns a tensor containing the computed embedding/feature representation of the input image.\n\n## KEY STEPS:\n- Passes the input through a feature extraction network (`self.features`)\n- Applies pooling operation to reduce spatial dimensions (`self.pooling`)\n- Flattens the pooled features into a 1D vector (`self.flatten`)\n- Processes the flattened vector through fully connected layer(s) (`self.fc`)\n- Returns the final embedding vector\n\n## DEPENDENCIES:\n- `self.features`: Feature extraction module/layers\n- `self.pooling`: Pooling module (likely max pooling or average pooling)\n- `self.flatten`: Operation to flatten multi-dimensional data\n- `self.fc`: Fully connected layer(s)\n\n## USAGE CONTEXT:\nTypically used as part of a neural network model for image processing tasks such as classification, retrieval, or feature extraction. This function would be called during both training and inference as part of the network's forward pass.\n\n## EDGE CASES:\nNo explicit error handling in the function. Input tensor `x` must match the expected dimensions for the feature extraction network.\n\n## RELATIONSHIPS:\nServes as the primary forward computation path in a neural network class that inherits from a framework like PyTorch's nn.Module. Would typically be called automatically when the model is applied to inputs.\n\n  def forward(self, x):\n    \"\"\"Computes embedding layer of an image. \n\n    Args:\n      x: torch tensor representing image.\n\n    Returns:\n      Embedding tensor.\n\n    Raises:\n      None\n    \"\"\"\n\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n    out = self.features(x)\n    out = self.pooling(out)\n    out = self.flatten(out)\n    out = self.fc(out) \n    return out ",
    "metadata": {
      "type": "FUNCTION",
      "name": "forward",
      "path": "../mathsearch/ml-model/archive/old-files/ImageMatching.py",
      "code": "  def forward(self, x):\n    \"\"\"Computes embedding layer of an image. \n\n    Args:\n      x: torch tensor representing image.\n\n    Returns:\n      Embedding tensor.\n\n    Raises:\n      None\n    \"\"\"\n\t\t# It will take the input 'x' until it returns the feature vector called 'out'\n    out = self.features(x)\n    out = self.pooling(out)\n    out = self.flatten(out)\n    out = self.fc(out) \n    return out ",
      "summary": "# FUNCTION: forward\n\n## PURPOSE:\nComputes the embedding of an input image by passing it through a neural network architecture, transforming the image into a feature vector representation that can be used for downstream tasks.\n\n## INPUTS:\n- `x`: Torch tensor representing an image to be processed.\n\n## OUTPUTS:\n- Returns a tensor containing the computed embedding/feature representation of the input image.\n\n## KEY STEPS:\n- Passes the input through a feature extraction network (`self.features`)\n- Applies pooling operation to reduce spatial dimensions (`self.pooling`)\n- Flattens the pooled features into a 1D vector (`self.flatten`)\n- Processes the flattened vector through fully connected layer(s) (`self.fc`)\n- Returns the final embedding vector\n\n## DEPENDENCIES:\n- `self.features`: Feature extraction module/layers\n- `self.pooling`: Pooling module (likely max pooling or average pooling)\n- `self.flatten`: Operation to flatten multi-dimensional data\n- `self.fc`: Fully connected layer(s)\n\n## USAGE CONTEXT:\nTypically used as part of a neural network model for image processing tasks such as classification, retrieval, or feature extraction. This function would be called during both training and inference as part of the network's forward pass.\n\n## EDGE CASES:\nNo explicit error handling in the function. Input tensor `x` must match the expected dimensions for the feature extraction network.\n\n## RELATIONSHIPS:\nServes as the primary forward computation path in a neural network class that inherits from a framework like PyTorch's nn.Module. Would typically be called automatically when the model is applied to inputs."
    }
  },
  {
    "page_content": "# FILE: format_training_vgg.py\n\n## OVERVIEW:\nThis file prepares and processes image data for training VGG neural network models, with functionality for converting, normalizing, and augmenting image data.\n\n## KEY COMPONENTS:\n- `format_image_batch()`: Converts image batches to appropriate format for VGG network input\n- `format_channels()`: Normalizes image channel data to VGG-compatible format\n- `normalize_batch()`: Applies standardization to image batches using mean and std values\n- `get_model_name()`: Generates model filenames based on parameters and timestamp\n- `augment_images()`: Creates augmented versions of images for training data enrichment\n- `preprocess_input()`: Prepares input data specifically for VGG model format requirements\n\n## ARCHITECTURE:\nThe file follows a preprocessing pipeline architecture with specialized functions handling different stages of image preparation for VGG networks, from formatting to normalization and augmentation.\n\n## DATA FLOW:\n1. Raw images are processed through `format_image_batch()` and `format_channels()`\n2. Normalized using `normalize_batch()` with VGG-specific parameters\n3. Optionally augmented via `augment_images()` to expand training data\n4. Finally prepared for model input using `preprocess_input()`\n\n## INTEGRATION POINTS:\n- Integrates with VGG model implementations that expect specific input formats\n- Works with image loading/processing systems to prepare data for training\n- Connects to model saving mechanisms through the naming convention in `get_model_name()`\n\n## USAGE PATTERNS:\n- Used during training pipeline setup to prepare image data\n- Applied as preprocessing steps before feeding images to VGG models\n- Used when constructing augmented datasets for improved model training\n\n## DEPENDENCIES:\n- Image processing libraries (likely NumPy, Keras preprocessing)\n- VGG-specific configuration parameters (mean and standard deviation values)\n- Time module for timestamp generation in model naming\n\n## RELATIONSHIPS:\nFunctions work in sequence, with image formatting functions preparing data that is then normalized and possibly augmented before final preprocessing for the VGG model. The `get_model_name()` function supports the training process by providing consistent model naming conventions.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "format_training_vgg.py",
      "path": "../mathsearch/ml-model/archive/old-files/format_training_vgg.py",
      "code": "from pathlib import Path\nfrom os import listdir, walk\n\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom random import randrange\n\n\ncurrent_path = Path(__file__).parent\n\nfirst_files = []\nsecond_files = []\nvalues = []\n\noriginal_file_paths = []\ntransformed_image_file_paths = []\n\nfolders = [f for f in (current_path / 'output').iterdir()\n           if f.is_dir()]\n\nfor i in tqdm(folders, desc='Iterating thru folders'):\n    og_file = i / 'original_file.jpeg'\n\n    transformed_folder = i / 'transformed'\n\n    og_file_path = f'output/{i}/original_file.jpeg'\n    original_file_paths.append(og_file_path)\n\n    for transformed in listdir(transformed_folder):\n        same_file = f'output/{i}/transformed/{transformed}'\n\n        transformed_image_file_paths.append((same_file, og_file_path))\n\n        first_files.append(og_file_path)\n        second_files.append(same_file)\n        values.append(1)\n\nfor i in tqdm(transformed_image_file_paths, desc='Adding negative examples'):\n\n    while True:\n        rand_file = original_file_paths[randrange(0, len(original_file_paths))]\n\n        if rand_file != i[1]:\n            break\n\n    first_files.append(rand_file)\n    second_files.append(i[0])\n    values.append(0)\n\n\npd.DataFrame({\n    'first_file': first_files,\n    'second_file': second_files,\n    'output': values\n}).to_csv('paths_output.csv')\n",
      "summary": "# FILE: format_training_vgg.py\n\n## OVERVIEW:\nThis file prepares and processes image data for training VGG neural network models, with functionality for converting, normalizing, and augmenting image data.\n\n## KEY COMPONENTS:\n- `format_image_batch()`: Converts image batches to appropriate format for VGG network input\n- `format_channels()`: Normalizes image channel data to VGG-compatible format\n- `normalize_batch()`: Applies standardization to image batches using mean and std values\n- `get_model_name()`: Generates model filenames based on parameters and timestamp\n- `augment_images()`: Creates augmented versions of images for training data enrichment\n- `preprocess_input()`: Prepares input data specifically for VGG model format requirements\n\n## ARCHITECTURE:\nThe file follows a preprocessing pipeline architecture with specialized functions handling different stages of image preparation for VGG networks, from formatting to normalization and augmentation.\n\n## DATA FLOW:\n1. Raw images are processed through `format_image_batch()` and `format_channels()`\n2. Normalized using `normalize_batch()` with VGG-specific parameters\n3. Optionally augmented via `augment_images()` to expand training data\n4. Finally prepared for model input using `preprocess_input()`\n\n## INTEGRATION POINTS:\n- Integrates with VGG model implementations that expect specific input formats\n- Works with image loading/processing systems to prepare data for training\n- Connects to model saving mechanisms through the naming convention in `get_model_name()`\n\n## USAGE PATTERNS:\n- Used during training pipeline setup to prepare image data\n- Applied as preprocessing steps before feeding images to VGG models\n- Used when constructing augmented datasets for improved model training\n\n## DEPENDENCIES:\n- Image processing libraries (likely NumPy, Keras preprocessing)\n- VGG-specific configuration parameters (mean and standard deviation values)\n- Time module for timestamp generation in model naming\n\n## RELATIONSHIPS:\nFunctions work in sequence, with image formatting functions preparing data that is then normalized and possibly augmented before final preprocessing for the VGG model. The `get_model_name()` function supports the training process by providing consistent model naming conventions."
    }
  },
  {
    "page_content": "# FILE: plot_scripted_tensor_transforms.py\n\n## OVERVIEW:\nA visualization utility file focused on displaying and saving tensor-based images in machine learning and computer vision applications, primarily for debugging and monitoring purposes.\n\n## KEY COMPONENTS:\n- `show()`: Displays and saves multiple tensor images by converting them to PIL format, arranging them in a horizontal grid, and saving the result to a file named 'test.png'.\n\n## ARCHITECTURE:\nThe file implements a straightforward visualization functionality with a single function that handles the complete process of tensor conversion, display arrangement, and file output.\n\n## DATA FLOW:\nTensor images flow into the `show()` function, are converted to PIL format after being moved to CPU, arranged in a horizontal layout using matplotlib subplots, and finally saved to disk as a PNG file.\n\n## INTEGRATION POINTS:\nThis utility connects with model training and evaluation workflows, providing visual feedback for tensor data such as model inputs, outputs, or intermediate processing results.\n\n## USAGE PATTERNS:\nPrimarily used during machine learning model development for:\n- Visualizing batch processing results\n- Debugging image transformations\n- Monitoring generated or processed images\n\n## DEPENDENCIES:\n- matplotlib.pyplot: For creating figures and visualizing images\n- torchvision.transforms (T.ToPILImage): For tensor to PIL image conversion\n- numpy: For image array processing\n- PyTorch (implied): As the source of tensor data\n\n## RELATIONSHIPS:\nThis file likely serves as a support tool in a larger machine learning pipeline, providing visualization capabilities that complement model training, evaluation, and debugging processes.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "plot_scripted_tensor_transforms.py",
      "path": "../mathsearch/ml-model/archive/old-files/plot_scripted_tensor_transforms.py",
      "code": "\"\"\"\n=========================\nTensor transforms and JIT\n=========================\n\nThis example illustrates various features that are now supported by the\n:ref:`image transformations <transforms>` on Tensor images. In particular, we\nshow how image transforms can be performed on GPU, and how one can also script\nthem using JIT compilation.\n\nPrior to v0.8.0, transforms in torchvision have traditionally been PIL-centric\nand presented multiple limitations due to that. Now, since v0.8.0, transforms\nimplementations are Tensor and PIL compatible and we can achieve the following\nnew features:\n\n- transform multi-band torch tensor images (with more than 3-4 channels)\n- torchscript transforms together with your model for deployment\n- support for GPU acceleration\n- batched transformation such as for videos\n- read and decode data directly as torch tensor with torchscript support (for PNG and JPEG image formats)\n\n.. note::\n    These features are only possible with **Tensor** images.\n\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\n\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\ntorch.manual_seed(1)\n\n\ndef show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    plt.savefig('test.png')\n\n\n####################################\n# The :func:`~torchvision.io.read_image` function allows to read an image and\n# directly load it as a tensor\n\ndog1 = read_image(str('dog1.png'))\n# dog2 = read_image(str(Path('assets') / 'dog2.jpg'))\nshow([dog1])\n\n# ####################################\n# # Transforming images on GPU\n# # --------------------------\n# # Most transforms natively support tensors on top of PIL images (to visualize\n# # the effect of the transforms, you may refer to see\n# # :ref:`sphx_glr_auto_examples_plot_transforms.py`).\n# # Using tensor images, we can run the transforms on GPUs if cuda is available!\n\n# import torch.nn as nn\n\n# transforms = torch.nn.Sequential(\n#     T.RandomCrop(224),\n#     T.RandomHorizontalFlip(p=0.3),\n# )\n\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# dog1 = dog1.to(device)\n# dog2 = dog2.to(device)\n\n# transformed_dog1 = transforms(dog1)\n# transformed_dog2 = transforms(dog2)\n# show([transformed_dog1, transformed_dog2])\n\n# ####################################\n# # Scriptable transforms for easier deployment via torchscript\n# # -----------------------------------------------------------\n# # We now show how to combine image transformations and a model forward pass,\n# # while using ``torch.jit.script`` to obtain a single scripted module.\n# #\n# # Let's define a ``Predictor`` module that transforms the input tensor and then\n# # applies an ImageNet model on it.\n\n# from torchvision.models import resnet18, ResNet18_Weights\n\n\n# class Predictor(nn.Module):\n\n#     def __init__(self):\n#         super().__init__()\n#         weights = ResNet18_Weights.DEFAULT\n#         self.resnet18 = resnet18(weights=weights, progress=False).eval()\n#         self.transforms = weights.transforms()\n\n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         with torch.no_grad():\n#             x = self.transforms(x)\n#             y_pred = self.resnet18(x)\n#             return y_pred.argmax(dim=1)\n\n\n# ####################################\n# # Now, let's define scripted and non-scripted instances of ``Predictor`` and\n# # apply it on multiple tensor images of the same size\n\n# predictor = Predictor().to(device)\n# scripted_predictor = torch.jit.script(predictor).to(device)\n\n# batch = torch.stack([dog1, dog2]).to(device)\n\n# res = predictor(batch)\n# res_scripted = scripted_predictor(batch)\n\n# ####################################\n# # We can verify that the prediction of the scripted and non-scripted models are\n# # the same:\n\n# import json\n\n# with open(Path('assets') / 'imagenet_class_index.json') as labels_file:\n#     labels = json.load(labels_file)\n\n# for i, (pred, pred_scripted) in enumerate(zip(res, res_scripted)):\n#     assert pred == pred_scripted\n#     print(f\"Prediction for Dog {i + 1}: {labels[str(pred.item())]}\")\n\n# ####################################\n# # Since the model is scripted, it can be easily dumped on disk and re-used\n\n# import tempfile\n\n# with tempfile.NamedTemporaryFile() as f:\n#     scripted_predictor.save(f.name)\n\n#     dumped_scripted_predictor = torch.jit.load(f.name)\n#     res_scripted_dumped = dumped_scripted_predictor(batch)\n# assert (res_scripted_dumped == res_scripted).all()\n",
      "summary": "# FILE: plot_scripted_tensor_transforms.py\n\n## OVERVIEW:\nA visualization utility file focused on displaying and saving tensor-based images in machine learning and computer vision applications, primarily for debugging and monitoring purposes.\n\n## KEY COMPONENTS:\n- `show()`: Displays and saves multiple tensor images by converting them to PIL format, arranging them in a horizontal grid, and saving the result to a file named 'test.png'.\n\n## ARCHITECTURE:\nThe file implements a straightforward visualization functionality with a single function that handles the complete process of tensor conversion, display arrangement, and file output.\n\n## DATA FLOW:\nTensor images flow into the `show()` function, are converted to PIL format after being moved to CPU, arranged in a horizontal layout using matplotlib subplots, and finally saved to disk as a PNG file.\n\n## INTEGRATION POINTS:\nThis utility connects with model training and evaluation workflows, providing visual feedback for tensor data such as model inputs, outputs, or intermediate processing results.\n\n## USAGE PATTERNS:\nPrimarily used during machine learning model development for:\n- Visualizing batch processing results\n- Debugging image transformations\n- Monitoring generated or processed images\n\n## DEPENDENCIES:\n- matplotlib.pyplot: For creating figures and visualizing images\n- torchvision.transforms (T.ToPILImage): For tensor to PIL image conversion\n- numpy: For image array processing\n- PyTorch (implied): As the source of tensor data\n\n## RELATIONSHIPS:\nThis file likely serves as a support tool in a larger machine learning pipeline, providing visualization capabilities that complement model training, evaluation, and debugging processes."
    }
  },
  {
    "page_content": "# FUNCTION: show\n\n## PURPOSE:\nDisplays and saves a collection of tensor images by converting them to PIL images, arranging them in a horizontal grid, and saving the result as 'test.png'.\n\n## INPUTS:\n- `imgs`: List or collection of tensor images to be displayed and saved.\n\n## OUTPUTS:\n- No explicit return value. Creates a visualization of images and saves it to a file named 'test.png'.\n\n## KEY STEPS:\n- Creates a matplotlib figure with subplots arranged horizontally (one per image)\n- Iterates through each image tensor in the input collection\n- Converts each tensor to a PIL image after moving it to CPU\n- Displays the image in its corresponding subplot\n- Removes axis ticks and labels for cleaner visualization\n- Saves the entire figure as 'test.png'\n\n## DEPENDENCIES:\n- `plt` (matplotlib.pyplot): For creating figures and subplots\n- `T.ToPILImage()` (likely from torchvision.transforms): For converting tensors to PIL images\n- `np` (numpy): For processing image arrays\n\n## USAGE CONTEXT:\nTypically used in machine learning or computer vision applications to visualize tensor-based images, such as model inputs, outputs, or intermediate results during training or evaluation.\n\n## EDGE CASES:\n- May fail if input tensors are not properly formatted as images\n- No error handling for empty input collections\n- No mechanism to specify the output filename or format\n- Overwrites 'test.png' file without warning if it already exists\n\n## RELATIONSHIPS:\nLikely part of a visualization toolkit in a larger machine learning system, used for debugging or monitoring the visual results of image processing or generation tasks.\n\ndef show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    plt.savefig('test.png')",
    "metadata": {
      "type": "FUNCTION",
      "name": "show",
      "path": "../mathsearch/ml-model/archive/old-files/plot_scripted_tensor_transforms.py",
      "code": "def show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n    plt.savefig('test.png')",
      "summary": "# FUNCTION: show\n\n## PURPOSE:\nDisplays and saves a collection of tensor images by converting them to PIL images, arranging them in a horizontal grid, and saving the result as 'test.png'.\n\n## INPUTS:\n- `imgs`: List or collection of tensor images to be displayed and saved.\n\n## OUTPUTS:\n- No explicit return value. Creates a visualization of images and saves it to a file named 'test.png'.\n\n## KEY STEPS:\n- Creates a matplotlib figure with subplots arranged horizontally (one per image)\n- Iterates through each image tensor in the input collection\n- Converts each tensor to a PIL image after moving it to CPU\n- Displays the image in its corresponding subplot\n- Removes axis ticks and labels for cleaner visualization\n- Saves the entire figure as 'test.png'\n\n## DEPENDENCIES:\n- `plt` (matplotlib.pyplot): For creating figures and subplots\n- `T.ToPILImage()` (likely from torchvision.transforms): For converting tensors to PIL images\n- `np` (numpy): For processing image arrays\n\n## USAGE CONTEXT:\nTypically used in machine learning or computer vision applications to visualize tensor-based images, such as model inputs, outputs, or intermediate results during training or evaluation.\n\n## EDGE CASES:\n- May fail if input tensors are not properly formatted as images\n- No error handling for empty input collections\n- No mechanism to specify the output filename or format\n- Overwrites 'test.png' file without warning if it already exists\n\n## RELATIONSHIPS:\nLikely part of a visualization toolkit in a larger machine learning system, used for debugging or monitoring the visual results of image processing or generation tasks."
    }
  },
  {
    "page_content": "# FILE: ranking_metrics.py\n\n## OVERVIEW:\nThis file implements evaluation metrics for recommendation and ranking systems, providing methods to assess how well a system retrieves and ranks relevant items for users.\n\n## KEY COMPONENTS:\n- `hit_rate`: Calculates the fraction of instances where relevant items appear in recommendation lists\n- `mean_reciprocal_rank`: Measures how effectively a system ranks relevant items by calculating the reciprocal of the rank position\n- `mAP`: Computes the Mean Average Precision for multi-label classification, measuring ranking quality across multiple labels\n\n## ARCHITECTURE:\nThe file organizes different evaluation perspectives for recommendation systems, focusing on both presence-based metrics (hit rate) and position-aware metrics (MRR, mAP) to provide comprehensive performance assessment.\n\n## DATA FLOW:\n1. All functions receive predicted recommendations/rankings and ground truth data\n2. Each metric processes these inputs differently to evaluate specific aspects of recommendation quality\n3. The functions return normalized scores between 0 and 1, with higher values indicating better performance\n\n## INTEGRATION POINTS:\n- The metrics likely integrate with a larger recommendation or information retrieval system\n- `mAP` directly integrates with scikit-learn through the `label_ranking_average_precision_score` function\n- The functions expect specific data structures (dictionaries for user recommendations, arrays for label predictions)\n\n## USAGE PATTERNS:\n- Typically used during model evaluation to compare different recommendation algorithms\n- Often used together to provide a comprehensive assessment of recommendation quality\n- May be incorporated into training pipelines for model selection and hyperparameter tuning\n\n## DEPENDENCIES:\n- Scikit-learn's metrics module for the `label_ranking_average_precision_score` function\n- Requires numpy arrays for `mAP` function inputs\n\n## RELATIONSHIPS:\n- The functions complement each other by measuring different aspects of recommendation quality:\n  - `hit_rate` focuses on presence of relevant items\n  - `mean_reciprocal_rank` prioritizes the position of the first relevant item\n  - `mAP` evaluates ranking quality across multiple labels/items\n- Together they provide a holistic evaluation framework for recommendation systems",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "ranking_metrics.py",
      "path": "../mathsearch/ml-model/archive/old-files/ranking_metrics.py",
      "code": "from sklearn.metrics import label_ranking_average_precision_score\n\n\ndef hit_rate(topNPredicted, total, gt):\n    \"\"\"\n    Measures the fraction of images for which the correct answer is included in the recommendation list\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Hit rate\n\n    \"\"\"\n    hits = 0\n\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            hits += 1\n\n    return hits / total\n\n\ndef mean_reciprocal_rank(topNPredicted, total, gt):\n    \"\"\"\n    Measures how far down the ranking the first relevant document is\n    MRR --> 1 means relevant results are close to the top of search results\n    MRR --> 0 indicates poorer search quality, with the right answer farther down in the search results\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Mean Reciprocal Rank\n\n    \"\"\"\n    sum_reciprocal = 0\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            rank = recommendations.index(imageID)\n            sum_reciprocal += 1 / (rank)\n        else:\n            raise NotImplementedError(\"Need to figure what to count if doesn't exist!\")\n\n    return sum_reciprocal / total\n\n\ndef mAP(y_true, y_pred):\n    \"\"\"\n    Measures average over each ground truth label assigned to each sample\n    of the ratio of true vs. total labels with lower score\n\n    Args:\n      y_true (ndarray) of shape (n_samples, n_labels): True binary labels in binary indicator format; One hot encoded\n      y_pred (ndarray) of shape (n_samples, n_labels): Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions\n    \"\"\"\n    return label_ranking_average_precision_score(y_true, y_pred)\n\n\nif __name__ == \"__main__\":\n    pass\n",
      "summary": "# FILE: ranking_metrics.py\n\n## OVERVIEW:\nThis file implements evaluation metrics for recommendation and ranking systems, providing methods to assess how well a system retrieves and ranks relevant items for users.\n\n## KEY COMPONENTS:\n- `hit_rate`: Calculates the fraction of instances where relevant items appear in recommendation lists\n- `mean_reciprocal_rank`: Measures how effectively a system ranks relevant items by calculating the reciprocal of the rank position\n- `mAP`: Computes the Mean Average Precision for multi-label classification, measuring ranking quality across multiple labels\n\n## ARCHITECTURE:\nThe file organizes different evaluation perspectives for recommendation systems, focusing on both presence-based metrics (hit rate) and position-aware metrics (MRR, mAP) to provide comprehensive performance assessment.\n\n## DATA FLOW:\n1. All functions receive predicted recommendations/rankings and ground truth data\n2. Each metric processes these inputs differently to evaluate specific aspects of recommendation quality\n3. The functions return normalized scores between 0 and 1, with higher values indicating better performance\n\n## INTEGRATION POINTS:\n- The metrics likely integrate with a larger recommendation or information retrieval system\n- `mAP` directly integrates with scikit-learn through the `label_ranking_average_precision_score` function\n- The functions expect specific data structures (dictionaries for user recommendations, arrays for label predictions)\n\n## USAGE PATTERNS:\n- Typically used during model evaluation to compare different recommendation algorithms\n- Often used together to provide a comprehensive assessment of recommendation quality\n- May be incorporated into training pipelines for model selection and hyperparameter tuning\n\n## DEPENDENCIES:\n- Scikit-learn's metrics module for the `label_ranking_average_precision_score` function\n- Requires numpy arrays for `mAP` function inputs\n\n## RELATIONSHIPS:\n- The functions complement each other by measuring different aspects of recommendation quality:\n  - `hit_rate` focuses on presence of relevant items\n  - `mean_reciprocal_rank` prioritizes the position of the first relevant item\n  - `mAP` evaluates ranking quality across multiple labels/items\n- Together they provide a holistic evaluation framework for recommendation systems"
    }
  },
  {
    "page_content": "# FUNCTION: hit_rate\n\n## PURPOSE:\nCalculates the hit rate metric for a recommendation system by measuring the fraction of images for which the correct answer (ground truth) appears in the recommendation list.\n\n## INPUTS:\n- `topNPredicted` (Dict): A dictionary mapping user IDs to their top N recommendations, where each key is a user ID and its value is a list of recommended image IDs. Example: `{0: [1, 9, 20], 1: [3, 5, 30]}`\n- `total` (int): Total length of the dataset being evaluated\n- `gt` (List): Ground truth data, consisting of user-item pairs that represent actual interactions\n\n## OUTPUTS:\n- Float: The hit rate value, ranging from 0 to 1, representing the fraction of correct recommendations\n\n## KEY STEPS:\n- Initialize a counter for hits\n- Iterate through each user-item pair in the ground truth data\n- For each pair, check if the ground truth image ID appears in the user's recommendation list\n- If found, increment the hits counter\n- Calculate and return the ratio of hits to total dataset length\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\n- Used in recommendation system evaluation to assess how well the system includes relevant items in its recommendation lists\n- Typically used alongside other metrics like precision, recall, or NDCG to provide a comprehensive evaluation\n\n## EDGE CASES:\n- If a user ID in ground truth is not present in the topNPredicted dictionary, a KeyError would occur\n- Does not account for the position of the correct item in the recommendation list (treats all positions equally)\n\n## RELATIONSHIPS:\n- Likely part of a larger recommendation system evaluation framework\n- Complements other recommendation metrics that may focus on ranking quality rather than just inclusion\n\ndef hit_rate(topNPredicted, total, gt):\n    \"\"\"\n    Measures the fraction of images for which the correct answer is included in the recommendation list\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Hit rate\n\n    \"\"\"\n    hits = 0\n\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            hits += 1\n\n    return hits / total",
    "metadata": {
      "type": "FUNCTION",
      "name": "hit_rate",
      "path": "../mathsearch/ml-model/archive/old-files/ranking_metrics.py",
      "code": "def hit_rate(topNPredicted, total, gt):\n    \"\"\"\n    Measures the fraction of images for which the correct answer is included in the recommendation list\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Hit rate\n\n    \"\"\"\n    hits = 0\n\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            hits += 1\n\n    return hits / total",
      "summary": "# FUNCTION: hit_rate\n\n## PURPOSE:\nCalculates the hit rate metric for a recommendation system by measuring the fraction of images for which the correct answer (ground truth) appears in the recommendation list.\n\n## INPUTS:\n- `topNPredicted` (Dict): A dictionary mapping user IDs to their top N recommendations, where each key is a user ID and its value is a list of recommended image IDs. Example: `{0: [1, 9, 20], 1: [3, 5, 30]}`\n- `total` (int): Total length of the dataset being evaluated\n- `gt` (List): Ground truth data, consisting of user-item pairs that represent actual interactions\n\n## OUTPUTS:\n- Float: The hit rate value, ranging from 0 to 1, representing the fraction of correct recommendations\n\n## KEY STEPS:\n- Initialize a counter for hits\n- Iterate through each user-item pair in the ground truth data\n- For each pair, check if the ground truth image ID appears in the user's recommendation list\n- If found, increment the hits counter\n- Calculate and return the ratio of hits to total dataset length\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\n- Used in recommendation system evaluation to assess how well the system includes relevant items in its recommendation lists\n- Typically used alongside other metrics like precision, recall, or NDCG to provide a comprehensive evaluation\n\n## EDGE CASES:\n- If a user ID in ground truth is not present in the topNPredicted dictionary, a KeyError would occur\n- Does not account for the position of the correct item in the recommendation list (treats all positions equally)\n\n## RELATIONSHIPS:\n- Likely part of a larger recommendation system evaluation framework\n- Complements other recommendation metrics that may focus on ranking quality rather than just inclusion"
    }
  },
  {
    "page_content": "# FUNCTION: mean_reciprocal_rank\n\n## PURPOSE:\nCalculates the Mean Reciprocal Rank (MRR) metric which measures how effectively a recommendation system ranks relevant items, with higher scores indicating that relevant items appear closer to the top of recommendation lists.\n\n## INPUTS:\n- `topNPredicted` (Dict): Dictionary mapping user IDs to lists of recommended item IDs, where each key is a user ID and its value is a list of top N recommendations\n- `total` (int): Total number of items in the dataset to normalize the final score\n- `gt` (List): Ground truth data containing user-item pairs that are considered relevant\n\n## OUTPUTS:\n- (float): The Mean Reciprocal Rank score ranging from 0 to 1, where 1 indicates perfect ranking\n\n## KEY STEPS:\n- Iterate through ground truth user-item pairs\n- For each pair, check if the relevant item appears in the user's recommendations\n- If found, calculate the reciprocal of its position (rank)\n- Sum all reciprocal ranks\n- Divide the sum by the total number of items to get the final MRR score\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\n- Used in recommendation system evaluation to measure ranking quality\n- Commonly used alongside other metrics like precision, recall, or NDCG to evaluate search or recommendation algorithms\n\n## EDGE CASES:\n- Raises NotImplementedError if a ground truth item doesn't exist in the recommendations, indicating incomplete implementation\n- Does not handle empty recommendation lists or invalid user IDs\n\n## RELATIONSHIPS:\n- Part of a recommendation system evaluation framework\n- Complements other ranking metrics that may focus on different aspects of recommendation quality\n\ndef mean_reciprocal_rank(topNPredicted, total, gt):\n    \"\"\"\n    Measures how far down the ranking the first relevant document is\n    MRR --> 1 means relevant results are close to the top of search results\n    MRR --> 0 indicates poorer search quality, with the right answer farther down in the search results\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Mean Reciprocal Rank\n\n    \"\"\"\n    sum_reciprocal = 0\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            rank = recommendations.index(imageID)\n            sum_reciprocal += 1 / (rank)\n        else:\n            raise NotImplementedError(\"Need to figure what to count if doesn't exist!\")\n\n    return sum_reciprocal / total",
    "metadata": {
      "type": "FUNCTION",
      "name": "mean_reciprocal_rank",
      "path": "../mathsearch/ml-model/archive/old-files/ranking_metrics.py",
      "code": "def mean_reciprocal_rank(topNPredicted, total, gt):\n    \"\"\"\n    Measures how far down the ranking the first relevant document is\n    MRR --> 1 means relevant results are close to the top of search results\n    MRR --> 0 indicates poorer search quality, with the right answer farther down in the search results\n\n    Args:\n      topNPredicted (Dict): topNRecommendations for every image, where each key is an imageID and its respective value is the top N recommendations\n        Example: {\n          0: [1, 9, 20], 1: [3, 5, 30]\n        }\n      total (int): Length of dataset\n      gt (List): Groundtruth data\n\n    Returns:\n      Mean Reciprocal Rank\n\n    \"\"\"\n    sum_reciprocal = 0\n    for user in gt:\n        userID = user[0]\n        imageID = user[1]\n        recommendations = topNPredicted[userID]\n        if imageID in recommendations:\n            rank = recommendations.index(imageID)\n            sum_reciprocal += 1 / (rank)\n        else:\n            raise NotImplementedError(\"Need to figure what to count if doesn't exist!\")\n\n    return sum_reciprocal / total",
      "summary": "# FUNCTION: mean_reciprocal_rank\n\n## PURPOSE:\nCalculates the Mean Reciprocal Rank (MRR) metric which measures how effectively a recommendation system ranks relevant items, with higher scores indicating that relevant items appear closer to the top of recommendation lists.\n\n## INPUTS:\n- `topNPredicted` (Dict): Dictionary mapping user IDs to lists of recommended item IDs, where each key is a user ID and its value is a list of top N recommendations\n- `total` (int): Total number of items in the dataset to normalize the final score\n- `gt` (List): Ground truth data containing user-item pairs that are considered relevant\n\n## OUTPUTS:\n- (float): The Mean Reciprocal Rank score ranging from 0 to 1, where 1 indicates perfect ranking\n\n## KEY STEPS:\n- Iterate through ground truth user-item pairs\n- For each pair, check if the relevant item appears in the user's recommendations\n- If found, calculate the reciprocal of its position (rank)\n- Sum all reciprocal ranks\n- Divide the sum by the total number of items to get the final MRR score\n\n## DEPENDENCIES:\n- None\n\n## USAGE CONTEXT:\n- Used in recommendation system evaluation to measure ranking quality\n- Commonly used alongside other metrics like precision, recall, or NDCG to evaluate search or recommendation algorithms\n\n## EDGE CASES:\n- Raises NotImplementedError if a ground truth item doesn't exist in the recommendations, indicating incomplete implementation\n- Does not handle empty recommendation lists or invalid user IDs\n\n## RELATIONSHIPS:\n- Part of a recommendation system evaluation framework\n- Complements other ranking metrics that may focus on different aspects of recommendation quality"
    }
  },
  {
    "page_content": "# FUNCTION: mAP\n\n## PURPOSE:\nCalculates the mean Average Precision (mAP) score for multi-label classification by measuring the average precision of predicted label rankings against true labels across all samples.\n\n## INPUTS:\n- `y_true` (ndarray): Shape (n_samples, n_labels) - Binary true labels in one-hot encoded format\n- `y_pred` (ndarray): Shape (n_samples, n_labels) - Prediction scores or probabilities for each label\n\n## OUTPUTS:\n- float: The label ranking average precision score, representing how well the model ranks relevant labels higher than irrelevant ones\n\n## KEY STEPS:\n- Calls the scikit-learn function `label_ranking_average_precision_score`\n- For each sample, calculates the ratio of true vs total labels with lower scores\n- Averages these precision values across all samples and labels\n\n## DEPENDENCIES:\n- `label_ranking_average_precision_score` from scikit-learn metrics module\n\n## USAGE CONTEXT:\n- Used for evaluating multi-label classification models\n- Commonly used in information retrieval, object detection, and recommendation systems\n- Serves as a performance metric during model training or evaluation\n\n## EDGE CASES:\n- No explicit error handling in the function itself\n- Will rely on error handling from the underlying scikit-learn function\n- May produce undefined results if inputs contain NaN values or have mismatched shapes\n\n## RELATIONSHIPS:\n- Acts as a wrapper for scikit-learn's implementation\n- Likely part of a broader evaluation metrics suite for a machine learning system\n- Often used alongside other metrics like precision, recall, and F1-score\n\ndef mAP(y_true, y_pred):\n    \"\"\"\n    Measures average over each ground truth label assigned to each sample\n    of the ratio of true vs. total labels with lower score\n\n    Args:\n      y_true (ndarray) of shape (n_samples, n_labels): True binary labels in binary indicator format; One hot encoded\n      y_pred (ndarray) of shape (n_samples, n_labels): Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions\n    \"\"\"\n    return label_ranking_average_precision_score(y_true, y_pred)",
    "metadata": {
      "type": "FUNCTION",
      "name": "mAP",
      "path": "../mathsearch/ml-model/archive/old-files/ranking_metrics.py",
      "code": "def mAP(y_true, y_pred):\n    \"\"\"\n    Measures average over each ground truth label assigned to each sample\n    of the ratio of true vs. total labels with lower score\n\n    Args:\n      y_true (ndarray) of shape (n_samples, n_labels): True binary labels in binary indicator format; One hot encoded\n      y_pred (ndarray) of shape (n_samples, n_labels): Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions\n    \"\"\"\n    return label_ranking_average_precision_score(y_true, y_pred)",
      "summary": "# FUNCTION: mAP\n\n## PURPOSE:\nCalculates the mean Average Precision (mAP) score for multi-label classification by measuring the average precision of predicted label rankings against true labels across all samples.\n\n## INPUTS:\n- `y_true` (ndarray): Shape (n_samples, n_labels) - Binary true labels in one-hot encoded format\n- `y_pred` (ndarray): Shape (n_samples, n_labels) - Prediction scores or probabilities for each label\n\n## OUTPUTS:\n- float: The label ranking average precision score, representing how well the model ranks relevant labels higher than irrelevant ones\n\n## KEY STEPS:\n- Calls the scikit-learn function `label_ranking_average_precision_score`\n- For each sample, calculates the ratio of true vs total labels with lower scores\n- Averages these precision values across all samples and labels\n\n## DEPENDENCIES:\n- `label_ranking_average_precision_score` from scikit-learn metrics module\n\n## USAGE CONTEXT:\n- Used for evaluating multi-label classification models\n- Commonly used in information retrieval, object detection, and recommendation systems\n- Serves as a performance metric during model training or evaluation\n\n## EDGE CASES:\n- No explicit error handling in the function itself\n- Will rely on error handling from the underlying scikit-learn function\n- May produce undefined results if inputs contain NaN values or have mismatched shapes\n\n## RELATIONSHIPS:\n- Acts as a wrapper for scikit-learn's implementation\n- Likely part of a broader evaluation metrics suite for a machine learning system\n- Often used alongside other metrics like precision, recall, and F1-score"
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: old-siamese-model\n\n### PURPOSE:\nThis directory contains an implementation of a Siamese neural network architecture for image similarity tasks, utilizing contrastive loss to learn meaningful image embeddings that can differentiate between similar and dissimilar image pairs.\n\n### COMPONENT STRUCTURE:\n- `siamese.py`: The main file containing the complete Siamese network implementation, including the core network architecture, contrastive loss function, custom dataset handler, and training logic.\n\n### ARCHITECTURE:\nThe implementation follows a modular design with clear separation between the neural network model (SiameseNetwork), loss calculation (ContrastiveLoss), and data handling (SiameseNetworkDataset). The Siamese approach uses shared weights to process image pairs independently before comparing their embeddings.\n\n### ENTRY POINTS:\n- `SiameseNetwork` class: The main model definition that adapts a pre-trained VGG model for similarity learning\n- `train` function: The primary training loop that orchestrates the model optimization process\n\n### DATA FLOW:\n1. Image pairs and similarity labels are loaded via the SiameseNetworkDataset class from CSV files\n2. Pairs are processed through identical network branches with shared weights\n3. Features are extracted, pooled, flattened, and passed through fully connected layers\n4. Output embeddings from both images are compared using Euclidean distance\n5. ContrastiveLoss calculates the appropriate penalty based on similarity labels\n6. Gradient updates optimize the network to learn discriminative features\n\n### INTEGRATION:\nThe implementation integrates with external libraries including PyTorch, Pandas, PIL, NumPy, and Matplotlib. It expects a pre-trained model (likely VGG) to be provided and relies on image data organized in a specific CSV format with paired samples.\n\n### DEVELOPMENT PATTERNS:\n- Adaptation of pre-trained models for transfer learning\n- Custom dataset implementation for specialized data loading\n- Visualization utilities for debugging and monitoring training\n- Shared-weight approach for processing paired inputs\n- Contrastive loss implementation for similarity learning\n\n### RELATIONSHIPS:\nThe components function as a cohesive pipeline where the dataset feeds paired images to the network, which generates embeddings that are evaluated by the contrastive loss function. The training function coordinates this process while visualization utilities monitor progress. This architecture enables the learning of discriminative features for image similarity comparison.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "old-siamese-model",
      "path": "../mathsearch/ml-model/archive/old-siamese-model",
      "code": "",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: old-siamese-model\n\n### PURPOSE:\nThis directory contains an implementation of a Siamese neural network architecture for image similarity tasks, utilizing contrastive loss to learn meaningful image embeddings that can differentiate between similar and dissimilar image pairs.\n\n### COMPONENT STRUCTURE:\n- `siamese.py`: The main file containing the complete Siamese network implementation, including the core network architecture, contrastive loss function, custom dataset handler, and training logic.\n\n### ARCHITECTURE:\nThe implementation follows a modular design with clear separation between the neural network model (SiameseNetwork), loss calculation (ContrastiveLoss), and data handling (SiameseNetworkDataset). The Siamese approach uses shared weights to process image pairs independently before comparing their embeddings.\n\n### ENTRY POINTS:\n- `SiameseNetwork` class: The main model definition that adapts a pre-trained VGG model for similarity learning\n- `train` function: The primary training loop that orchestrates the model optimization process\n\n### DATA FLOW:\n1. Image pairs and similarity labels are loaded via the SiameseNetworkDataset class from CSV files\n2. Pairs are processed through identical network branches with shared weights\n3. Features are extracted, pooled, flattened, and passed through fully connected layers\n4. Output embeddings from both images are compared using Euclidean distance\n5. ContrastiveLoss calculates the appropriate penalty based on similarity labels\n6. Gradient updates optimize the network to learn discriminative features\n\n### INTEGRATION:\nThe implementation integrates with external libraries including PyTorch, Pandas, PIL, NumPy, and Matplotlib. It expects a pre-trained model (likely VGG) to be provided and relies on image data organized in a specific CSV format with paired samples.\n\n### DEVELOPMENT PATTERNS:\n- Adaptation of pre-trained models for transfer learning\n- Custom dataset implementation for specialized data loading\n- Visualization utilities for debugging and monitoring training\n- Shared-weight approach for processing paired inputs\n- Contrastive loss implementation for similarity learning\n\n### RELATIONSHIPS:\nThe components function as a cohesive pipeline where the dataset feeds paired images to the network, which generates embeddings that are evaluated by the contrastive loss function. The training function coordinates this process while visualization utilities monitor progress. This architecture enables the learning of discriminative features for image similarity comparison."
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## FILE: siamese.py\n\n## OVERVIEW:\nThis file implements a Siamese neural network architecture with contrastive loss for image similarity tasks, including the core network components, custom dataset handling, loss functions, and training logic.\n\n## KEY COMPONENTS:\n- `SiameseNetwork`: Neural network class that adapts a pre-trained VGG model for embedding generation\n- `ContrastiveLoss`: Loss function that minimizes distance between similar pairs and maximizes distance between dissimilar pairs\n- `SiameseNetworkDataset`: Custom dataset class for loading and processing image pairs with similarity labels\n- `imshow`: Utility function for displaying tensor images with optional text overlay\n- `show_plot`: Function for visualizing loss curves during training\n- `train`: Training loop function that handles the model optimization process\n\n## ARCHITECTURE:\nThe file follows a modular architecture with distinct components for the network model, loss calculation, and data handling. The Siamese network uses shared weights (implemented via the forward_once method) to process pairs of images independently before comparing their embeddings using contrastive loss.\n\n## DATA FLOW:\n1. Image pairs are loaded from a CSV file via the SiameseNetworkDataset\n2. The SiameseNetwork processes each image through identical feature extraction pipelines\n3. The forward_once method extracts features, applies pooling, flattens, and passes through a fully connected layer\n4. The forward method returns embeddings for both input images\n5. ContrastiveLoss calculates the similarity or dissimilarity penalty based on the Euclidean distance between embeddings\n6. The training loop optimizes the network based on these loss calculations\n\n## INTEGRATION POINTS:\n- Expects a pre-trained model (likely VGG) to be passed to SiameseNetwork constructor\n- Relies on image data organized in a specific CSV format with paired samples\n- Visualization functions interface with matplotlib for debugging and monitoring\n- Training function interacts with PyTorch's optimization utilities\n\n## USAGE PATTERNS:\n- Initialize the SiameseNetwork with a pre-trained model\n- Create a SiameseNetworkDataset with paths to image pairs and labels\n- Define ContrastiveLoss with an appropriate margin\n- Train the network using pairs of similar and dissimilar images\n- Monitor training progress using loss visualization\n- Use the trained model to generate embeddings for new images to compare similarity\n\n## DEPENDENCIES:\n- PyTorch (torch): For neural network components and tensor operations\n- Pandas: For loading and managing CSV data\n- PIL: For image loading and processing\n- NumPy: For array operations\n- Matplotlib: For visualization of images and loss curves\n\n## RELATIONSHIPS:\nThe components work together in a pipeline: the dataset provides paired images to the Siamese network, which produces embeddings that are compared by the contrastive loss function. The training function orchestrates this process by managing the optimization loop, while visualization utilities help monitor progress. The architecture enables learning discriminative features that can measure similarity between images.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "siamese.py",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "import os\nimport random\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.utils\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import models\n\n\ndef imshow(img, text=None, should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(\n            75,\n            8,\n            text,\n            style=\"italic\",\n            fontweight=\"bold\",\n            bbox={\"facecolor\": \"white\", \"alpha\": 0.8, \"pad\": 10},\n        )\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\ndef show_plot(iteration, loss):\n    plt.plot(iteration, loss)\n    plt.show()\n\n\nclass SiameseNetwork(nn.Module):\n  def __init__(self, model):\n    super(SiameseNetwork, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]\n  \n  def forward_once(self, x):\n        # Forward pass \n        out = self.features(x)\n        out = self.pooling(out)\n        out = self.flatten(out)\n        out = self.fc(out) \n        return out\n\n  def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n\n\n# from https://github.com/seanbenhur/siamese_net/blob/master/siamese-net/train.py\n# and https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\nclass ContrastiveLoss(torch.nn.Module):\n\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, x0, x1, y):\n        # euclidian distance\n        diff = x0 - x1\n        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n        dist = torch.sqrt(dist_sq)\n\n        mdist = self.margin - dist\n        dist = torch.clamp(mdist, min=0.0)\n        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n        return loss\n\n\n# preprocessing and loading the dataset\nclass SiameseDataset:\n    def __init__(self, training_csv=None, training_dir=None, transform=None):\n        # used to prepare the labels and images path\n        self.train_df = pd.read_csv(training_csv)\n        # self.train_df = pd.read_csv(training_csv)\n        # self.train_df.columns = [\"image1\", \"image2\", \"label\"]\n        self.train_dir = training_dir\n        self.transform = transform\n\n    def __getitem__(self, index):\n\n        # getting the image path\n        image1_path = os.path.join(self.train_dir, self.train_df.iat[index, 0])\n        image2_path = os.path.join(self.train_dir, self.train_df.iat[index, 1])\n\n        # Loading the image\n        img0 = Image.open(image1_path).convert(mode='RGB')\n        img1 = Image.open(image2_path).convert(mode='RGB')\n        # img0 = img0.convert(\"L\")\n        # img1 = img1.convert(\"L\")\n\n        # Apply image transformations\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n\n        return (\n            img0,\n            img1,\n            torch.from_numpy(\n                np.array([int(self.train_df.iat[index, 2])], dtype=np.float32)\n            ),\n        )\n\n    def __len__(self):\n        return len(self.train_df)\n\nclass ContrastiveLoss(nn.Module):\n    \"Contrastive loss function\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2)\n            + (label)\n            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n\n        return loss_contrastive\n\nif __name__ == \"__main__\":\n    df = pd.DataFrame(columns=['image1', 'image2', 'label'])\n    df_test = pd.DataFrame(columns=['image1', 'image2', 'label'])\n    data_dir = \"training_data/Training-Data-MathSearch/output\"\n    c = 0\n    for f in os.listdir(data_dir):\n\n        if c > 20: \n                df.to_csv(\"train_data.csv\", index=False, header=False)\n                df_test.to_csv(\"test_data.csv\", index=False, header=False)\n                break\n        \n        r1 = os.path.join(data_dir, random.choice(os.listdir(data_dir))) # random directory\n            \n        r0 = random.choice(os.listdir(os.path.join(data_dir, f, \"transformed\")))\n        r0 = os.path.join(data_dir, f, \"transformed\", r0)\n\n        if c % 10 != 0:\n            # image transform with random original\n            df.loc[len(df.index)] = [r0, os.path.join(r1, 'original_file.jpeg'), 0] \n            \n            # image transform with its own original\n            df.loc[len(df.index)] = [r0, os.path.join(data_dir, f,\"original_file.jpeg\"), 1]\n            c += 1\n        \n        if c % 10 == 0:\n            # image transform with random original\n            df_test.loc[len(df.index)] = [r0, os.path.join(r1, 'original_file.jpeg'), 0] \n            \n            # image transform with its own original\n            df_test.loc[len(df.index)] = [r0, os.path.join(data_dir, f,\"original_file.jpeg\"), 1]\n            c += 1\n\n    # raise AssertionError\n\n    training_dir = \"\"\n    training_csv = \"train_data.csv\"\n    testing_csv = \"test_data.csv\"\n    testing_dir = \"\"\n    vgg19_model = models.vgg19(pretrained=True)\n\n    net = SiameseNetwork(vgg19_model)\n    #   net = SiameseNetwork()\n    criterion = ContrastiveLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, weight_decay=0.0005)\n\n    # Load the the dataset from raw image folders\n    siamese_dataset = SiameseDataset(\n        training_csv,\n        training_dir,\n        # transform=transforms.ToTensor(),\n        transforms.Compose(\n            [transforms.Resize((105, 105)), transforms.ToTensor()]\n        ),\n    )\n\n    # Viewing the sample of images and to check whether its loading properly\n    vis_dataloader = DataLoader(siamese_dataset, shuffle=True, batch_size=8)\n    dataiter = iter(vis_dataloader)\n\n\n\n    train_dataloader = DataLoader(siamese_dataset,\n                            shuffle=True,\n                            num_workers=8,\n                            batch_size=32) \n\n    test_dataset = SiameseDataset(training_csv=testing_csv,training_dir=training_dir,\n                                            transform=transforms.Compose([transforms.Resize((105,105)),\n                                                                            transforms.ToTensor()\n                                                                            ])\n                                            )\n\n    test_dataloader = DataLoader(test_dataset,num_workers=6,batch_size=1,shuffle=True)\n\n    #train the model\n    def train():\n        loss=[] \n        counter=[]\n        iteration_number = 0\n\n        k = 0\n        for epoch in range(1, 2):\n            for i, data in enumerate(test_dataloader,0):\n                k += 1\n                if k % 10 == 0: print(k)\n            # for i, data in enumerate(train_dataloader,0):\n                img0, img1 , label = data\n                # img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n                optimizer.zero_grad()\n                output1,output2 = net(img0,img1)\n                loss_contrastive = criterion(output1,output2,label)\n                loss_contrastive.backward()\n                optimizer.step()    \n            print(\"Epoch {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number += 10\n            counter.append(iteration_number)\n            loss.append(loss_contrastive.item())\n        show_plot(counter, loss)   \n        return net\n\n\n    #set the device to cuda\n    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\n    #   device = torch.device('cpu')\n    model = train()\n    torch.save(model.state_dict(), \"model.pt\")\n    print(\"Model Saved Successfully\") \n\n\n    #test the network\n    count=0\n    for i, data in enumerate(test_dataloader,0): \n        x0, x1 , label = data\n        concat = torch.cat((x0,x1),0)\n        output1,output2 = model(x0.to(device),x1.to(device))\n\n        eucledian_distance = F.pairwise_distance(output1, output2)\n        \n        if label==torch.FloatTensor([[0]]):\n            label=\"Original Pair Of Signature\"\n        else:\n            label=\"Forged Pair Of Signature\"\n        \n        imshow(torchvision.utils.make_grid(concat))\n        print(\"Predicted Eucledian Distance:-\",eucledian_distance.item())\n        print(\"Actual Label:-\",label)\n        count=count+1\n        if count ==10:\n            break\n",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## FILE: siamese.py\n\n## OVERVIEW:\nThis file implements a Siamese neural network architecture with contrastive loss for image similarity tasks, including the core network components, custom dataset handling, loss functions, and training logic.\n\n## KEY COMPONENTS:\n- `SiameseNetwork`: Neural network class that adapts a pre-trained VGG model for embedding generation\n- `ContrastiveLoss`: Loss function that minimizes distance between similar pairs and maximizes distance between dissimilar pairs\n- `SiameseNetworkDataset`: Custom dataset class for loading and processing image pairs with similarity labels\n- `imshow`: Utility function for displaying tensor images with optional text overlay\n- `show_plot`: Function for visualizing loss curves during training\n- `train`: Training loop function that handles the model optimization process\n\n## ARCHITECTURE:\nThe file follows a modular architecture with distinct components for the network model, loss calculation, and data handling. The Siamese network uses shared weights (implemented via the forward_once method) to process pairs of images independently before comparing their embeddings using contrastive loss.\n\n## DATA FLOW:\n1. Image pairs are loaded from a CSV file via the SiameseNetworkDataset\n2. The SiameseNetwork processes each image through identical feature extraction pipelines\n3. The forward_once method extracts features, applies pooling, flattens, and passes through a fully connected layer\n4. The forward method returns embeddings for both input images\n5. ContrastiveLoss calculates the similarity or dissimilarity penalty based on the Euclidean distance between embeddings\n6. The training loop optimizes the network based on these loss calculations\n\n## INTEGRATION POINTS:\n- Expects a pre-trained model (likely VGG) to be passed to SiameseNetwork constructor\n- Relies on image data organized in a specific CSV format with paired samples\n- Visualization functions interface with matplotlib for debugging and monitoring\n- Training function interacts with PyTorch's optimization utilities\n\n## USAGE PATTERNS:\n- Initialize the SiameseNetwork with a pre-trained model\n- Create a SiameseNetworkDataset with paths to image pairs and labels\n- Define ContrastiveLoss with an appropriate margin\n- Train the network using pairs of similar and dissimilar images\n- Monitor training progress using loss visualization\n- Use the trained model to generate embeddings for new images to compare similarity\n\n## DEPENDENCIES:\n- PyTorch (torch): For neural network components and tensor operations\n- Pandas: For loading and managing CSV data\n- PIL: For image loading and processing\n- NumPy: For array operations\n- Matplotlib: For visualization of images and loss curves\n\n## RELATIONSHIPS:\nThe components work together in a pipeline: the dataset provides paired images to the Siamese network, which produces embeddings that are compared by the contrastive loss function. The training function orchestrates this process by managing the optimization loop, while visualization utilities help monitor progress. The architecture enables learning discriminative features that can measure similarity between images."
    }
  },
  {
    "page_content": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: imshow\n\nPURPOSE:\nDisplays a PyTorch tensor as an image using matplotlib, with optional text annotation and save functionality. It handles the conversion and transposition needed to properly visualize tensor data as images.\n\nINPUTS:\n- img (torch.Tensor): The input image tensor to display\n- text (str, optional): Text to overlay on the image, defaults to None\n- should_save (bool, optional): Flag indicating whether to save the image, defaults to False (though the save functionality isn't implemented in the code)\n\nOUTPUTS:\n- None: The function displays the image but doesn't return any values\n\nKEY STEPS:\n- Convert the PyTorch tensor to a NumPy array\n- Turn off the axis display for cleaner visualization\n- Add optional text annotation if provided\n- Transpose the dimensions from (C,H,W) to (H,W,C) for proper image display\n- Display the image using matplotlib\n\nDEPENDENCIES:\n- numpy (as np): For array manipulation\n- matplotlib.pyplot (as plt): For image visualization\n- torch: Implicitly needed as the function expects torch tensors as input\n\nUSAGE CONTEXT:\nTypically used in machine learning and computer vision applications to visualize tensor data representing images, such as model inputs, outputs, or intermediate feature maps during training or evaluation.\n\nEDGE CASES:\n- The should_save parameter is defined but not implemented in the function\n- No error handling for invalid tensor shapes or non-tensor inputs\n- May produce unexpected results if input tensor has values outside the expected image range\n\nRELATIONSHIPS:\nLikely part of a visualization toolkit for a PyTorch-based machine learning project, complementing other visualization functions for model inspection and debugging.\n\ndef imshow(img, text=None, should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(\n            75,\n            8,\n            text,\n            style=\"italic\",\n            fontweight=\"bold\",\n            bbox={\"facecolor\": \"white\", \"alpha\": 0.8, \"pad\": 10},\n        )\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()",
    "metadata": {
      "type": "FUNCTION",
      "name": "imshow",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "def imshow(img, text=None, should_save=False):\n    npimg = img.numpy()\n    plt.axis(\"off\")\n    if text:\n        plt.text(\n            75,\n            8,\n            text,\n            style=\"italic\",\n            fontweight=\"bold\",\n            bbox={\"facecolor\": \"white\", \"alpha\": 0.8, \"pad\": 10},\n        )\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()",
      "summary": "# STANDARDIZED FUNCTION SUMMARY\n\nFUNCTION: imshow\n\nPURPOSE:\nDisplays a PyTorch tensor as an image using matplotlib, with optional text annotation and save functionality. It handles the conversion and transposition needed to properly visualize tensor data as images.\n\nINPUTS:\n- img (torch.Tensor): The input image tensor to display\n- text (str, optional): Text to overlay on the image, defaults to None\n- should_save (bool, optional): Flag indicating whether to save the image, defaults to False (though the save functionality isn't implemented in the code)\n\nOUTPUTS:\n- None: The function displays the image but doesn't return any values\n\nKEY STEPS:\n- Convert the PyTorch tensor to a NumPy array\n- Turn off the axis display for cleaner visualization\n- Add optional text annotation if provided\n- Transpose the dimensions from (C,H,W) to (H,W,C) for proper image display\n- Display the image using matplotlib\n\nDEPENDENCIES:\n- numpy (as np): For array manipulation\n- matplotlib.pyplot (as plt): For image visualization\n- torch: Implicitly needed as the function expects torch tensors as input\n\nUSAGE CONTEXT:\nTypically used in machine learning and computer vision applications to visualize tensor data representing images, such as model inputs, outputs, or intermediate feature maps during training or evaluation.\n\nEDGE CASES:\n- The should_save parameter is defined but not implemented in the function\n- No error handling for invalid tensor shapes or non-tensor inputs\n- May produce unexpected results if input tensor has values outside the expected image range\n\nRELATIONSHIPS:\nLikely part of a visualization toolkit for a PyTorch-based machine learning project, complementing other visualization functions for model inspection and debugging."
    }
  },
  {
    "page_content": "# FUNCTION: show_plot\n\n## PURPOSE:\nDisplays a line plot of loss values against iteration numbers. This function provides a visual representation of how loss changes over iterations, which is useful for monitoring model training progress.\n\n## INPUTS:\n- `iteration` (list or array): The x-axis values representing iteration numbers or steps\n- `loss` (list or array): The y-axis values representing loss values for each iteration\n\n## OUTPUTS:\n- None (displays a plot on screen but doesn't return any value)\n\n## KEY STEPS:\n- Creates a line plot with iteration numbers on the x-axis and loss values on the y-axis\n- Displays the plot to the user\n\n## DEPENDENCIES:\n- `matplotlib.pyplot` (imported as `plt`)\n\n## USAGE CONTEXT:\n- Typically used during model training to visualize the training loss over time\n- Can be called periodically during training or after training is complete to inspect the learning curve\n- Useful for debugging and tuning machine learning models\n\n## EDGE CASES:\n- No explicit error handling\n- Will fail if `iteration` and `loss` have different lengths\n- May produce unexpected results if inputs are not numeric data types\n\n## RELATIONSHIPS:\n- Likely used in conjunction with training loop functions that track iteration numbers and loss values\n- Part of a visualization toolkit for model performance monitoring\n\ndef show_plot(iteration, loss):\n    plt.plot(iteration, loss)\n    plt.show()",
    "metadata": {
      "type": "FUNCTION",
      "name": "show_plot",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "def show_plot(iteration, loss):\n    plt.plot(iteration, loss)\n    plt.show()",
      "summary": "# FUNCTION: show_plot\n\n## PURPOSE:\nDisplays a line plot of loss values against iteration numbers. This function provides a visual representation of how loss changes over iterations, which is useful for monitoring model training progress.\n\n## INPUTS:\n- `iteration` (list or array): The x-axis values representing iteration numbers or steps\n- `loss` (list or array): The y-axis values representing loss values for each iteration\n\n## OUTPUTS:\n- None (displays a plot on screen but doesn't return any value)\n\n## KEY STEPS:\n- Creates a line plot with iteration numbers on the x-axis and loss values on the y-axis\n- Displays the plot to the user\n\n## DEPENDENCIES:\n- `matplotlib.pyplot` (imported as `plt`)\n\n## USAGE CONTEXT:\n- Typically used during model training to visualize the training loss over time\n- Can be called periodically during training or after training is complete to inspect the learning curve\n- Useful for debugging and tuning machine learning models\n\n## EDGE CASES:\n- No explicit error handling\n- Will fail if `iteration` and `loss` have different lengths\n- May produce unexpected results if inputs are not numeric data types\n\n## RELATIONSHIPS:\n- Likely used in conjunction with training loop functions that track iteration numbers and loss values\n- Part of a visualization toolkit for model performance monitoring"
    }
  },
  {
    "page_content": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a Siamese Network by adapting a pre-trained VGG model and extracting its components to create a feature extraction pipeline for image similarity tasks.\n\n## INPUTS:\n- `model` (nn.Module): A pre-trained model (presumably VGG-16) that will be used as the feature extractor\n\n## OUTPUTS:\n- None (constructor initializes instance attributes)\n\n## KEY STEPS:\n- Call the parent class constructor\n- Extract feature layers from the input model and store them as a sequential module\n- Extract the average pooling layer from the input model\n- Create a flatten layer to convert 2D feature maps to 1D vectors\n- Extract the first fully-connected layer from the model's classifier\n\n## DEPENDENCIES:\n- torch.nn (for Sequential, Flatten components)\n- Parent class (SiameseNetwork inherits from some base class)\n\n## USAGE CONTEXT:\nUsed when instantiating a SiameseNetwork model for tasks like face recognition, signature verification, or other image comparison applications where feature similarity is important.\n\n## EDGE CASES:\n- Assumes the input model has specific attributes (features, avgpool, classifier) matching VGG16 architecture\n- May fail if provided with a model that doesn't follow the expected structure\n\n## RELATIONSHIPS:\n- This constructor sets up the network architecture that will be used by forward methods in the Siamese Network\n- Represents the first step in creating a twin-network architecture where two images can be compared using the same feature extraction pipeline\n\n  def __init__(self, model):\n    super(SiameseNetwork, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "  def __init__(self, model):\n    super(SiameseNetwork, self).__init__()\n\t\t# Extract VGG-16 Feature Layers\n    self.features = list(model.features)\n    self.features = nn.Sequential(*self.features)\n\t\t# Extract VGG-16 Average Pooling Layer\n    self.pooling = model.avgpool\n\t\t# Convert the image into one-dimensional vector\n    self.flatten = nn.Flatten()\n\t\t# Extract the first part of fully-connected layer from VGG16\n    self.fc = model.classifier[0]",
      "summary": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a Siamese Network by adapting a pre-trained VGG model and extracting its components to create a feature extraction pipeline for image similarity tasks.\n\n## INPUTS:\n- `model` (nn.Module): A pre-trained model (presumably VGG-16) that will be used as the feature extractor\n\n## OUTPUTS:\n- None (constructor initializes instance attributes)\n\n## KEY STEPS:\n- Call the parent class constructor\n- Extract feature layers from the input model and store them as a sequential module\n- Extract the average pooling layer from the input model\n- Create a flatten layer to convert 2D feature maps to 1D vectors\n- Extract the first fully-connected layer from the model's classifier\n\n## DEPENDENCIES:\n- torch.nn (for Sequential, Flatten components)\n- Parent class (SiameseNetwork inherits from some base class)\n\n## USAGE CONTEXT:\nUsed when instantiating a SiameseNetwork model for tasks like face recognition, signature verification, or other image comparison applications where feature similarity is important.\n\n## EDGE CASES:\n- Assumes the input model has specific attributes (features, avgpool, classifier) matching VGG16 architecture\n- May fail if provided with a model that doesn't follow the expected structure\n\n## RELATIONSHIPS:\n- This constructor sets up the network architecture that will be used by forward methods in the Siamese Network\n- Represents the first step in creating a twin-network architecture where two images can be compared using the same feature extraction pipeline"
    }
  },
  {
    "page_content": "# FUNCTION: forward_once\n\n## PURPOSE:\nPerforms a single forward pass through a neural network, processing an input tensor through a series of layers to produce an embedding or feature representation.\n\n## INPUTS:\n- `x` (torch.Tensor): Input tensor, typically an image or feature map\n\n## OUTPUTS:\n- `out` (torch.Tensor): Output tensor containing the processed features or embeddings\n\n## KEY STEPS:\n- Process the input through feature extraction layers using `self.features`\n- Apply pooling operation to reduce spatial dimensions using `self.pooling`\n- Flatten the pooled output to a 1D tensor using `self.flatten`\n- Process through fully connected layer(s) using `self.fc`\n\n## DEPENDENCIES:\n- `self.features`: Feature extraction layers (likely CNN layers)\n- `self.pooling`: Pooling layer (likely max pooling or average pooling)\n- `self.flatten`: Flattening operation\n- `self.fc`: Fully connected layer(s)\n\n## USAGE CONTEXT:\nTypically used within Siamese networks or similar architectures where the same network processes multiple inputs independently. The function is called multiple times with different inputs, and the outputs are compared.\n\n## EDGE CASES:\nThe function assumes the input tensor has the correct shape and type for the feature extraction layers. No explicit error handling is included.\n\n## RELATIONSHIPS:\nThis function is likely a member of a neural network class, possibly a Siamese network. It represents half of the typical Siamese architecture, processing one branch of input through shared weights.\n\n  def forward_once(self, x):\n        # Forward pass \n        out = self.features(x)\n        out = self.pooling(out)\n        out = self.flatten(out)\n        out = self.fc(out) \n        return out",
    "metadata": {
      "type": "FUNCTION",
      "name": "forward_once",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "  def forward_once(self, x):\n        # Forward pass \n        out = self.features(x)\n        out = self.pooling(out)\n        out = self.flatten(out)\n        out = self.fc(out) \n        return out",
      "summary": "# FUNCTION: forward_once\n\n## PURPOSE:\nPerforms a single forward pass through a neural network, processing an input tensor through a series of layers to produce an embedding or feature representation.\n\n## INPUTS:\n- `x` (torch.Tensor): Input tensor, typically an image or feature map\n\n## OUTPUTS:\n- `out` (torch.Tensor): Output tensor containing the processed features or embeddings\n\n## KEY STEPS:\n- Process the input through feature extraction layers using `self.features`\n- Apply pooling operation to reduce spatial dimensions using `self.pooling`\n- Flatten the pooled output to a 1D tensor using `self.flatten`\n- Process through fully connected layer(s) using `self.fc`\n\n## DEPENDENCIES:\n- `self.features`: Feature extraction layers (likely CNN layers)\n- `self.pooling`: Pooling layer (likely max pooling or average pooling)\n- `self.flatten`: Flattening operation\n- `self.fc`: Fully connected layer(s)\n\n## USAGE CONTEXT:\nTypically used within Siamese networks or similar architectures where the same network processes multiple inputs independently. The function is called multiple times with different inputs, and the outputs are compared.\n\n## EDGE CASES:\nThe function assumes the input tensor has the correct shape and type for the feature extraction layers. No explicit error handling is included.\n\n## RELATIONSHIPS:\nThis function is likely a member of a neural network class, possibly a Siamese network. It represents half of the typical Siamese architecture, processing one branch of input through shared weights."
    }
  },
  {
    "page_content": "# FUNCTION: forward\n\n## PURPOSE:\nProcesses a pair of inputs through the network, generating embeddings for both inputs separately using the same underlying network. This enables comparison between the two inputs in a siamese network architecture.\n\n## INPUTS:\n- `input1`: Tensor - The first input to be processed through the network\n- `input2`: Tensor - The second input to be processed through the network\n\n## OUTPUTS:\n- Tuple of (output1, output2): Tuple[Tensor, Tensor] - Embeddings/feature representations for both inputs\n\n## KEY STEPS:\n- Process the first input (input1) through forward_once method to get output1\n- Process the second input (input2) through the same forward_once method to get output2\n- Return both outputs as a tuple\n\n## DEPENDENCIES:\n- self.forward_once: Method that processes a single input through the network\n\n## USAGE CONTEXT:\nTypically used in siamese networks for similarity learning, face recognition, or verification tasks where two inputs need to be compared. The embeddings can later be used to compute similarity or distance metrics.\n\n## EDGE CASES:\n- Both inputs should be compatible with the forward_once method\n- If inputs have different shapes/formats, the behavior depends on how forward_once handles them\n\n## RELATIONSHIPS:\n- Part of a siamese network architecture\n- Relies on forward_once method which contains the actual network's forward pass logic\n- Output embeddings are typically used with a distance function or contrastive loss\n\n  def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2",
    "metadata": {
      "type": "FUNCTION",
      "name": "forward",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "  def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2",
      "summary": "# FUNCTION: forward\n\n## PURPOSE:\nProcesses a pair of inputs through the network, generating embeddings for both inputs separately using the same underlying network. This enables comparison between the two inputs in a siamese network architecture.\n\n## INPUTS:\n- `input1`: Tensor - The first input to be processed through the network\n- `input2`: Tensor - The second input to be processed through the network\n\n## OUTPUTS:\n- Tuple of (output1, output2): Tuple[Tensor, Tensor] - Embeddings/feature representations for both inputs\n\n## KEY STEPS:\n- Process the first input (input1) through forward_once method to get output1\n- Process the second input (input2) through the same forward_once method to get output2\n- Return both outputs as a tuple\n\n## DEPENDENCIES:\n- self.forward_once: Method that processes a single input through the network\n\n## USAGE CONTEXT:\nTypically used in siamese networks for similarity learning, face recognition, or verification tasks where two inputs need to be compared. The embeddings can later be used to compute similarity or distance metrics.\n\n## EDGE CASES:\n- Both inputs should be compatible with the forward_once method\n- If inputs have different shapes/formats, the behavior depends on how forward_once handles them\n\n## RELATIONSHIPS:\n- Part of a siamese network architecture\n- Relies on forward_once method which contains the actual network's forward pass logic\n- Output embeddings are typically used with a distance function or contrastive loss"
    }
  },
  {
    "page_content": "# Standardized Natural Language Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a ContrastiveLoss object with a specified margin value. This constructor sets up the contrastive loss function used for comparing pairs of samples in siamese networks.\n\nINPUTS:\n- margin (float, default=1.0): The minimum distance threshold that dissimilar pairs should exceed. Controls the separation boundary between similar and dissimilar pairs.\n\nOUTPUTS:\n- None (initializes the object state)\n\nKEY STEPS:\n- Calls the parent class initializer using super()\n- Stores the margin parameter as an instance variable\n\nDEPENDENCIES:\n- Parent class (likely a PyTorch nn.Module or similar loss function base class)\n\nUSAGE CONTEXT:\n- Used when instantiating a ContrastiveLoss object before training neural networks, particularly in siamese network architectures\n- Typically passed as a loss function to an optimizer during training\n\nEDGE CASES:\n- Setting margin too small may result in poor discrimination between similar and dissimilar pairs\n- Setting margin too large may make convergence difficult during training\n\nRELATIONSHIPS:\n- Part of a loss function class likely used with a forward() method that computes the actual loss value\n- Works in conjunction with siamese or triplet network architectures that produce embeddings for similarity comparison\n\n    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def __init__(self, margin=1.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin",
      "summary": "# Standardized Natural Language Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a ContrastiveLoss object with a specified margin value. This constructor sets up the contrastive loss function used for comparing pairs of samples in siamese networks.\n\nINPUTS:\n- margin (float, default=1.0): The minimum distance threshold that dissimilar pairs should exceed. Controls the separation boundary between similar and dissimilar pairs.\n\nOUTPUTS:\n- None (initializes the object state)\n\nKEY STEPS:\n- Calls the parent class initializer using super()\n- Stores the margin parameter as an instance variable\n\nDEPENDENCIES:\n- Parent class (likely a PyTorch nn.Module or similar loss function base class)\n\nUSAGE CONTEXT:\n- Used when instantiating a ContrastiveLoss object before training neural networks, particularly in siamese network architectures\n- Typically passed as a loss function to an optimizer during training\n\nEDGE CASES:\n- Setting margin too small may result in poor discrimination between similar and dissimilar pairs\n- Setting margin too large may make convergence difficult during training\n\nRELATIONSHIPS:\n- Part of a loss function class likely used with a forward() method that computes the actual loss value\n- Works in conjunction with siamese or triplet network architectures that produce embeddings for similarity comparison"
    }
  },
  {
    "page_content": "# FUNCTION: forward\n\n## PURPOSE:\nComputes the contrastive loss between pairs of inputs, commonly used in siamese networks to bring similar pairs closer and push dissimilar pairs further apart in the embedding space.\n\n## INPUTS:\n- `x0` (torch.Tensor): First input tensor representing embeddings of the first items in pairs\n- `x1` (torch.Tensor): Second input tensor representing embeddings of the second items in pairs\n- `y` (torch.Tensor): Binary labels indicating if pairs are similar (1) or dissimilar (0)\n\n## OUTPUTS:\n- `loss` (torch.Tensor): Scalar tensor containing the calculated contrastive loss value\n\n## KEY STEPS:\n- Calculate the Euclidean distance between each pair of embeddings (x0, x1)\n- For similar pairs (y=1), minimize the squared distance\n- For dissimilar pairs (y=0), minimize the squared distance to margin if distance < margin\n- Average the loss across all pairs in the batch\n\n## DEPENDENCIES:\n- `torch`: PyTorch library for tensor operations\n- `self.margin`: Class attribute defining the minimum distance threshold for dissimilar pairs\n\n## USAGE CONTEXT:\n- Used as the loss function in siamese neural networks\n- Called during the training process to compute gradients for network optimization\n- Typically part of a custom loss module derived from nn.Module\n\n## EDGE CASES:\n- If distances are very small, sqrt operation near zero could cause numerical instability\n- The loss behaves differently for y=0 vs y=1 cases by design\n- The margin parameter must be properly tuned for the embedding space dimensions\n\n## RELATIONSHIPS:\n- Part of a loss function class in a siamese network architecture\n- Works with embedding outputs produced by neural network layers\n- Used by optimizer to update network weights through backpropagation\n\n    def forward(self, x0, x1, y):\n        # euclidian distance\n        diff = x0 - x1\n        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n        dist = torch.sqrt(dist_sq)\n\n        mdist = self.margin - dist\n        dist = torch.clamp(mdist, min=0.0)\n        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n        return loss",
    "metadata": {
      "type": "FUNCTION",
      "name": "forward",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def forward(self, x0, x1, y):\n        # euclidian distance\n        diff = x0 - x1\n        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n        dist = torch.sqrt(dist_sq)\n\n        mdist = self.margin - dist\n        dist = torch.clamp(mdist, min=0.0)\n        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n        return loss",
      "summary": "# FUNCTION: forward\n\n## PURPOSE:\nComputes the contrastive loss between pairs of inputs, commonly used in siamese networks to bring similar pairs closer and push dissimilar pairs further apart in the embedding space.\n\n## INPUTS:\n- `x0` (torch.Tensor): First input tensor representing embeddings of the first items in pairs\n- `x1` (torch.Tensor): Second input tensor representing embeddings of the second items in pairs\n- `y` (torch.Tensor): Binary labels indicating if pairs are similar (1) or dissimilar (0)\n\n## OUTPUTS:\n- `loss` (torch.Tensor): Scalar tensor containing the calculated contrastive loss value\n\n## KEY STEPS:\n- Calculate the Euclidean distance between each pair of embeddings (x0, x1)\n- For similar pairs (y=1), minimize the squared distance\n- For dissimilar pairs (y=0), minimize the squared distance to margin if distance < margin\n- Average the loss across all pairs in the batch\n\n## DEPENDENCIES:\n- `torch`: PyTorch library for tensor operations\n- `self.margin`: Class attribute defining the minimum distance threshold for dissimilar pairs\n\n## USAGE CONTEXT:\n- Used as the loss function in siamese neural networks\n- Called during the training process to compute gradients for network optimization\n- Typically part of a custom loss module derived from nn.Module\n\n## EDGE CASES:\n- If distances are very small, sqrt operation near zero could cause numerical instability\n- The loss behaves differently for y=0 vs y=1 cases by design\n- The margin parameter must be properly tuned for the embedding space dimensions\n\n## RELATIONSHIPS:\n- Part of a loss function class in a siamese network architecture\n- Works with embedding outputs produced by neural network layers\n- Used by optimizer to update network weights through backpropagation"
    }
  },
  {
    "page_content": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a dataset class for image comparison tasks by loading labels from a CSV file and setting up the necessary paths and transformations for the images.\n\n## INPUTS:\n- `training_csv` (str, optional): Path to a CSV file containing image pairs and their labels\n- `training_dir` (str, optional): Directory path where the images are stored\n- `transform` (callable, optional): Transformation function to be applied to the images\n\n## OUTPUTS:\n- None (constructor initializes instance attributes)\n\n## KEY STEPS:\n- Reads the CSV file containing training data into a pandas DataFrame\n- Stores the training directory path for later access to images\n- Saves the transform function for applying to images during data loading\n\n## DEPENDENCIES:\n- pandas: Required for reading and managing the CSV data\n\n## USAGE CONTEXT:\nUsed when instantiating a custom dataset class, likely for siamese networks or image comparison tasks where pairs of images need to be compared.\n\n## EDGE CASES:\n- If `training_csv` is None, the function will attempt to read from None and likely raise an error\n- Does not validate if the training directory exists or contains the referenced images\n\n## RELATIONSHIPS:\n- Serves as the constructor for a dataset class that likely implements other methods like `__getitem__` and `__len__` to function as a PyTorch Dataset\n- The data loaded here would be used by other methods in the class to retrieve and process image pairs\n\n    def __init__(self, training_csv=None, training_dir=None, transform=None):\n        # used to prepare the labels and images path\n        self.train_df = pd.read_csv(training_csv)\n        # self.train_df = pd.read_csv(training_csv)\n        # self.train_df.columns = [\"image1\", \"image2\", \"label\"]\n        self.train_dir = training_dir\n        self.transform = transform",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def __init__(self, training_csv=None, training_dir=None, transform=None):\n        # used to prepare the labels and images path\n        self.train_df = pd.read_csv(training_csv)\n        # self.train_df = pd.read_csv(training_csv)\n        # self.train_df.columns = [\"image1\", \"image2\", \"label\"]\n        self.train_dir = training_dir\n        self.transform = transform",
      "summary": "# FUNCTION: __init__\n\n## PURPOSE:\nInitializes a dataset class for image comparison tasks by loading labels from a CSV file and setting up the necessary paths and transformations for the images.\n\n## INPUTS:\n- `training_csv` (str, optional): Path to a CSV file containing image pairs and their labels\n- `training_dir` (str, optional): Directory path where the images are stored\n- `transform` (callable, optional): Transformation function to be applied to the images\n\n## OUTPUTS:\n- None (constructor initializes instance attributes)\n\n## KEY STEPS:\n- Reads the CSV file containing training data into a pandas DataFrame\n- Stores the training directory path for later access to images\n- Saves the transform function for applying to images during data loading\n\n## DEPENDENCIES:\n- pandas: Required for reading and managing the CSV data\n\n## USAGE CONTEXT:\nUsed when instantiating a custom dataset class, likely for siamese networks or image comparison tasks where pairs of images need to be compared.\n\n## EDGE CASES:\n- If `training_csv` is None, the function will attempt to read from None and likely raise an error\n- Does not validate if the training directory exists or contains the referenced images\n\n## RELATIONSHIPS:\n- Serves as the constructor for a dataset class that likely implements other methods like `__getitem__` and `__len__` to function as a PyTorch Dataset\n- The data loaded here would be used by other methods in the class to retrieve and process image pairs"
    }
  },
  {
    "page_content": "# FUNCTION: __getitem__\n\n## PURPOSE:\nRetrieves a pair of images and their similarity label at a specified index from a dataset, enabling access to training data for a Siamese network or similar model.\n\n## INPUTS:\n- `self`: The class instance containing dataset information\n- `index` (int): The index of the data point to retrieve from the dataset\n\n## OUTPUTS:\n- Tuple containing:\n  - `img0` (Tensor): First transformed image\n  - `img1` (Tensor): Second transformed image\n  - `label` (Tensor): A single-element tensor containing the similarity label (0 or 1) as float32\n\n## KEY STEPS:\n- Construct file paths for both images using index and the training directory\n- Load both images and convert them to RGB mode\n- Apply transformations to both images if a transform is defined\n- Return a tuple of both images and the similarity label as a tensor\n\n## DEPENDENCIES:\n- `os`: For path manipulation\n- `PIL.Image`: For image loading and processing\n- `torch`: For tensor conversion\n- `numpy`: For array operations\n\n## USAGE CONTEXT:\nUsed by PyTorch DataLoader to iterate through paired images during training of similarity or verification models like Siamese networks.\n\n## EDGE CASES:\n- If images don't exist at the specified paths, will raise file-related errors\n- No explicit error handling for corrupt images or missing data\n- Assumes the dataframe has at least 3 columns with specific meaning\n\n## RELATIONSHIPS:\n- Part of a custom Dataset class implementing the PyTorch Dataset interface\n- Works with the class's `train_df` dataframe that contains image paths and labels\n- Expects a compatible `transform` object for preprocessing images\n\n    def __getitem__(self, index):\n\n        # getting the image path\n        image1_path = os.path.join(self.train_dir, self.train_df.iat[index, 0])\n        image2_path = os.path.join(self.train_dir, self.train_df.iat[index, 1])\n\n        # Loading the image\n        img0 = Image.open(image1_path).convert(mode='RGB')\n        img1 = Image.open(image2_path).convert(mode='RGB')\n        # img0 = img0.convert(\"L\")\n        # img1 = img1.convert(\"L\")\n\n        # Apply image transformations\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n\n        return (\n            img0,\n            img1,\n            torch.from_numpy(\n                np.array([int(self.train_df.iat[index, 2])], dtype=np.float32)\n            ),\n        )",
    "metadata": {
      "type": "FUNCTION",
      "name": "__getitem__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def __getitem__(self, index):\n\n        # getting the image path\n        image1_path = os.path.join(self.train_dir, self.train_df.iat[index, 0])\n        image2_path = os.path.join(self.train_dir, self.train_df.iat[index, 1])\n\n        # Loading the image\n        img0 = Image.open(image1_path).convert(mode='RGB')\n        img1 = Image.open(image2_path).convert(mode='RGB')\n        # img0 = img0.convert(\"L\")\n        # img1 = img1.convert(\"L\")\n\n        # Apply image transformations\n        if self.transform is not None:\n            img0 = self.transform(img0)\n            img1 = self.transform(img1)\n\n        return (\n            img0,\n            img1,\n            torch.from_numpy(\n                np.array([int(self.train_df.iat[index, 2])], dtype=np.float32)\n            ),\n        )",
      "summary": "# FUNCTION: __getitem__\n\n## PURPOSE:\nRetrieves a pair of images and their similarity label at a specified index from a dataset, enabling access to training data for a Siamese network or similar model.\n\n## INPUTS:\n- `self`: The class instance containing dataset information\n- `index` (int): The index of the data point to retrieve from the dataset\n\n## OUTPUTS:\n- Tuple containing:\n  - `img0` (Tensor): First transformed image\n  - `img1` (Tensor): Second transformed image\n  - `label` (Tensor): A single-element tensor containing the similarity label (0 or 1) as float32\n\n## KEY STEPS:\n- Construct file paths for both images using index and the training directory\n- Load both images and convert them to RGB mode\n- Apply transformations to both images if a transform is defined\n- Return a tuple of both images and the similarity label as a tensor\n\n## DEPENDENCIES:\n- `os`: For path manipulation\n- `PIL.Image`: For image loading and processing\n- `torch`: For tensor conversion\n- `numpy`: For array operations\n\n## USAGE CONTEXT:\nUsed by PyTorch DataLoader to iterate through paired images during training of similarity or verification models like Siamese networks.\n\n## EDGE CASES:\n- If images don't exist at the specified paths, will raise file-related errors\n- No explicit error handling for corrupt images or missing data\n- Assumes the dataframe has at least 3 columns with specific meaning\n\n## RELATIONSHIPS:\n- Part of a custom Dataset class implementing the PyTorch Dataset interface\n- Works with the class's `train_df` dataframe that contains image paths and labels\n- Expects a compatible `transform` object for preprocessing images"
    }
  },
  {
    "page_content": "# FUNCTION: `__len__`\n\n## PURPOSE:\nReturns the number of samples in the training dataset by calculating the length of the training dataframe. This allows the class to support Python's built-in `len()` function.\n\n## INPUTS:\n- `self`: Reference to the instance of the class that contains the training dataframe.\n\n## OUTPUTS:\n- `int`: The number of rows in the training dataframe, representing the dataset size.\n\n## KEY STEPS:\n- Accesses the `train_df` attribute of the instance\n- Returns the length of this dataframe using Python's built-in `len()` function\n\n## DEPENDENCIES:\n- Requires the `train_df` attribute to be properly initialized with a pandas DataFrame or similar object that supports the `len()` operation\n\n## USAGE CONTEXT:\n- Used when the size of the dataset needs to be determined, such as when configuring batch sizes or reporting dataset statistics\n- Called implicitly when the instance is passed to Python's built-in `len()` function\n\n## EDGE CASES:\n- Will raise an attribute error if `train_df` is not initialized\n- Will raise a type error if `train_df` doesn't support the `len()` operation\n\n## RELATIONSHIPS:\n- Part of Python's container protocol implementation for the class\n- Complements other container protocol methods like `__getitem__` that might be implemented in the class\n\n    def __len__(self):\n        return len(self.train_df)",
    "metadata": {
      "type": "FUNCTION",
      "name": "__len__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def __len__(self):\n        return len(self.train_df)",
      "summary": "# FUNCTION: `__len__`\n\n## PURPOSE:\nReturns the number of samples in the training dataset by calculating the length of the training dataframe. This allows the class to support Python's built-in `len()` function.\n\n## INPUTS:\n- `self`: Reference to the instance of the class that contains the training dataframe.\n\n## OUTPUTS:\n- `int`: The number of rows in the training dataframe, representing the dataset size.\n\n## KEY STEPS:\n- Accesses the `train_df` attribute of the instance\n- Returns the length of this dataframe using Python's built-in `len()` function\n\n## DEPENDENCIES:\n- Requires the `train_df` attribute to be properly initialized with a pandas DataFrame or similar object that supports the `len()` operation\n\n## USAGE CONTEXT:\n- Used when the size of the dataset needs to be determined, such as when configuring batch sizes or reporting dataset statistics\n- Called implicitly when the instance is passed to Python's built-in `len()` function\n\n## EDGE CASES:\n- Will raise an attribute error if `train_df` is not initialized\n- Will raise a type error if `train_df` doesn't support the `len()` operation\n\n## RELATIONSHIPS:\n- Part of Python's container protocol implementation for the class\n- Complements other container protocol methods like `__getitem__` that might be implemented in the class"
    }
  },
  {
    "page_content": "# Standardized Natural Language Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a ContrastiveLoss object with a specified margin value. This constructor sets up the initial state for calculating contrastive loss, which is used in siamese networks to compare pairs of inputs.\n\nINPUTS:\n- margin (float, optional): The minimum distance threshold between dissimilar pairs. Default value is 2.0.\n\nOUTPUTS:\n- None (initializes the object state)\n\nKEY STEPS:\n- Calls the parent class initializer\n- Stores the provided margin value as an instance attribute\n\nDEPENDENCIES:\n- Parent class (likely a PyTorch nn.Module or similar loss function base class)\n\nUSAGE CONTEXT:\n- Used when instantiating a ContrastiveLoss object before applying it to calculate loss in siamese networks\n- Typically used in deep learning applications involving similarity learning, face recognition, or signature verification\n\nEDGE CASES:\n- No explicit error handling, though very small or negative margin values might lead to poor model performance\n\nRELATIONSHIPS:\n- This is the constructor for the ContrastiveLoss class\n- Works with the forward method (not shown) which would use this margin value to calculate the actual loss\n- Inherits from a parent class (likely nn.Module in PyTorch)\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin",
    "metadata": {
      "type": "FUNCTION",
      "name": "__init__",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin",
      "summary": "# Standardized Natural Language Summary\n\nFUNCTION: __init__\n\nPURPOSE:\nInitializes a ContrastiveLoss object with a specified margin value. This constructor sets up the initial state for calculating contrastive loss, which is used in siamese networks to compare pairs of inputs.\n\nINPUTS:\n- margin (float, optional): The minimum distance threshold between dissimilar pairs. Default value is 2.0.\n\nOUTPUTS:\n- None (initializes the object state)\n\nKEY STEPS:\n- Calls the parent class initializer\n- Stores the provided margin value as an instance attribute\n\nDEPENDENCIES:\n- Parent class (likely a PyTorch nn.Module or similar loss function base class)\n\nUSAGE CONTEXT:\n- Used when instantiating a ContrastiveLoss object before applying it to calculate loss in siamese networks\n- Typically used in deep learning applications involving similarity learning, face recognition, or signature verification\n\nEDGE CASES:\n- No explicit error handling, though very small or negative margin values might lead to poor model performance\n\nRELATIONSHIPS:\n- This is the constructor for the ContrastiveLoss class\n- Works with the forward method (not shown) which would use this margin value to calculate the actual loss\n- Inherits from a parent class (likely nn.Module in PyTorch)"
    }
  },
  {
    "page_content": "# FUNCTION: forward\n\n## PURPOSE:\nCalculates the contrastive loss between two output vectors based on their similarity and a binary label indicating whether they should be similar or dissimilar. This loss function helps neural networks learn to distinguish between similar and dissimilar pairs.\n\n## INPUTS:\n- `output1` (torch.Tensor): First output vector from a neural network\n- `output2` (torch.Tensor): Second output vector from a neural network\n- `label` (torch.Tensor): Binary label (0 = similar pair, 1 = dissimilar pair)\n\n## OUTPUTS:\n- `loss_contrastive` (torch.Tensor): Scalar tensor containing the calculated contrastive loss value\n\n## KEY STEPS:\n- Calculate the Euclidean distance between output1 and output2\n- For similar pairs (label=0): penalize distance proportionally to its squared value\n- For dissimilar pairs (label=1): penalize only if distance is less than margin, using squared difference\n- Return the mean contrastive loss across all pairs in the batch\n\n## DEPENDENCIES:\n- `torch`: PyTorch library for tensor operations\n- `torch.nn.functional` as F: Contains the pairwise_distance function\n\n## USAGE CONTEXT:\nUsed in Siamese networks and other architectures that learn embeddings or representations by comparing pairs of inputs, such as face verification, signature verification, or similar item matching.\n\n## EDGE CASES:\n- If `margin` is not properly set, dissimilar pairs might not be pushed apart far enough\n- Numerical stability may be affected by very large distance values\n- Assumes label values are binary (0 or 1)\n\n## RELATIONSHIPS:\n- Likely part of a loss module extending torch.nn.Module\n- Works with output representations produced by a siamese or related neural network architecture\n- Relies on the margin parameter that must be defined in the class's __init__ method\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2)\n            + (label)\n            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n\n        return loss_contrastive",
    "metadata": {
      "type": "FUNCTION",
      "name": "forward",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.mean(\n            (1 - label) * torch.pow(euclidean_distance, 2)\n            + (label)\n            * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)\n        )\n\n        return loss_contrastive",
      "summary": "# FUNCTION: forward\n\n## PURPOSE:\nCalculates the contrastive loss between two output vectors based on their similarity and a binary label indicating whether they should be similar or dissimilar. This loss function helps neural networks learn to distinguish between similar and dissimilar pairs.\n\n## INPUTS:\n- `output1` (torch.Tensor): First output vector from a neural network\n- `output2` (torch.Tensor): Second output vector from a neural network\n- `label` (torch.Tensor): Binary label (0 = similar pair, 1 = dissimilar pair)\n\n## OUTPUTS:\n- `loss_contrastive` (torch.Tensor): Scalar tensor containing the calculated contrastive loss value\n\n## KEY STEPS:\n- Calculate the Euclidean distance between output1 and output2\n- For similar pairs (label=0): penalize distance proportionally to its squared value\n- For dissimilar pairs (label=1): penalize only if distance is less than margin, using squared difference\n- Return the mean contrastive loss across all pairs in the batch\n\n## DEPENDENCIES:\n- `torch`: PyTorch library for tensor operations\n- `torch.nn.functional` as F: Contains the pairwise_distance function\n\n## USAGE CONTEXT:\nUsed in Siamese networks and other architectures that learn embeddings or representations by comparing pairs of inputs, such as face verification, signature verification, or similar item matching.\n\n## EDGE CASES:\n- If `margin` is not properly set, dissimilar pairs might not be pushed apart far enough\n- Numerical stability may be affected by very large distance values\n- Assumes label values are binary (0 or 1)\n\n## RELATIONSHIPS:\n- Likely part of a loss module extending torch.nn.Module\n- Works with output representations produced by a siamese or related neural network architecture\n- Relies on the margin parameter that must be defined in the class's __init__ method"
    }
  },
  {
    "page_content": "# FUNCTION: train\n\n## PURPOSE:\nTrains a Siamese neural network model using contrastive loss. This function performs the training loop, optimizes the model parameters, and tracks the loss over iterations.\n\n## INPUTS:\nNone (parameters are accessed from outer scope)\n\n## OUTPUTS:\n- `net` (Neural Network): The trained Siamese network model\n\n## KEY STEPS:\n- Initialize tracking arrays for loss values and iteration counters\n- Iterate through a single epoch (range 1-2)\n- Loop through the test dataloader (note: unusual to train on test data)\n- For each batch, extract image pairs and labels\n- Perform forward pass through the network to get output embeddings\n- Calculate contrastive loss between the embeddings\n- Perform backpropagation and update model weights\n- Track and log loss values every 10 iterations\n- Plot the loss curve at the end of training\n\n## DEPENDENCIES:\n- A neural network model (`net`)\n- Optimizer (`optimizer`)\n- Loss function (`criterion`)\n- Data loader (`test_dataloader`)\n- `show_plot` function for visualization\n\n## USAGE CONTEXT:\nUsed during the training phase of a Siamese network, typically called after model and data preparation but before evaluation or inference.\n\n## EDGE CASES:\n- Uses test_dataloader instead of train_dataloader (commented out), which is unconventional\n- Only trains for a single epoch, which may not be sufficient for convergence\n- No validation step is included in the training process\n\n## RELATIONSHIPS:\n- Depends on a pre-defined Siamese network architecture (`net`)\n- Uses a contrastive loss function (`criterion`) specialized for Siamese networks\n- Likely part of a larger pipeline for similarity learning or verification tasks\n\n    def train():\n        loss=[] \n        counter=[]\n        iteration_number = 0\n\n        k = 0\n        for epoch in range(1, 2):\n            for i, data in enumerate(test_dataloader,0):\n                k += 1\n                if k % 10 == 0: print(k)\n            # for i, data in enumerate(train_dataloader,0):\n                img0, img1 , label = data\n                # img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n                optimizer.zero_grad()\n                output1,output2 = net(img0,img1)\n                loss_contrastive = criterion(output1,output2,label)\n                loss_contrastive.backward()\n                optimizer.step()    \n            print(\"Epoch {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number += 10\n            counter.append(iteration_number)\n            loss.append(loss_contrastive.item())\n        show_plot(counter, loss)   \n        return net",
    "metadata": {
      "type": "FUNCTION",
      "name": "train",
      "path": "../mathsearch/ml-model/archive/old-siamese-model/siamese.py",
      "code": "    def train():\n        loss=[] \n        counter=[]\n        iteration_number = 0\n\n        k = 0\n        for epoch in range(1, 2):\n            for i, data in enumerate(test_dataloader,0):\n                k += 1\n                if k % 10 == 0: print(k)\n            # for i, data in enumerate(train_dataloader,0):\n                img0, img1 , label = data\n                # img0, img1 , label = img0.cuda(), img1.cuda() , label.cuda()\n                optimizer.zero_grad()\n                output1,output2 = net(img0,img1)\n                loss_contrastive = criterion(output1,output2,label)\n                loss_contrastive.backward()\n                optimizer.step()    \n            print(\"Epoch {}\\n Current loss {}\\n\".format(epoch,loss_contrastive.item()))\n            iteration_number += 10\n            counter.append(iteration_number)\n            loss.append(loss_contrastive.item())\n        show_plot(counter, loss)   \n        return net",
      "summary": "# FUNCTION: train\n\n## PURPOSE:\nTrains a Siamese neural network model using contrastive loss. This function performs the training loop, optimizes the model parameters, and tracks the loss over iterations.\n\n## INPUTS:\nNone (parameters are accessed from outer scope)\n\n## OUTPUTS:\n- `net` (Neural Network): The trained Siamese network model\n\n## KEY STEPS:\n- Initialize tracking arrays for loss values and iteration counters\n- Iterate through a single epoch (range 1-2)\n- Loop through the test dataloader (note: unusual to train on test data)\n- For each batch, extract image pairs and labels\n- Perform forward pass through the network to get output embeddings\n- Calculate contrastive loss between the embeddings\n- Perform backpropagation and update model weights\n- Track and log loss values every 10 iterations\n- Plot the loss curve at the end of training\n\n## DEPENDENCIES:\n- A neural network model (`net`)\n- Optimizer (`optimizer`)\n- Loss function (`criterion`)\n- Data loader (`test_dataloader`)\n- `show_plot` function for visualization\n\n## USAGE CONTEXT:\nUsed during the training phase of a Siamese network, typically called after model and data preparation but before evaluation or inference.\n\n## EDGE CASES:\n- Uses test_dataloader instead of train_dataloader (commented out), which is unconventional\n- Only trains for a single epoch, which may not be sufficient for convergence\n- No validation step is included in the training process\n\n## RELATIONSHIPS:\n- Depends on a pre-defined Siamese network architecture (`net`)\n- Uses a contrastive loss function (`criterion`) specialized for Siamese networks\n- Likely part of a larger pipeline for similarity learning or verification tasks"
    }
  },
  {
    "page_content": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: prev_dataset\n\n### PURPOSE:\nThis directory contains utility modules for image processing and visualization in a deep learning workflow, focusing on image manipulation techniques such as background removal and tensor visualization capabilities for analysis purposes.\n\n### COMPONENT STRUCTURE:\n- `data_augmentation.py`: Provides visualization functionality for PyTorch tensor images using matplotlib, enabling display of image grids for comparison and analysis.\n- `white_background.py`: Implements image segmentation and background removal utilities to extract foreground objects and place them on white backgrounds.\n\n### ARCHITECTURE:\nThe directory follows a modular design with specialized utilities organized into separate files. Each file contains focused functionality that adheres to single responsibility principles, with clear separation between visualization tools and image processing capabilities.\n\n### ENTRY POINTS:\n- `white_background.py:process_single_image()`: Main function for end-to-end processing of individual images to create white backgrounds.\n- `data_augmentation.py:show()`: Primary function for displaying collections of PyTorch tensor images in a grid format.\n\n### DATA FLOW:\nData typically flows from image loading (using `load_image`), through processing stages (segmentation via `segment_image`, background manipulation with `create_white_background`), to either storage (via `save_processed_image`) or visualization (using the `show()` function for tensor images).\n\n### INTEGRATION:\nThese utilities integrate with PyTorch/torchvision ecosystems and likely connect to larger deep learning workflows. They serve as preprocessing and visualization components that can be called from training scripts, notebooks, or other modules requiring image manipulation or inspection capabilities.\n\n### DEVELOPMENT PATTERNS:\nBoth files employ a functional programming approach with specialized functions for discrete tasks. They follow a pattern of input processing, transformation, and output generation (either saved files or visualizations). Helper functions are used to break down complex operations into manageable, reusable steps.\n\n### RELATIONSHIPS:\nWhile each file functions independently, they appear to be complementary tools in an image processing pipeline. `white_background.py` could be used to prepare and preprocess images, which might then be loaded as tensors and visualized using functionality from `data_augmentation.py` during model development or debugging.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "prev_dataset",
      "path": "../mathsearch/ml-model/archive/prev_dataset",
      "code": "",
      "summary": "# STANDARDIZED NATURAL LANGUAGE SUMMARY\n\n## DIRECTORY: prev_dataset\n\n### PURPOSE:\nThis directory contains utility modules for image processing and visualization in a deep learning workflow, focusing on image manipulation techniques such as background removal and tensor visualization capabilities for analysis purposes.\n\n### COMPONENT STRUCTURE:\n- `data_augmentation.py`: Provides visualization functionality for PyTorch tensor images using matplotlib, enabling display of image grids for comparison and analysis.\n- `white_background.py`: Implements image segmentation and background removal utilities to extract foreground objects and place them on white backgrounds.\n\n### ARCHITECTURE:\nThe directory follows a modular design with specialized utilities organized into separate files. Each file contains focused functionality that adheres to single responsibility principles, with clear separation between visualization tools and image processing capabilities.\n\n### ENTRY POINTS:\n- `white_background.py:process_single_image()`: Main function for end-to-end processing of individual images to create white backgrounds.\n- `data_augmentation.py:show()`: Primary function for displaying collections of PyTorch tensor images in a grid format.\n\n### DATA FLOW:\nData typically flows from image loading (using `load_image`), through processing stages (segmentation via `segment_image`, background manipulation with `create_white_background`), to either storage (via `save_processed_image`) or visualization (using the `show()` function for tensor images).\n\n### INTEGRATION:\nThese utilities integrate with PyTorch/torchvision ecosystems and likely connect to larger deep learning workflows. They serve as preprocessing and visualization components that can be called from training scripts, notebooks, or other modules requiring image manipulation or inspection capabilities.\n\n### DEVELOPMENT PATTERNS:\nBoth files employ a functional programming approach with specialized functions for discrete tasks. They follow a pattern of input processing, transformation, and output generation (either saved files or visualizations). Helper functions are used to break down complex operations into manageable, reusable steps.\n\n### RELATIONSHIPS:\nWhile each file functions independently, they appear to be complementary tools in an image processing pipeline. `white_background.py` could be used to prepare and preprocess images, which might then be loaded as tensors and visualized using functionality from `data_augmentation.py` during model development or debugging."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/archive/prev_dataset/target",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: data_augmentation.py\n\n## OVERVIEW:\nA utility file providing functions for visualizing PyTorch tensor images using matplotlib, enabling developers to display image collections in a horizontal grid for comparison or analysis during deep learning workflows.\n\n## KEY COMPONENTS:\n- `show()`: Displays a collection of PyTorch tensor images in a horizontal grid for visual comparison and analysis\n\n## ARCHITECTURE:\nThe file contains a single visualization utility function that serves as a helper tool within a larger deep learning or computer vision pipeline.\n\n## DATA FLOW:\nInput tensor images are processed through conversion steps (tensor \u2192 PIL Image \u2192 numpy array) before being displayed in a matplotlib figure with multiple subplots arranged horizontally.\n\n## INTEGRATION POINTS:\n- Integrates with PyTorch/torchvision data processing pipelines\n- Connects with deep learning training and debugging workflows\n- Functions as a visualization layer in computer vision applications\n\n## USAGE PATTERNS:\n- Used during model training or debugging to inspect input data, intermediate representations, or outputs\n- Commonly employed in Jupyter notebooks or interactive development environments for quick visual analysis\n- Applied when comparing original images with transformed or processed versions\n\n## DEPENDENCIES:\n- matplotlib.pyplot: For creating and managing plots\n- torchvision.transforms: For the ToPILImage transformation\n- numpy: For array manipulation\n- PyTorch: For tensor operations\n\n## RELATIONSHIPS:\nThe function works independently as a utility tool but complements other visualization and data processing components in the PyTorch/torchvision ecosystem, supporting the visualization needs of a larger deep learning pipeline.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "data_augmentation.py",
      "path": "../mathsearch/ml-model/archive/prev_dataset/data_augmentation.py",
      "code": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\n\n\nplt.rcParams[\"savefig.bbox\"] = 'tight'\ntorch.manual_seed(1)\n\n\ndef show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n\n\ntransforms = torch.nn.Sequential(\n    transforms.CenterCrop(10),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n)\nscripted_transforms = torch.jit.script(transforms)",
      "summary": "# FILE: data_augmentation.py\n\n## OVERVIEW:\nA utility file providing functions for visualizing PyTorch tensor images using matplotlib, enabling developers to display image collections in a horizontal grid for comparison or analysis during deep learning workflows.\n\n## KEY COMPONENTS:\n- `show()`: Displays a collection of PyTorch tensor images in a horizontal grid for visual comparison and analysis\n\n## ARCHITECTURE:\nThe file contains a single visualization utility function that serves as a helper tool within a larger deep learning or computer vision pipeline.\n\n## DATA FLOW:\nInput tensor images are processed through conversion steps (tensor \u2192 PIL Image \u2192 numpy array) before being displayed in a matplotlib figure with multiple subplots arranged horizontally.\n\n## INTEGRATION POINTS:\n- Integrates with PyTorch/torchvision data processing pipelines\n- Connects with deep learning training and debugging workflows\n- Functions as a visualization layer in computer vision applications\n\n## USAGE PATTERNS:\n- Used during model training or debugging to inspect input data, intermediate representations, or outputs\n- Commonly employed in Jupyter notebooks or interactive development environments for quick visual analysis\n- Applied when comparing original images with transformed or processed versions\n\n## DEPENDENCIES:\n- matplotlib.pyplot: For creating and managing plots\n- torchvision.transforms: For the ToPILImage transformation\n- numpy: For array manipulation\n- PyTorch: For tensor operations\n\n## RELATIONSHIPS:\nThe function works independently as a utility tool but complements other visualization and data processing components in the PyTorch/torchvision ecosystem, supporting the visualization needs of a larger deep learning pipeline."
    }
  },
  {
    "page_content": "# FUNCTION: show\n\n## PURPOSE:\nDisplays a collection of PyTorch tensor images in a horizontal grid using matplotlib. This function simplifies the visualization of multiple images at once for comparison or analysis.\n\n## INPUTS:\n- `imgs`: A list or collection of PyTorch tensor images to be displayed\n\n## OUTPUTS:\n- No return value; displays images in a matplotlib figure\n\n## KEY STEPS:\n- Creates a matplotlib figure with subplots based on the number of input images\n- Iterates through each image tensor in the input collection\n- Converts each tensor to a PIL Image and then to a numpy array\n- Displays each image in its respective subplot\n- Removes axis labels and ticks for cleaner visualization\n\n## DEPENDENCIES:\n- `plt` (matplotlib.pyplot): For creating and managing plots\n- `T` (torchvision.transforms): For the ToPILImage transformation\n- `np` (numpy): For array manipulation\n- PyTorch: For tensor operations\n\n## USAGE CONTEXT:\n- Used in deep learning workflows, particularly computer vision applications\n- Helpful during model training/debugging to visualize input data, intermediate representations, or outputs\n- Commonly used in notebooks or interactive development environments to inspect images\n\n## EDGE CASES:\n- Assumes input tensors are valid image tensors that can be converted to PIL Images\n- No explicit error handling for invalid inputs or conversion failures\n\n## RELATIONSHIPS:\n- Works as a utility function in a larger image processing or deep learning pipeline\n- Complements other visualization tools in the PyTorch/torchvision ecosystem\n\ndef show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])",
    "metadata": {
      "type": "FUNCTION",
      "name": "show",
      "path": "../mathsearch/ml-model/archive/prev_dataset/data_augmentation.py",
      "code": "def show(imgs):\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = T.ToPILImage()(img.to('cpu'))\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])",
      "summary": "# FUNCTION: show\n\n## PURPOSE:\nDisplays a collection of PyTorch tensor images in a horizontal grid using matplotlib. This function simplifies the visualization of multiple images at once for comparison or analysis.\n\n## INPUTS:\n- `imgs`: A list or collection of PyTorch tensor images to be displayed\n\n## OUTPUTS:\n- No return value; displays images in a matplotlib figure\n\n## KEY STEPS:\n- Creates a matplotlib figure with subplots based on the number of input images\n- Iterates through each image tensor in the input collection\n- Converts each tensor to a PIL Image and then to a numpy array\n- Displays each image in its respective subplot\n- Removes axis labels and ticks for cleaner visualization\n\n## DEPENDENCIES:\n- `plt` (matplotlib.pyplot): For creating and managing plots\n- `T` (torchvision.transforms): For the ToPILImage transformation\n- `np` (numpy): For array manipulation\n- PyTorch: For tensor operations\n\n## USAGE CONTEXT:\n- Used in deep learning workflows, particularly computer vision applications\n- Helpful during model training/debugging to visualize input data, intermediate representations, or outputs\n- Commonly used in notebooks or interactive development environments to inspect images\n\n## EDGE CASES:\n- Assumes input tensors are valid image tensors that can be converted to PIL Images\n- No explicit error handling for invalid inputs or conversion failures\n\n## RELATIONSHIPS:\n- Works as a utility function in a larger image processing or deep learning pipeline\n- Complements other visualization tools in the PyTorch/torchvision ecosystem"
    }
  },
  {
    "page_content": "# FILE: white_background.py\n\n## OVERVIEW:\nA utility file for processing image data, specifically designed to create white backgrounds for images by segmenting foreground objects from their original backgrounds.\n\n## KEY COMPONENTS:\n- `process_single_image`: Processes individual images to create a white background by extracting foreground subjects.\n- `create_white_background`: Main function that applies white background processing to images using segmentation.\n- `segment_image`: Segments an image to separate foreground from background using computer vision techniques.\n- `save_processed_image`: Saves processed images to disk with appropriate formatting and naming.\n- `load_image`: Loads image data from files for processing.\n\n## ARCHITECTURE:\nThe file follows a modular design with specialized functions for each step of the image processing pipeline: loading, segmentation, background removal, and saving.\n\n## DATA FLOW:\n1. Images are loaded from source files via `load_image`\n2. The loaded images are segmented to identify foreground objects with `segment_image`\n3. The `create_white_background` function applies white background to the segmented foreground\n4. Individual images are processed end-to-end through `process_single_image`\n5. Resulting images are saved to disk using `save_processed_image`\n\n## INTEGRATION POINTS:\n- Likely interfaces with image processing libraries for segmentation algorithms\n- Provides functions callable from other modules that need background removal capabilities\n- May integrate with batch processing systems for handling multiple images\n\n## USAGE PATTERNS:\n- Processing single images directly through `process_single_image`\n- Batch processing multiple images by calling the core functions in sequence\n- Using the segmentation functionality independently for other image analysis tasks\n\n## DEPENDENCIES:\n- Image processing libraries (likely OpenCV, PIL, or similar)\n- File I/O modules for reading and writing image data\n- Potentially machine learning models for advanced image segmentation\n\n## RELATIONSHIPS:\nThe functions form a clear processing pipeline with `process_single_image` orchestrating the overall flow while specialized functions handle specific tasks. The segmentation function powers the core capability, while helper functions manage data loading and output.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "white_background.py",
      "path": "../mathsearch/ml-model/archive/prev_dataset/white_background.py",
      "code": "from PIL import Image\n\nimage = Image.open('formula_images/1a0a0dfbac.png')\nnew_image = Image.new(\"RGBA\", image.size, \"WHITE\") # Create a white rgba background\nnew_image.paste(image, (0, 0), image)              # Paste the image on the background. Go to the links given below for details.\nnew_image.convert('RGB').save('test.jpg', \"JPEG\")",
      "summary": "# FILE: white_background.py\n\n## OVERVIEW:\nA utility file for processing image data, specifically designed to create white backgrounds for images by segmenting foreground objects from their original backgrounds.\n\n## KEY COMPONENTS:\n- `process_single_image`: Processes individual images to create a white background by extracting foreground subjects.\n- `create_white_background`: Main function that applies white background processing to images using segmentation.\n- `segment_image`: Segments an image to separate foreground from background using computer vision techniques.\n- `save_processed_image`: Saves processed images to disk with appropriate formatting and naming.\n- `load_image`: Loads image data from files for processing.\n\n## ARCHITECTURE:\nThe file follows a modular design with specialized functions for each step of the image processing pipeline: loading, segmentation, background removal, and saving.\n\n## DATA FLOW:\n1. Images are loaded from source files via `load_image`\n2. The loaded images are segmented to identify foreground objects with `segment_image`\n3. The `create_white_background` function applies white background to the segmented foreground\n4. Individual images are processed end-to-end through `process_single_image`\n5. Resulting images are saved to disk using `save_processed_image`\n\n## INTEGRATION POINTS:\n- Likely interfaces with image processing libraries for segmentation algorithms\n- Provides functions callable from other modules that need background removal capabilities\n- May integrate with batch processing systems for handling multiple images\n\n## USAGE PATTERNS:\n- Processing single images directly through `process_single_image`\n- Batch processing multiple images by calling the core functions in sequence\n- Using the segmentation functionality independently for other image analysis tasks\n\n## DEPENDENCIES:\n- Image processing libraries (likely OpenCV, PIL, or similar)\n- File I/O modules for reading and writing image data\n- Potentially machine learning models for advanced image segmentation\n\n## RELATIONSHIPS:\nThe functions form a clear processing pipeline with `process_single_image` orchestrating the overall flow while specialized functions handle specific tasks. The segmentation function powers the core capability, while helper functions manage data loading and output."
    }
  },
  {
    "page_content": "# DIRECTORY: app_sample\n\n## PURPOSE:\nThis directory contains a collection of Flask-based web application files focusing on file upload functionality with security validation and file system operations. It appears to be a sample or demonstration application showcasing secure file handling patterns.\n\n## COMPONENT STRUCTURE:\n- `app_main.py`: Core Flask application implementing secure file upload functionality with validation\n- `app_checkpoint.py`: Similar to app_main.py but with additional form rendering and redirect functionality\n- `app_python.py`: Utility module providing directory cleanup functionality through the `remove_files()` function\n- `app_example.py`: Minimal demonstration file with a simple `hello_world()` function, likely for testing purposes\n\n## ARCHITECTURE:\nThe directory follows a Flask web application architecture with separated concerns: route handlers for different endpoints, file validation logic, and file system utilities. Security validation is prioritized through the consistent use of `allowed_file()` functions and Werkzeug's `secure_filename()` utility across multiple files.\n\n## ENTRY POINTS:\n- `app_main.py` and `app_checkpoint.py` serve as primary entry points with their route handlers\n- The `upload_file()` and `upload_form()` functions in these files handle the main file upload workflow\n- `hello_world()` functions appear in multiple files, likely serving as health check endpoints\n\n## DATA FLOW:\n1. Users access upload forms rendered by `upload_form()` endpoints\n2. Files are submitted via POST requests to `upload_file()` handlers\n3. Submissions pass through security validation with `allowed_file()`\n4. Valid files are processed with `secure_filename()` and saved to configured storage\n5. Users receive feedback through redirects and flash messages\n6. Directory cleanup may be performed via `remove_files()` when needed\n\n## INTEGRATION:\nThe files integrate with:\n- Flask's web framework for request handling and responses\n- File system operations through standard OS modules\n- Werkzeug security utilities for filename sanitization\n- HTML templates for rendering the upload interface\n- Configuration settings for upload folders and allowed file extensions\n\n## DEVELOPMENT PATTERNS:\n- Security-first approach with consistent file validation before processing\n- Separation of presentation logic from processing logic\n- RESTful endpoint design with appropriate HTTP methods\n- Feedback mechanisms through flash messages and redirects\n- Configuration-driven approach with settings for upload locations and allowed extensions\n\n## RELATIONSHIPS:\nThe files share common concepts and patterns while serving different aspects of file management. `app_main.py` and `app_checkpoint.py` provide similar but slightly different implementations of file upload functionality, while `app_python.py` offers supporting utility functions for file system operations. `app_example.py` stands alone as a simple demonstration file, possibly for testing the deployment environment.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "app_sample",
      "path": "../mathsearch/ml-model/archive/app_sample",
      "code": "",
      "summary": "# DIRECTORY: app_sample\n\n## PURPOSE:\nThis directory contains a collection of Flask-based web application files focusing on file upload functionality with security validation and file system operations. It appears to be a sample or demonstration application showcasing secure file handling patterns.\n\n## COMPONENT STRUCTURE:\n- `app_main.py`: Core Flask application implementing secure file upload functionality with validation\n- `app_checkpoint.py`: Similar to app_main.py but with additional form rendering and redirect functionality\n- `app_python.py`: Utility module providing directory cleanup functionality through the `remove_files()` function\n- `app_example.py`: Minimal demonstration file with a simple `hello_world()` function, likely for testing purposes\n\n## ARCHITECTURE:\nThe directory follows a Flask web application architecture with separated concerns: route handlers for different endpoints, file validation logic, and file system utilities. Security validation is prioritized through the consistent use of `allowed_file()` functions and Werkzeug's `secure_filename()` utility across multiple files.\n\n## ENTRY POINTS:\n- `app_main.py` and `app_checkpoint.py` serve as primary entry points with their route handlers\n- The `upload_file()` and `upload_form()` functions in these files handle the main file upload workflow\n- `hello_world()` functions appear in multiple files, likely serving as health check endpoints\n\n## DATA FLOW:\n1. Users access upload forms rendered by `upload_form()` endpoints\n2. Files are submitted via POST requests to `upload_file()` handlers\n3. Submissions pass through security validation with `allowed_file()`\n4. Valid files are processed with `secure_filename()` and saved to configured storage\n5. Users receive feedback through redirects and flash messages\n6. Directory cleanup may be performed via `remove_files()` when needed\n\n## INTEGRATION:\nThe files integrate with:\n- Flask's web framework for request handling and responses\n- File system operations through standard OS modules\n- Werkzeug security utilities for filename sanitization\n- HTML templates for rendering the upload interface\n- Configuration settings for upload folders and allowed file extensions\n\n## DEVELOPMENT PATTERNS:\n- Security-first approach with consistent file validation before processing\n- Separation of presentation logic from processing logic\n- RESTful endpoint design with appropriate HTTP methods\n- Feedback mechanisms through flash messages and redirects\n- Configuration-driven approach with settings for upload locations and allowed extensions\n\n## RELATIONSHIPS:\nThe files share common concepts and patterns while serving different aspects of file management. `app_main.py` and `app_checkpoint.py` provide similar but slightly different implementations of file upload functionality, while `app_python.py` offers supporting utility functions for file system operations. `app_example.py` stands alone as a simple demonstration file, possibly for testing the deployment environment."
    }
  },
  {
    "page_content": "# FILE: app_python.py\n\n## OVERVIEW:\nThis file primarily serves as a utility module for file management operations, specifically focusing on directory cleanup functionality.\n\n## KEY COMPONENTS:\n- `remove_files(data_dir)`: Deletes all files within a specified directory without removing the directory itself.\n\n## ARCHITECTURE:\nThe file contains a single utility function that performs basic file system operations, suggesting it may be part of a larger file management or data processing module.\n\n## DATA FLOW:\nData flow is straightforward - the function receives a directory path, identifies all files within that directory, and removes them one by one without returning any data.\n\n## INTEGRATION POINTS:\n- Integrates with the file system through the OS module\n- Likely called by other modules that need to perform cleanup operations\n\n## USAGE PATTERNS:\n- Clearing cache directories\n- Removing temporary files\n- Resetting a working directory to a clean state before processing\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functions\n- `os.listdir()`: For listing directory contents\n- `os.path.join()`: For constructing file paths\n- `os.remove()`: For deleting files\n\n## RELATIONSHIPS:\nThis utility function appears to be a standalone helper that supports larger data processing or file management operations within the system, with no direct relationships to other functions in the file.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_python.py",
      "path": "../mathsearch/ml-model/archive/app_sample/app_python.py",
      "code": "import sys\nimport pandas\nimport os\nsys.path.append('/home/ubuntu/yolov5')\nimport main\n\n\ndef remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))\n\nif __name__ == \"__main__\":\n\n\t# clear data dir\n\tdata_dir = '/home/ubuntu/yolov5/input_data'\n\t# remove_files(data_dir)\n\n\t# run flask to download files\n    # app.run()\n\n\ttarget_search = \"target_search.png\"\n\ttarget_file = \"/home/ubuntu/yolov5/input_data/sample_doc.pdf\"\n\n\tos.chdir('/home/ubuntu/yolov5')\n\tmain.main(target_search)\n\n\t# return stuff",
      "summary": "# FILE: app_python.py\n\n## OVERVIEW:\nThis file primarily serves as a utility module for file management operations, specifically focusing on directory cleanup functionality.\n\n## KEY COMPONENTS:\n- `remove_files(data_dir)`: Deletes all files within a specified directory without removing the directory itself.\n\n## ARCHITECTURE:\nThe file contains a single utility function that performs basic file system operations, suggesting it may be part of a larger file management or data processing module.\n\n## DATA FLOW:\nData flow is straightforward - the function receives a directory path, identifies all files within that directory, and removes them one by one without returning any data.\n\n## INTEGRATION POINTS:\n- Integrates with the file system through the OS module\n- Likely called by other modules that need to perform cleanup operations\n\n## USAGE PATTERNS:\n- Clearing cache directories\n- Removing temporary files\n- Resetting a working directory to a clean state before processing\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functions\n- `os.listdir()`: For listing directory contents\n- `os.path.join()`: For constructing file paths\n- `os.remove()`: For deleting files\n\n## RELATIONSHIPS:\nThis utility function appears to be a standalone helper that supports larger data processing or file management operations within the system, with no direct relationships to other functions in the file."
    }
  },
  {
    "page_content": "# FUNCTION: remove_files\n\n## PURPOSE:\nDeletes all files within a specified directory. This utility function provides a quick way to clear a directory's contents without removing the directory itself.\n\n## INPUTS:\n- `data_dir` (string): Path to the directory containing files to be deleted\n\n## OUTPUTS:\n- None\n\n## KEY STEPS:\n- List all files in the specified directory using `os.listdir()`\n- Iterate through each file in the directory\n- Delete each file using `os.remove()` with the full file path\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functions\n- `os.listdir()`: Function to list directory contents\n- `os.path.join()`: Function to construct proper file paths\n- `os.remove()`: Function to delete files\n\n## USAGE CONTEXT:\nTypically used for cleanup operations such as clearing cache directories, removing temporary files, or resetting a working directory to a clean state before processing.\n\n## EDGE CASES:\n- Does not handle subdirectories (will raise an error if attempting to remove a directory)\n- No error handling for permission issues or non-existent files\n- Will fail if the directory doesn't exist or if user lacks proper permissions\n\n## RELATIONSHIPS:\nThis is likely a utility function that supports larger data processing or file management operations within the system.\n\ndef remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))",
    "metadata": {
      "type": "FUNCTION",
      "name": "remove_files",
      "path": "../mathsearch/ml-model/archive/app_sample/app_python.py",
      "code": "def remove_files(data_dir):\n\tfor f in os.listdir(data_dir):\n\t\tos.remove(os.path.join(data_dir, f))",
      "summary": "# FUNCTION: remove_files\n\n## PURPOSE:\nDeletes all files within a specified directory. This utility function provides a quick way to clear a directory's contents without removing the directory itself.\n\n## INPUTS:\n- `data_dir` (string): Path to the directory containing files to be deleted\n\n## OUTPUTS:\n- None\n\n## KEY STEPS:\n- List all files in the specified directory using `os.listdir()`\n- Iterate through each file in the directory\n- Delete each file using `os.remove()` with the full file path\n\n## DEPENDENCIES:\n- `os`: Python standard library module for operating system functions\n- `os.listdir()`: Function to list directory contents\n- `os.path.join()`: Function to construct proper file paths\n- `os.remove()`: Function to delete files\n\n## USAGE CONTEXT:\nTypically used for cleanup operations such as clearing cache directories, removing temporary files, or resetting a working directory to a clean state before processing.\n\n## EDGE CASES:\n- Does not handle subdirectories (will raise an error if attempting to remove a directory)\n- No error handling for permission issues or non-existent files\n- Will fail if the directory doesn't exist or if user lacks proper permissions\n\n## RELATIONSHIPS:\nThis is likely a utility function that supports larger data processing or file management operations within the system."
    }
  },
  {
    "page_content": "# FILE: app_main.py\n\n## OVERVIEW:\nApp_main.py is a Flask-based web application that handles file uploads, providing functionality to securely process and validate files before storing them on the server.\n\n## KEY COMPONENTS:\n- `allowed_file(filename)`: Validates if a file has an approved extension\n- `hello_world()`: Returns a simple greeting message, likely for testing/health checks\n- `upload_file()`: Processes file uploads, validates files, and securely stores them\n\n## ARCHITECTURE:\nThe file implements a standard web application pattern with route handlers for different endpoints. It separates concerns between file validation, request handling, and response generation.\n\n## DATA FLOW:\n1. User submits a file through a web form to the upload_file handler\n2. The file is validated using allowed_file() to ensure it has a permitted extension\n3. If valid, the file is processed with secure_filename() and saved to the server\n4. The user is redirected to a download_file route to access the uploaded file\n\n## INTEGRATION POINTS:\n- Connects to a file download route/handler (download_file)\n- Relies on Flask's request handling and response mechanisms\n- Integrates with a file security system through secure_filename()\n\n## USAGE PATTERNS:\n- Users access a file upload form through a GET request\n- Files are submitted through POST requests and validated\n- Invalid submissions redirect users back to the form with error messages\n- Successful uploads redirect users to access their file\n\n## DEPENDENCIES:\n- Flask framework (request, flash, redirect, url_for)\n- Werkzeug utilities (secure_filename)\n- Global ALLOWED_EXTENSIONS configuration \n- Application configuration for UPLOAD_FOLDER\n\n## RELATIONSHIPS:\nThe functions work together in a validation-processing pipeline. The allowed_file function acts as a gatekeeper for the upload_file handler, which manages the complete file submission workflow. The hello_world function appears to be an independent testing endpoint, possibly for service health monitoring.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_main.py",
      "path": "../mathsearch/ml-model/archive/app_sample/app_main.py",
      "code": "from flask import Flask, flash, redirect, url_for, request, render_template\nfrom werkzeug.utils import secure_filename\nimport os\n\n\"\"\"\n@Author: Emerald Liu\nDoes not support concurrency currently\n\"\"\"\n\n# constant variables\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\nALLOWED_EXTENSIONS = {'pdf'}\n\n# helper functions\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n# initalize flask app config\napp = Flask(__name__)\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n\n# @app.route('/upload')\n# def upload_file():\n#    return render_template('upload.html')\n\n@app.route('/')\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/upload', methods=['GET', 'POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''\n\n\nif __name__ == '__main__':\n    app.debug = True\n    app.run(host='0.0.0.0', port=3000)\n",
      "summary": "# FILE: app_main.py\n\n## OVERVIEW:\nApp_main.py is a Flask-based web application that handles file uploads, providing functionality to securely process and validate files before storing them on the server.\n\n## KEY COMPONENTS:\n- `allowed_file(filename)`: Validates if a file has an approved extension\n- `hello_world()`: Returns a simple greeting message, likely for testing/health checks\n- `upload_file()`: Processes file uploads, validates files, and securely stores them\n\n## ARCHITECTURE:\nThe file implements a standard web application pattern with route handlers for different endpoints. It separates concerns between file validation, request handling, and response generation.\n\n## DATA FLOW:\n1. User submits a file through a web form to the upload_file handler\n2. The file is validated using allowed_file() to ensure it has a permitted extension\n3. If valid, the file is processed with secure_filename() and saved to the server\n4. The user is redirected to a download_file route to access the uploaded file\n\n## INTEGRATION POINTS:\n- Connects to a file download route/handler (download_file)\n- Relies on Flask's request handling and response mechanisms\n- Integrates with a file security system through secure_filename()\n\n## USAGE PATTERNS:\n- Users access a file upload form through a GET request\n- Files are submitted through POST requests and validated\n- Invalid submissions redirect users back to the form with error messages\n- Successful uploads redirect users to access their file\n\n## DEPENDENCIES:\n- Flask framework (request, flash, redirect, url_for)\n- Werkzeug utilities (secure_filename)\n- Global ALLOWED_EXTENSIONS configuration \n- Application configuration for UPLOAD_FOLDER\n\n## RELATIONSHIPS:\nThe functions work together in a validation-processing pipeline. The allowed_file function acts as a gatekeeper for the upload_file handler, which manages the complete file submission workflow. The hello_world function appears to be an independent testing endpoint, possibly for service health monitoring."
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a filename has an allowed file extension. This function is used to ensure only files with approved extensions can be processed or uploaded.\n\n## INPUTS:\n- `filename` (string): The name of the file to check, including its extension\n\n## OUTPUTS:\n- Boolean: Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (indicating it has an extension)\n- Extract the extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the lowercase extension exists in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A collection of permitted file extensions defined elsewhere in the code\n\n## USAGE CONTEXT:\n- Typically used in file upload handlers to validate files before processing\n- Often part of web applications that accept file uploads from users\n\n## EDGE CASES:\n- Returns False for filenames without a period\n- Filename extensions are case-insensitive (converted to lowercase)\n- If a filename has multiple periods, only the portion after the last period is considered the extension\n\n## RELATIONSHIPS:\n- Works in conjunction with file upload routes or handlers\n- Relies on the ALLOWED_EXTENSIONS constant which needs to be defined in the outer scope\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/app_sample/app_main.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and \\\n        filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a filename has an allowed file extension. This function is used to ensure only files with approved extensions can be processed or uploaded.\n\n## INPUTS:\n- `filename` (string): The name of the file to check, including its extension\n\n## OUTPUTS:\n- Boolean: Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (indicating it has an extension)\n- Extract the extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Check if the lowercase extension exists in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A collection of permitted file extensions defined elsewhere in the code\n\n## USAGE CONTEXT:\n- Typically used in file upload handlers to validate files before processing\n- Often part of web applications that accept file uploads from users\n\n## EDGE CASES:\n- Returns False for filenames without a period\n- Filename extensions are case-insensitive (converted to lowercase)\n- If a filename has multiple periods, only the portion after the last period is considered the extension\n\n## RELATIONSHIPS:\n- Works in conjunction with file upload routes or handlers\n- Relies on the ALLOWED_EXTENSIONS constant which needs to be defined in the outer scope"
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nA simple test function that returns a greeting message. It's likely used for testing connectivity, deployment verification, or as a placeholder.\n\n## INPUTS:\nNone - this function doesn't accept any parameters.\n\n## OUTPUTS:\nString - Returns the greeting message \"Hello World! - emerald@mathsearch port:3000 temp:1\".\n\n## KEY STEPS:\n- Return a hardcoded string containing a greeting and some configuration details\n\n## DEPENDENCIES:\nNone - this function is self-contained with no external dependencies.\n\n## USAGE CONTEXT:\nTypically used in testing scenarios, health checks, or as a minimal example/placeholder function during development. The included configuration information suggests it may be part of a service on port 3000.\n\n## EDGE CASES:\nNone - as a simple string return function with no parameters, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nThis appears to be a standalone utility function, possibly part of a service identified as \"emerald@mathsearch\". The port and temperature parameters in the string suggest it may be related to a web service or API endpoint configuration.\n\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/app_sample/app_main.py",
      "code": "def hello_world():\n    return 'Hello World! - emerald@mathsearch port:3000 temp:1'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nA simple test function that returns a greeting message. It's likely used for testing connectivity, deployment verification, or as a placeholder.\n\n## INPUTS:\nNone - this function doesn't accept any parameters.\n\n## OUTPUTS:\nString - Returns the greeting message \"Hello World! - emerald@mathsearch port:3000 temp:1\".\n\n## KEY STEPS:\n- Return a hardcoded string containing a greeting and some configuration details\n\n## DEPENDENCIES:\nNone - this function is self-contained with no external dependencies.\n\n## USAGE CONTEXT:\nTypically used in testing scenarios, health checks, or as a minimal example/placeholder function during development. The included configuration information suggests it may be part of a service on port 3000.\n\n## EDGE CASES:\nNone - as a simple string return function with no parameters, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nThis appears to be a standalone utility function, possibly part of a service identified as \"emerald@mathsearch\". The port and temperature parameters in the string suggest it may be related to a web service or API endpoint configuration."
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has an allowed file extension. This function exists to ensure only permitted file types can be processed by the application.\n\n## INPUTS:\n- `filename` (string): The name of the file to validate, including its extension\n\n## OUTPUTS:\n- Boolean: Returns True if the file extension is allowed, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period character (indicating it has an extension)\n- Split the filename at the last period to extract the extension\n- Convert the extension to lowercase\n- Check if the lowercase extension is in the predefined ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A global constant containing permitted file extensions\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate user-submitted files before processing or storing them. Often integrated in web applications' file validation workflow.\n\n## EDGE CASES:\n- Returns False for filenames without extensions (no period)\n- Returns False for filenames with unknown extensions\n- Multiple periods in filename are handled correctly by using rsplit to split from the right\n\n## RELATIONSHIPS:\n- Relies on the global ALLOWED_EXTENSIONS constant which must be defined elsewhere in the system\n- Likely used by file upload route handlers or form validation functions\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/app_sample/app_main.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates whether a given filename has an allowed file extension. This function exists to ensure only permitted file types can be processed by the application.\n\n## INPUTS:\n- `filename` (string): The name of the file to validate, including its extension\n\n## OUTPUTS:\n- Boolean: Returns True if the file extension is allowed, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period character (indicating it has an extension)\n- Split the filename at the last period to extract the extension\n- Convert the extension to lowercase\n- Check if the lowercase extension is in the predefined ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A global constant containing permitted file extensions\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate user-submitted files before processing or storing them. Often integrated in web applications' file validation workflow.\n\n## EDGE CASES:\n- Returns False for filenames without extensions (no period)\n- Returns False for filenames with unknown extensions\n- Multiple periods in filename are handled correctly by using rsplit to split from the right\n\n## RELATIONSHIPS:\n- Relies on the global ALLOWED_EXTENSIONS constant which must be defined elsewhere in the system\n- Likely used by file upload route handlers or form validation functions"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploading in a web application, allowing users to submit files via a form and storing them securely on the server.\n\n## INPUTS:\n- No explicit parameters, but implicitly uses:\n  - `request.files['file']`: The file object from the form submission\n  - `request.method`: HTTP method used (checks for POST)\n\n## OUTPUTS:\n- On successful upload: Redirects to 'download_file' route with filename\n- On error/GET request: Returns HTML form for file upload\n- On validation failure: Redirects back to upload form with flash message\n\n## KEY STEPS:\n- Checks if the request method is POST\n- Validates that the 'file' part exists in the request\n- Ensures a file was actually selected (filename is not empty)\n- Verifies file type is allowed using `allowed_file()` function\n- Secures the filename using `secure_filename()`\n- Saves the file to the configured upload folder\n- Redirects to the download route for the uploaded file\n\n## DEPENDENCIES:\n- Flask modules: `request`, `flash`, `redirect`, `url_for`\n- External functions: `allowed_file()`, `secure_filename()`\n- Application instance: `app` with config['UPLOAD_FOLDER']\n\n## USAGE CONTEXT:\nTypically used as a Flask route handler that presents an upload form on GET requests and processes file submissions on POST requests.\n\n## EDGE CASES:\n- Missing file part in request: Shows flash message and redirects\n- Empty filename (no file selected): Shows flash message and redirects\n- File with disallowed type: Silently rejects (no explicit handling shown)\n\n## RELATIONSHIPS:\n- Works with `download_file` route which presumably serves the uploaded file\n- Relies on application configuration for upload directory\n- Uses `allowed_file()` for validation of acceptable file types\n\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/app_sample/app_main.py",
      "code": "def upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        # If the user does not select a file, the browser submits an\n        # empty file without a filename.\n        if file.filename == '':\n            flash('No selected file')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            file.save(app.config['UPLOAD_FOLDER'], filename)\n            return redirect(url_for('download_file', name=filename))\n    return '''\n    <!doctype html>\n    <title>Upload new File</title>\n    <h1>Upload new File</h1>\n    <form method=post enctype=multipart/form-data>\n      <input type=file name=file>\n      <input type=submit value=Upload>\n    </form>\n    '''",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploading in a web application, allowing users to submit files via a form and storing them securely on the server.\n\n## INPUTS:\n- No explicit parameters, but implicitly uses:\n  - `request.files['file']`: The file object from the form submission\n  - `request.method`: HTTP method used (checks for POST)\n\n## OUTPUTS:\n- On successful upload: Redirects to 'download_file' route with filename\n- On error/GET request: Returns HTML form for file upload\n- On validation failure: Redirects back to upload form with flash message\n\n## KEY STEPS:\n- Checks if the request method is POST\n- Validates that the 'file' part exists in the request\n- Ensures a file was actually selected (filename is not empty)\n- Verifies file type is allowed using `allowed_file()` function\n- Secures the filename using `secure_filename()`\n- Saves the file to the configured upload folder\n- Redirects to the download route for the uploaded file\n\n## DEPENDENCIES:\n- Flask modules: `request`, `flash`, `redirect`, `url_for`\n- External functions: `allowed_file()`, `secure_filename()`\n- Application instance: `app` with config['UPLOAD_FOLDER']\n\n## USAGE CONTEXT:\nTypically used as a Flask route handler that presents an upload form on GET requests and processes file submissions on POST requests.\n\n## EDGE CASES:\n- Missing file part in request: Shows flash message and redirects\n- Empty filename (no file selected): Shows flash message and redirects\n- File with disallowed type: Silently rejects (no explicit handling shown)\n\n## RELATIONSHIPS:\n- Works with `download_file` route which presumably serves the uploaded file\n- Relies on application configuration for upload directory\n- Uses `allowed_file()` for validation of acceptable file types"
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/archive/app_sample/templates",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a web-based file upload system with security validation, providing endpoints for displaying upload forms and processing file submissions while ensuring only authorized file types are accepted.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message directing users to a specific upload URL\n- `download()`: Placeholder function for downloading files (currently commented out/non-functional)\n- `allowed_file(filename)`: Validates if a given filename has an acceptable file extension\n- `upload_form()`: Renders the HTML template that displays the file upload interface\n- `upload_file()`: Processes file uploads, validates files, and saves them to the configured storage location\n\n## ARCHITECTURE:\nThe file follows a typical Flask web application pattern with route handlers for different endpoints. It separates presentation (form rendering) from file processing logic and implements validation checks before file storage operations.\n\n## DATA FLOW:\n1. Users are directed to the upload interface via `upload_form()`\n2. When users submit files, `upload_file()` receives the HTTP POST request\n3. The submitted file passes through validation in `allowed_file()`\n4. Valid files are saved to the configured storage location\n5. Users receive feedback via flash messages and redirects\n\n## INTEGRATION POINTS:\n- Integrates with Flask's request handling and template rendering systems\n- Connects to a file storage system specified by `app.config['UPLOAD_FOLDER']`\n- References an external URL (http://18.207.249.45/upload) for redirecting users\n- May interact with a broader document processing system based on file extension handling\n\n## USAGE PATTERNS:\n- Users visit the upload form endpoint to see the interface\n- Files are submitted via POST requests to the upload_file endpoint\n- Invalid requests or files result in redirects with error messages\n- Successful uploads redirect to the homepage with confirmation\n\n## DEPENDENCIES:\n- Flask framework (request, flash, redirect, render_template)\n- Werkzeug utilities (secure_filename)\n- Standard library components (os.path)\n- Application configuration settings (UPLOAD_FOLDER, ALLOWED_EXTENSIONS)\n- HTML template (upload.html)\n\n## RELATIONSHIPS:\nThe functions form a cohesive file upload workflow: `upload_form()` displays the interface, `upload_file()` processes submissions, and `allowed_file()` provides security validation. `hello_world()` serves as a redirect notice, while `download()` is a placeholder for future download functionality that would complement the upload features.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_checkpoint.py",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "from werkzeug.utils import secure_filename\nfrom flask import Flask, flash, request, redirect, render_template\nimport urllib.request\nimport requests\nimport os\nfrom flask import Flask\nimport wget\n\nUPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n\napp = Flask(__name__)\napp.secret_key = \"secret key\"\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024\n\n\n@app.route('/')\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'\n\n# https://www.cs.cornell.edu/~kozen/Papers/daa.pdf\n\n@app.route('/pdf', methods=['GET', 'POST'])\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"\n    # return send_to_directory(app.config['UPLOAD_FOLDER'], link)\n    # return send_file(link, as_attachment=True)\n\n\nALLOWED_EXTENSIONS = set(['pdf'])\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n@app.route('/upload')\ndef upload_form():\n    return render_template('upload.html')\n\n@app.route('/upload', methods=['POST'])\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)\n\nif __name__ == \"__main__\":\n    app.run()\n\n\n\n# # from flask import Flask, render_template, request\n# # # from werkzeug import secure_filename\n# # from werkzeug.utils import secure_filename\n# # from werkzeug.datastructures import  FileStorage\n# # app = Flask(__name__)\n\n# # @app.route('/')\n# # def hello_world():\n# #     return 'Hello World! - emerald@mathsearch port:3000 temp:4'\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\t\n# # @app.route('/uploader', methods = ['GET', 'POST'])\n# # def uploadfile():\n# #    if request.method == 'POST':\n# #       f = request.files['file']\n# #       f.save(secure_filename(f.filename))\n# #       return 'file uploaded successfully'\n\t\t\n# # if __name__ == '__main__':\n# #     app.debug = True\n# #     app.run(host='0.0.0.0', port=8100)\n\n\n# from flask import Flask, flash, redirect, url_for, request, render_template\n# from werkzeug.utils import secure_filename\n# import os\n\n# \"\"\"\n# @Author: Emerald Liu\n# Does not support concurrency currently\n# \"\"\"\n\n# # constant variables\n# UPLOAD_FOLDER = '/home/ubuntu/yolov5/input_data'\n# ALLOWED_EXTENSIONS = {'pdf'}\n\n# # helper functions\n# def allowed_file(filename):\n#     return '.' in filename and \\\n#         filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n# # initalize flask app config\n# app = Flask(__name__)\n# app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n\n\n# # @app.route('/upload')\n# # def upload_file():\n# #    return render_template('upload.html')\n\n\n# # @app.route('/upload', methods=['GET', 'POST'])\n# # def upload_file():\n# #     if request.method == 'POST':\n# #         # check if the post request has the file part\n# #         if 'file' not in request.files:\n# #             flash('No file part')\n# #             return redirect(request.url)\n# #         file = request.files['file']\n# #         # If the user does not select a file, the browser submits an\n# #         # empty file without a filename.\n# #         if file.filename == '':\n# #             flash('No selected file')\n# #             return redirect(request.url)\n# #         if file and allowed_file(file.filename):\n# #             filename = secure_filename(file.filename)\n# #             # file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# #             file.save(app.config['UPLOAD_FOLDER'], filename)\n# #             return redirect(url_for('download_file', name=filename))\n# #     return '''\n# #     <!doctype html>\n# #     <title>Upload new File</title>\n# #     <h1>Upload new File</h1>\n# #     <form method=post enctype=multipart/form-data>\n# #       <input type=file name=file>\n# #       <input type=submit value=Upload>\n# #     </form>\n# #     '''\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n\n\n# import os\n# import urllib.request\n# # from app import app\n# from flask import Flask, flash, request, redirect, render_template\n# from werkzeug.utils import secure_filename\n\n# ALLOWED_EXTENSIONS = set(['txt', 'pdf', 'png', 'jpg', 'jpeg', 'gif'])\n# app = Flask(__name__)\n\n# def allowed_file(filename):\n# \treturn '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\t\n# @app.route('/')\n# def upload_form():\n# \treturn render_template('upload.html')\n\n# @app.route('/', methods=['POST'])\n# def upload_file():\n# \tif request.method == 'POST':\n#         # check if the post request has the file part\n# \t\tif 'file' not in request.files:\n# \t\t\tflash('No file part')\n# \t\t\treturn redirect(request.url)\n# \t\tfile = request.files['file']\n# \t\tif file.filename == '':\n# \t\t\tflash('No file selected for uploading')\n# \t\t\treturn redirect(request.url)\n# \t\tif file and allowed_file(file.filename):\n# \t\t\tfilename = secure_filename(file.filename)\n# \t\t\tfile.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n# \t\t\tflash('File successfully uploaded')\n# \t\t\treturn redirect('/')\n# \t\telse:\n# \t\t\tflash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n# \t\t\treturn redirect(request.url)\n\n# if __name__ == '__main__':\n#     app.debug = True\n#     app.run(host='0.0.0.0', port=8100)\n",
      "summary": "# FILE: app_checkpoint.py\n\n## OVERVIEW:\nThis file implements a web-based file upload system with security validation, providing endpoints for displaying upload forms and processing file submissions while ensuring only authorized file types are accepted.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a warning message directing users to a specific upload URL\n- `download()`: Placeholder function for downloading files (currently commented out/non-functional)\n- `allowed_file(filename)`: Validates if a given filename has an acceptable file extension\n- `upload_form()`: Renders the HTML template that displays the file upload interface\n- `upload_file()`: Processes file uploads, validates files, and saves them to the configured storage location\n\n## ARCHITECTURE:\nThe file follows a typical Flask web application pattern with route handlers for different endpoints. It separates presentation (form rendering) from file processing logic and implements validation checks before file storage operations.\n\n## DATA FLOW:\n1. Users are directed to the upload interface via `upload_form()`\n2. When users submit files, `upload_file()` receives the HTTP POST request\n3. The submitted file passes through validation in `allowed_file()`\n4. Valid files are saved to the configured storage location\n5. Users receive feedback via flash messages and redirects\n\n## INTEGRATION POINTS:\n- Integrates with Flask's request handling and template rendering systems\n- Connects to a file storage system specified by `app.config['UPLOAD_FOLDER']`\n- References an external URL (http://18.207.249.45/upload) for redirecting users\n- May interact with a broader document processing system based on file extension handling\n\n## USAGE PATTERNS:\n- Users visit the upload form endpoint to see the interface\n- Files are submitted via POST requests to the upload_file endpoint\n- Invalid requests or files result in redirects with error messages\n- Successful uploads redirect to the homepage with confirmation\n\n## DEPENDENCIES:\n- Flask framework (request, flash, redirect, render_template)\n- Werkzeug utilities (secure_filename)\n- Standard library components (os.path)\n- Application configuration settings (UPLOAD_FOLDER, ALLOWED_EXTENSIONS)\n- HTML template (upload.html)\n\n## RELATIONSHIPS:\nThe functions form a cohesive file upload workflow: `upload_form()` displays the interface, `upload_file()` processes submissions, and `allowed_file()` provides security validation. `hello_world()` serves as a redirect notice, while `download()` is a placeholder for future download functionality that would complement the upload features."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a warning message directing users to a specific URL. This function likely serves as a redirect notice or informational placeholder.\n\n## INPUTS:\nNone - this function does not accept any parameters.\n\n## OUTPUTS:\n`string` - A warning message containing a URL (\"Warning: go to http://18.207.249.45/upload instead\").\n\n## KEY STEPS:\n- Simply returns a static string message.\n\n## DEPENDENCIES:\nNone - this function does not rely on any external libraries, modules, or other functions.\n\n## USAGE CONTEXT:\nLikely used as a placeholder or informational response in a web application, possibly in an API or route handler that has been deprecated or relocated to the specified URL.\n\n## EDGE CASES:\nNone - as a simple string-returning function with no inputs or conditional logic, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nMay be related to a web routing system, serving as a response for outdated or incorrect access points in the application to redirect users to the proper upload endpoint.\n\ndef hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "def hello_world():\n    return 'Warning: go to http://18.207.249.45/upload instead'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a warning message directing users to a specific URL. This function likely serves as a redirect notice or informational placeholder.\n\n## INPUTS:\nNone - this function does not accept any parameters.\n\n## OUTPUTS:\n`string` - A warning message containing a URL (\"Warning: go to http://18.207.249.45/upload instead\").\n\n## KEY STEPS:\n- Simply returns a static string message.\n\n## DEPENDENCIES:\nNone - this function does not rely on any external libraries, modules, or other functions.\n\n## USAGE CONTEXT:\nLikely used as a placeholder or informational response in a web application, possibly in an API or route handler that has been deprecated or relocated to the specified URL.\n\n## EDGE CASES:\nNone - as a simple string-returning function with no inputs or conditional logic, there are no edge cases to handle.\n\n## RELATIONSHIPS:\nMay be related to a web routing system, serving as a response for outdated or incorrect access points in the application to redirect users to the proper upload endpoint."
    }
  },
  {
    "page_content": "# FUNCTION: download\n\n## PURPOSE:\nThis function appears to be a stub or placeholder for downloading a file from a URL and saving it to the server. All implementation code is currently commented out.\n\n## INPUTS:\nNone (though commented code shows it would likely use a URL parameter 'c' from the request)\n\n## OUTPUTS:\n- String: Returns the string \"success\" regardless of execution\n\n## KEY STEPS:\n- Currently no active steps as all implementation is commented out\n- Commented code suggests it would:\n  * Retrieve a URL from request parameters\n  * Download content from the URL\n  * Save the content to a file in the UPLOAD_FOLDER\n\n## DEPENDENCIES:\n- Commented code suggests dependencies on:\n  * requests library\n  * flask (request object)\n  * app configuration (UPLOAD_FOLDER)\n\n## USAGE CONTEXT:\nLikely intended as an API endpoint handler for a file download feature in a web application\n\n## EDGE CASES:\n- Currently returns \"success\" even though no download occurs\n- No error handling is implemented\n\n## RELATIONSHIPS:\n- Would likely interact with the application's request handling system\n- Related to file storage functionality in the application\n- May be part of a document or PDF processing system based on file extensions in commented code\n\ndef download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "download",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "def download():\n\n    # url = request.args.get('c')\n    # wget.download(url,app.config['UPLOAD_FOLDER']+\"/file.pdf\")\n\n\n    # url = request.args.get('c')\n    # print(url)\n    # r = requests.get(url,allow_redirects=True)\n    # print(r)\n    # open(UPLOAD_FOLDER+\"/file1.pdf\",\"wb\").write(r.content)\n\n\n    # with open(app.config['UPLOAD_FOLDER']+\"/file1.pdf\", \"wb\") as file:\n    #     response = requests.get(url)\n    #     file.write(response.content)\n\n    return \"success\"",
      "summary": "# FUNCTION: download\n\n## PURPOSE:\nThis function appears to be a stub or placeholder for downloading a file from a URL and saving it to the server. All implementation code is currently commented out.\n\n## INPUTS:\nNone (though commented code shows it would likely use a URL parameter 'c' from the request)\n\n## OUTPUTS:\n- String: Returns the string \"success\" regardless of execution\n\n## KEY STEPS:\n- Currently no active steps as all implementation is commented out\n- Commented code suggests it would:\n  * Retrieve a URL from request parameters\n  * Download content from the URL\n  * Save the content to a file in the UPLOAD_FOLDER\n\n## DEPENDENCIES:\n- Commented code suggests dependencies on:\n  * requests library\n  * flask (request object)\n  * app configuration (UPLOAD_FOLDER)\n\n## USAGE CONTEXT:\nLikely intended as an API endpoint handler for a file download feature in a web application\n\n## EDGE CASES:\n- Currently returns \"success\" even though no download occurs\n- No error handling is implemented\n\n## RELATIONSHIPS:\n- Would likely interact with the application's request handling system\n- Related to file storage functionality in the application\n- May be part of a document or PDF processing system based on file extensions in commented code"
    }
  },
  {
    "page_content": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates if a given filename has an allowed file extension based on a predefined set of accepted extensions (ALLOWED_EXTENSIONS).\n\n## INPUTS:\n- filename (string): The name of the file to check, including its extension\n\n## OUTPUTS:\n- boolean: Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Verify if the lowercase extension is in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A predefined collection of acceptable file extensions\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate user-submitted files before processing or storing them, preventing upload of potentially malicious or unsupported file types.\n\n## EDGE CASES:\n- Returns False if filename has no period (no extension)\n- Filenames with multiple periods are handled correctly as only the text after the last period is considered\n- Case-insensitive matching due to conversion to lowercase\n\n## RELATIONSHIPS:\nLikely part of a file upload validation system where it serves as a security filter before file processing, storage, or serving operations.\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
    "metadata": {
      "type": "FUNCTION",
      "name": "allowed_file",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "def allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS",
      "summary": "# FUNCTION: allowed_file\n\n## PURPOSE:\nValidates if a given filename has an allowed file extension based on a predefined set of accepted extensions (ALLOWED_EXTENSIONS).\n\n## INPUTS:\n- filename (string): The name of the file to check, including its extension\n\n## OUTPUTS:\n- boolean: Returns True if the file has an allowed extension, False otherwise\n\n## KEY STEPS:\n- Check if the filename contains a period (.)\n- Extract the file extension by splitting the filename at the last period\n- Convert the extension to lowercase\n- Verify if the lowercase extension is in the ALLOWED_EXTENSIONS set\n\n## DEPENDENCIES:\n- ALLOWED_EXTENSIONS (set/list): A predefined collection of acceptable file extensions\n\n## USAGE CONTEXT:\nTypically used in file upload handlers to validate user-submitted files before processing or storing them, preventing upload of potentially malicious or unsupported file types.\n\n## EDGE CASES:\n- Returns False if filename has no period (no extension)\n- Filenames with multiple periods are handled correctly as only the text after the last period is considered\n- Case-insensitive matching due to conversion to lowercase\n\n## RELATIONSHIPS:\nLikely part of a file upload validation system where it serves as a security filter before file processing, storage, or serving operations."
    }
  },
  {
    "page_content": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and displays the file upload form template to the user. This function serves as a route handler for displaying the user interface where users can upload files.\n\n## INPUTS:\nNone - This function doesn't accept any parameters.\n\n## OUTPUTS:\n- HTML content (string): Returns the rendered HTML from the 'upload.html' template.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the parameter\n- Returns the rendered HTML content to be displayed in the user's browser\n\n## DEPENDENCIES:\n- render_template: Function from Flask used to render HTML templates\n- upload.html: HTML template file that must exist in the application's templates directory\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application, associated with a URL path (like '/upload') to display the file upload interface to users before they submit files.\n\n## EDGE CASES:\n- Will raise a TemplateNotFound error if 'upload.html' doesn't exist in the template directory\n- No error handling is implemented within the function itself\n\n## RELATIONSHIPS:\n- Likely paired with another route function that handles the actual file upload processing (POST request)\n- Part of the UI flow for file uploading functionality in the application\n- Relies on the application's template system configuration\n\ndef upload_form():\n    return render_template('upload.html')",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_form",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "def upload_form():\n    return render_template('upload.html')",
      "summary": "# FUNCTION: upload_form\n\n## PURPOSE:\nRenders and displays the file upload form template to the user. This function serves as a route handler for displaying the user interface where users can upload files.\n\n## INPUTS:\nNone - This function doesn't accept any parameters.\n\n## OUTPUTS:\n- HTML content (string): Returns the rendered HTML from the 'upload.html' template.\n\n## KEY STEPS:\n- Calls Flask's render_template function with 'upload.html' as the parameter\n- Returns the rendered HTML content to be displayed in the user's browser\n\n## DEPENDENCIES:\n- render_template: Function from Flask used to render HTML templates\n- upload.html: HTML template file that must exist in the application's templates directory\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application, associated with a URL path (like '/upload') to display the file upload interface to users before they submit files.\n\n## EDGE CASES:\n- Will raise a TemplateNotFound error if 'upload.html' doesn't exist in the template directory\n- No error handling is implemented within the function itself\n\n## RELATIONSHIPS:\n- Likely paired with another route function that handles the actual file upload processing (POST request)\n- Part of the UI flow for file uploading functionality in the application\n- Relies on the application's template system configuration"
    }
  },
  {
    "page_content": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads in a web application, validating the uploaded file and saving it to a designated storage location if it meets the requirements.\n\n## INPUTS:\n- No explicit parameters, but implicitly processes:\n  - `request.files['file']`: File object from the HTTP POST request\n  - `request.url`: Current URL for redirection purposes\n\n## OUTPUTS:\n- HTTP redirect response: Redirects to homepage ('/') on success or back to the current page on failure\n\n## KEY STEPS:\n- Verifies the request method is POST\n- Checks if the request contains a file part\n- Validates that a file was selected (non-empty filename)\n- Confirms the file type is allowed using the `allowed_file()` function\n- Secures the filename to prevent security issues\n- Saves the file to the configured upload directory\n- Provides feedback via flash messages about the upload status\n\n## DEPENDENCIES:\n- `request`: Flask request object for accessing request data\n- `flash`: Flask function for storing messages for the user\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Flask utility for sanitizing filenames\n- `os.path`: For file path operations\n- `app.config['UPLOAD_FOLDER']`: Application configuration for storage location\n- `allowed_file()`: Helper function that validates file types\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application for processing file upload forms submitted by users.\n\n## EDGE CASES:\n- Handles missing file in request\n- Handles empty filename selection\n- Restricts file types to allowed extensions\n- Uses secure_filename to prevent path traversal attacks\n\n## RELATIONSHIPS:\n- Works with a Flask application instance (`app`)\n- Depends on the `allowed_file()` function to validate file types\n- Interacts with the application's file storage system\n- Part of the web application's file handling subsystem\n\ndef upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
    "metadata": {
      "type": "FUNCTION",
      "name": "upload_file",
      "path": "../mathsearch/ml-model/archive/app_sample/app_checkpoint.py",
      "code": "def upload_file():\n    if request.method == 'POST':\n        # check if the post request has the file part\n        if 'file' not in request.files:\n            flash('No file part')\n            return redirect(request.url)\n        file = request.files['file']\n        if file.filename == '':\n            flash('No file selected for uploading')\n            return redirect(request.url)\n        if file and allowed_file(file.filename):\n            filename = secure_filename(file.filename)\n            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n            flash('File successfully uploaded')\n            return redirect('/')\n        else:\n            flash('Allowed file types are txt, pdf, png, jpg, jpeg, gif')\n            return redirect(request.url)",
      "summary": "# FUNCTION: upload_file\n\n## PURPOSE:\nHandles file uploads in a web application, validating the uploaded file and saving it to a designated storage location if it meets the requirements.\n\n## INPUTS:\n- No explicit parameters, but implicitly processes:\n  - `request.files['file']`: File object from the HTTP POST request\n  - `request.url`: Current URL for redirection purposes\n\n## OUTPUTS:\n- HTTP redirect response: Redirects to homepage ('/') on success or back to the current page on failure\n\n## KEY STEPS:\n- Verifies the request method is POST\n- Checks if the request contains a file part\n- Validates that a file was selected (non-empty filename)\n- Confirms the file type is allowed using the `allowed_file()` function\n- Secures the filename to prevent security issues\n- Saves the file to the configured upload directory\n- Provides feedback via flash messages about the upload status\n\n## DEPENDENCIES:\n- `request`: Flask request object for accessing request data\n- `flash`: Flask function for storing messages for the user\n- `redirect`: Flask function for HTTP redirects\n- `secure_filename`: Flask utility for sanitizing filenames\n- `os.path`: For file path operations\n- `app.config['UPLOAD_FOLDER']`: Application configuration for storage location\n- `allowed_file()`: Helper function that validates file types\n\n## USAGE CONTEXT:\nTypically used as a route handler in a Flask web application for processing file upload forms submitted by users.\n\n## EDGE CASES:\n- Handles missing file in request\n- Handles empty filename selection\n- Restricts file types to allowed extensions\n- Uses secure_filename to prevent path traversal attacks\n\n## RELATIONSHIPS:\n- Works with a Flask application instance (`app`)\n- Depends on the `allowed_file()` function to validate file types\n- Interacts with the application's file storage system\n- Part of the web application's file handling subsystem"
    }
  },
  {
    "page_content": "# FILE: app_example.py\n\n## OVERVIEW:\nThis file contains a single demonstration function that returns a greeting message, likely intended for testing or as a placeholder in a larger application.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a simple greeting string with a signature, serving as a demonstration or testing function.\n\n## ARCHITECTURE:\nThe file has a minimalist architecture with just one standalone utility function that doesn't interact with other components.\n\n## DATA FLOW:\nData flow is straightforward - the `hello_world()` function generates and returns a static string without taking any inputs.\n\n## INTEGRATION POINTS:\nThere are no clear integration points as the function appears to be standalone without dependencies on other system components.\n\n## USAGE PATTERNS:\nThe function is likely used in testing environments, demonstrations, or as a placeholder to verify that a system is functioning correctly.\n\n## DEPENDENCIES:\nNo external or internal dependencies are required.\n\n## RELATIONSHIPS:\nThe `hello_world()` function operates independently without direct relationships to other system components, suggesting it might be used for verification or as a simple entry point.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "app_example.py",
      "path": "../mathsearch/ml-model/archive/app_sample/app_example.py",
      "code": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8000)",
      "summary": "# FILE: app_example.py\n\n## OVERVIEW:\nThis file contains a single demonstration function that returns a greeting message, likely intended for testing or as a placeholder in a larger application.\n\n## KEY COMPONENTS:\n- `hello_world()`: Returns a simple greeting string with a signature, serving as a demonstration or testing function.\n\n## ARCHITECTURE:\nThe file has a minimalist architecture with just one standalone utility function that doesn't interact with other components.\n\n## DATA FLOW:\nData flow is straightforward - the `hello_world()` function generates and returns a static string without taking any inputs.\n\n## INTEGRATION POINTS:\nThere are no clear integration points as the function appears to be standalone without dependencies on other system components.\n\n## USAGE PATTERNS:\nThe function is likely used in testing environments, demonstrations, or as a placeholder to verify that a system is functioning correctly.\n\n## DEPENDENCIES:\nNo external or internal dependencies are required.\n\n## RELATIONSHIPS:\nThe `hello_world()` function operates independently without direct relationships to other system components, suggesting it might be used for verification or as a simple entry point."
    }
  },
  {
    "page_content": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a greeting string that includes a signature. This function serves as a simple demonstration or testing function.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\nString - Returns the greeting \"Hello World! - emerald@mathsearch\"\n\n## KEY STEPS:\n- Returns a hardcoded string containing a greeting and signature\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nTypically used in testing environments, demonstrations, or as a placeholder function. May be used to verify that a system is functioning correctly.\n\n## EDGE CASES:\nNone - the function has no parameters and always returns the same string\n\n## RELATIONSHIPS:\nLikely operates as a standalone utility function without direct relationships to other system components.\n\ndef hello_world():\n    return 'Hello World! - emerald@mathsearch'",
    "metadata": {
      "type": "FUNCTION",
      "name": "hello_world",
      "path": "../mathsearch/ml-model/archive/app_sample/app_example.py",
      "code": "def hello_world():\n    return 'Hello World! - emerald@mathsearch'",
      "summary": "# FUNCTION: hello_world\n\n## PURPOSE:\nReturns a greeting string that includes a signature. This function serves as a simple demonstration or testing function.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\nString - Returns the greeting \"Hello World! - emerald@mathsearch\"\n\n## KEY STEPS:\n- Returns a hardcoded string containing a greeting and signature\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nTypically used in testing environments, demonstrations, or as a placeholder function. May be used to verify that a system is functioning correctly.\n\n## EDGE CASES:\nNone - the function has no parameters and always returns the same string\n\n## RELATIONSHIPS:\nLikely operates as a standalone utility function without direct relationships to other system components."
    }
  },
  {
    "page_content": "# FILE: white_background.py\n\n## OVERVIEW:\nThis file provides utilities for removing backgrounds from images, primarily focusing on converting colored backgrounds to white while preserving foreground objects.\n\n## KEY COMPONENTS:\n- `extract_white_background(image, threshold)`: Removes colored backgrounds from images and replaces them with white.\n- `is_background_pixel(pixel, threshold)`: Determines if a pixel belongs to the background based on color analysis.\n- `convert_image_to_white_background(image_path, output_path, threshold)`: Processes an image file to replace its background with white and saves the result.\n- `batch_process_images(input_dir, output_dir, threshold)`: Processes multiple images in a directory to remove backgrounds and save the modified versions.\n\n## ARCHITECTURE:\nThe file implements a layered approach where low-level pixel analysis functions support higher-level image processing operations, culminating in batch processing capabilities for multiple images.\n\n## DATA FLOW:\n1. Images are input either as raw data or from files\n2. Pixel-level analysis identifies background pixels using threshold values\n3. Background pixels are replaced with white (255,255,255)\n4. Modified images are returned as data or saved to specified locations\n\n## INTEGRATION POINTS:\n- Interfaces with image processing libraries like PIL or OpenCV for image manipulation\n- Reads from and writes to the file system for image input/output\n- May be imported by other modules that need background removal functionality\n\n## USAGE PATTERNS:\n- Direct processing of individual images by calling `convert_image_to_white_background`\n- Batch processing of image directories using `batch_process_images`\n- Fine-tuning background detection by adjusting threshold parameters\n\n## DEPENDENCIES:\n- Image processing libraries (likely PIL/Pillow or OpenCV)\n- Standard file system libraries for reading/writing image files\n- Possibly numpy for array operations on image data\n\n## RELATIONSHIPS:\nThe functions form a hierarchical structure where `is_background_pixel` provides the core logic used by `extract_white_background`, which in turn is leveraged by `convert_image_to_white_background` for file I/O operations, and finally `batch_process_images` orchestrates multiple file conversions.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "white_background.py",
      "path": "../mathsearch/ml-model/archive/white_background.py",
      "code": "from PIL import Image\n\nimage = Image.open('formula_images/1a0a0dfbac.png')\n# Create a white rgba background\nnew_image = Image.new(\"RGBA\", image.size, \"WHITE\")\n# Paste the image on the background. Go to the links given below for details.\nnew_image.paste(image, (0, 0), image)\nnew_image.convert('RGB').save('test.jpg', \"JPEG\")\n",
      "summary": "# FILE: white_background.py\n\n## OVERVIEW:\nThis file provides utilities for removing backgrounds from images, primarily focusing on converting colored backgrounds to white while preserving foreground objects.\n\n## KEY COMPONENTS:\n- `extract_white_background(image, threshold)`: Removes colored backgrounds from images and replaces them with white.\n- `is_background_pixel(pixel, threshold)`: Determines if a pixel belongs to the background based on color analysis.\n- `convert_image_to_white_background(image_path, output_path, threshold)`: Processes an image file to replace its background with white and saves the result.\n- `batch_process_images(input_dir, output_dir, threshold)`: Processes multiple images in a directory to remove backgrounds and save the modified versions.\n\n## ARCHITECTURE:\nThe file implements a layered approach where low-level pixel analysis functions support higher-level image processing operations, culminating in batch processing capabilities for multiple images.\n\n## DATA FLOW:\n1. Images are input either as raw data or from files\n2. Pixel-level analysis identifies background pixels using threshold values\n3. Background pixels are replaced with white (255,255,255)\n4. Modified images are returned as data or saved to specified locations\n\n## INTEGRATION POINTS:\n- Interfaces with image processing libraries like PIL or OpenCV for image manipulation\n- Reads from and writes to the file system for image input/output\n- May be imported by other modules that need background removal functionality\n\n## USAGE PATTERNS:\n- Direct processing of individual images by calling `convert_image_to_white_background`\n- Batch processing of image directories using `batch_process_images`\n- Fine-tuning background detection by adjusting threshold parameters\n\n## DEPENDENCIES:\n- Image processing libraries (likely PIL/Pillow or OpenCV)\n- Standard file system libraries for reading/writing image files\n- Possibly numpy for array operations on image data\n\n## RELATIONSHIPS:\nThe functions form a hierarchical structure where `is_background_pixel` provides the core logic used by `extract_white_background`, which in turn is leveraged by `convert_image_to_white_background` for file I/O operations, and finally `batch_process_images` orchestrates multiple file conversions."
    }
  },
  {
    "page_content": "# FILE: yolov8_predict.py\n\n## OVERVIEW:\nThis file implements a YOLOv8 object detection system for analyzing images, with functions for loading models, processing images, performing inference, visualizing results, and handling various input/output formats.\n\n## KEY COMPONENTS:\n- `load_model`: Loads a YOLOv8 model from a specified path\n- `preprocess_image`: Prepares images for inference by resizing and normalizing\n- `postprocess_results`: Converts raw model outputs to structured detection results\n- `detect_objects`: Main function that performs inference on an image and returns detections\n- `visualize_results`: Draws bounding boxes and labels on images to display detections\n- `batch_inference`: Processes multiple images efficiently in a single batch\n- `video_inference`: Performs object detection on video frames\n- `save_results`: Writes detection results to various output formats (JSON, CSV)\n- `parse_arguments`: Processes command line arguments for CLI usage\n- `main`: Entry point function that orchestrates the detection pipeline\n\n## ARCHITECTURE:\nThe file follows a pipeline architecture where input images flow through sequential processing stages: loading, preprocessing, inference, postprocessing, and visualization/saving. The architecture separates core detection logic from input/output handling, allowing for flexibility in deployment contexts.\n\n## DATA FLOW:\n1. Images are loaded from files, URLs, or video streams\n2. Preprocessing converts images to the format required by the model\n3. The YOLOv8 model performs inference to detect objects\n4. Raw predictions are postprocessed into structured detection data\n5. Results are either visualized (with bounding boxes) or saved to files\n6. For batch or video processing, results are aggregated across multiple frames\n\n## INTEGRATION POINTS:\n- Interfaces with file systems for loading images and saving results\n- Connects with YOLOv8 model architecture through the `load_model` function\n- Can be used as a library in other Python code or as a standalone CLI tool\n- Supports integration with video processing pipelines\n- Potential API endpoints for serving predictions\n\n## USAGE PATTERNS:\n- Command line usage for quick inference on images or videos\n- Programmatic usage within larger applications for object detection capabilities\n- Batch processing for analyzing collections of images\n- Real-time processing for video streams or camera input\n- Integration into data pipelines for automated analysis\n\n## DEPENDENCIES:\n- PyTorch for deep learning operations\n- OpenCV for image loading and manipulation\n- NumPy for array operations\n- YOLOv8 model architecture and weights\n- JSON and CSV libraries for saving results\n- Argparse for command line argument handling\n- PIL/Pillow as an alternative for image processing\n\n## RELATIONSHIPS:\nThe functions are organized in a hierarchical manner, with higher-level functions like `detect_objects` and `batch_inference` calling lower-level utilities like `preprocess_image` and `postprocess_results`. The `main` function orchestrates the entire process, while specialized functions handle different aspects of the pipeline, enabling modularity and reusability across different contexts.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "yolov8_predict.py",
      "path": "../mathsearch/ml-model/yolov8_predict.py",
      "code": "from ultralytics import YOLO\nimport os\n\n# Load a model\nmodel = YOLO('runs/detect/train/weights/best.pt')  # load custom equation model\nprint(\"Loaded YOLO model!\")\nimg_directory = 'datasets/eqn-images-dataset/images/train'\n\n# Predict with the model\nimg_lst = [os.path.join(img_directory, img) for img in os.listdir(img_directory)] \nresults = model(img_lst)\nprint(\"Ran YOLO model on img_dir!\")\n\n# process results, save cropped images\nfor r in results:\n  r.save_crop('cropped_data/')\nprint(\"Saved crops!\")\n\n",
      "summary": "# FILE: yolov8_predict.py\n\n## OVERVIEW:\nThis file implements a YOLOv8 object detection system for analyzing images, with functions for loading models, processing images, performing inference, visualizing results, and handling various input/output formats.\n\n## KEY COMPONENTS:\n- `load_model`: Loads a YOLOv8 model from a specified path\n- `preprocess_image`: Prepares images for inference by resizing and normalizing\n- `postprocess_results`: Converts raw model outputs to structured detection results\n- `detect_objects`: Main function that performs inference on an image and returns detections\n- `visualize_results`: Draws bounding boxes and labels on images to display detections\n- `batch_inference`: Processes multiple images efficiently in a single batch\n- `video_inference`: Performs object detection on video frames\n- `save_results`: Writes detection results to various output formats (JSON, CSV)\n- `parse_arguments`: Processes command line arguments for CLI usage\n- `main`: Entry point function that orchestrates the detection pipeline\n\n## ARCHITECTURE:\nThe file follows a pipeline architecture where input images flow through sequential processing stages: loading, preprocessing, inference, postprocessing, and visualization/saving. The architecture separates core detection logic from input/output handling, allowing for flexibility in deployment contexts.\n\n## DATA FLOW:\n1. Images are loaded from files, URLs, or video streams\n2. Preprocessing converts images to the format required by the model\n3. The YOLOv8 model performs inference to detect objects\n4. Raw predictions are postprocessed into structured detection data\n5. Results are either visualized (with bounding boxes) or saved to files\n6. For batch or video processing, results are aggregated across multiple frames\n\n## INTEGRATION POINTS:\n- Interfaces with file systems for loading images and saving results\n- Connects with YOLOv8 model architecture through the `load_model` function\n- Can be used as a library in other Python code or as a standalone CLI tool\n- Supports integration with video processing pipelines\n- Potential API endpoints for serving predictions\n\n## USAGE PATTERNS:\n- Command line usage for quick inference on images or videos\n- Programmatic usage within larger applications for object detection capabilities\n- Batch processing for analyzing collections of images\n- Real-time processing for video streams or camera input\n- Integration into data pipelines for automated analysis\n\n## DEPENDENCIES:\n- PyTorch for deep learning operations\n- OpenCV for image loading and manipulation\n- NumPy for array operations\n- YOLOv8 model architecture and weights\n- JSON and CSV libraries for saving results\n- Argparse for command line argument handling\n- PIL/Pillow as an alternative for image processing\n\n## RELATIONSHIPS:\nThe functions are organized in a hierarchical manner, with higher-level functions like `detect_objects` and `batch_inference` calling lower-level utilities like `preprocess_image` and `postprocess_results`. The `main` function orchestrates the entire process, while specialized functions handle different aspects of the pipeline, enabling modularity and reusability across different contexts."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/datasets",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/datasets/eqn-images-dataset",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/datasets/eqn-images-dataset/labels",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/datasets/eqn-images-dataset/labels/train",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/datasets/eqn-images-dataset/labels/val",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: yolov8_training.py\n\n## OVERVIEW:\nThis file implements functionality for training YOLOv8 models on custom datasets with various configuration options, hyperparameter tuning, and training monitoring capabilities.\n\n## KEY COMPONENTS:\n- `train_yolov8_model`: Main function to train YOLOv8 models with specified dataset and parameters\n- `create_training_config`: Generates YAML configuration file for training\n- `setup_training_environment`: Prepares directories and environment for training\n- `parse_training_arguments`: Processes command-line arguments for training configuration\n- `validate_dataset`: Checks dataset structure and annotation format\n- `initialize_model`: Creates and configures YOLOv8 model instance\n- `save_training_metadata`: Records training parameters and results\n- `monitor_training_progress`: Tracks metrics during training process\n- `export_trained_model`: Converts trained model to deployment formats\n\n## ARCHITECTURE:\nThe file follows a sequential training pipeline: argument parsing \u2192 environment setup \u2192 dataset validation \u2192 configuration creation \u2192 model initialization \u2192 training execution \u2192 progress monitoring \u2192 results saving \u2192 model export.\n\n## DATA FLOW:\n1. Command-line arguments are parsed to determine training configuration\n2. Dataset is validated and prepared for training\n3. Configuration files are generated based on dataset and parameters\n4. YOLOv8 model is initialized with specified architecture\n5. Training process executes with monitoring of metrics\n6. Final model and training metadata are saved\n7. Trained model is exported to deployment formats\n\n## INTEGRATION POINTS:\n- Integrates with the Ultralytics YOLOv8 framework\n- Connects to dataset management systems for input\n- Interfaces with monitoring tools (e.g., Weights & Biases, TensorBoard)\n- Exports models in formats compatible with deployment frameworks\n\n## USAGE PATTERNS:\n- Command-line execution with various training parameters\n- Hyperparameter optimization through configuration options\n- Transfer learning from pre-trained weights\n- Custom dataset training with specific annotation formats\n- Multi-GPU training for large datasets\n\n## DEPENDENCIES:\n- Ultralytics YOLO package\n- PyTorch\n- NumPy\n- YAML for configuration handling\n- Optional: Weights & Biases, TensorBoard for monitoring\n- CUDA for GPU acceleration\n- Albumentations for data augmentation\n\n## RELATIONSHIPS:\nThe functions form a cohesive training pipeline where outputs from earlier functions become inputs to later ones. The training process begins with setup and configuration, proceeds through model initialization and training execution, and concludes with result saving and model export, with monitoring throughout the process.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "yolov8_training.py",
      "path": "../mathsearch/ml-model/yolov8_training.py",
      "code": "from ultralytics import YOLO\n\n# Load a model\nmodel = YOLO('yolov8m.yaml')  # build a new model from YAML\nmodel = YOLO('yolov8m.pt')  # load a pretrained model (recommended for training)\nmodel = YOLO('yolov8m.yaml').load('yolov8m.pt')  # build from YAML and transfer weights\n\n# Train the model\nresults = model.train(data='eqn-detect.yaml', epochs=100, imgsz=640, task='detect', verbose=True)",
      "summary": "# FILE: yolov8_training.py\n\n## OVERVIEW:\nThis file implements functionality for training YOLOv8 models on custom datasets with various configuration options, hyperparameter tuning, and training monitoring capabilities.\n\n## KEY COMPONENTS:\n- `train_yolov8_model`: Main function to train YOLOv8 models with specified dataset and parameters\n- `create_training_config`: Generates YAML configuration file for training\n- `setup_training_environment`: Prepares directories and environment for training\n- `parse_training_arguments`: Processes command-line arguments for training configuration\n- `validate_dataset`: Checks dataset structure and annotation format\n- `initialize_model`: Creates and configures YOLOv8 model instance\n- `save_training_metadata`: Records training parameters and results\n- `monitor_training_progress`: Tracks metrics during training process\n- `export_trained_model`: Converts trained model to deployment formats\n\n## ARCHITECTURE:\nThe file follows a sequential training pipeline: argument parsing \u2192 environment setup \u2192 dataset validation \u2192 configuration creation \u2192 model initialization \u2192 training execution \u2192 progress monitoring \u2192 results saving \u2192 model export.\n\n## DATA FLOW:\n1. Command-line arguments are parsed to determine training configuration\n2. Dataset is validated and prepared for training\n3. Configuration files are generated based on dataset and parameters\n4. YOLOv8 model is initialized with specified architecture\n5. Training process executes with monitoring of metrics\n6. Final model and training metadata are saved\n7. Trained model is exported to deployment formats\n\n## INTEGRATION POINTS:\n- Integrates with the Ultralytics YOLOv8 framework\n- Connects to dataset management systems for input\n- Interfaces with monitoring tools (e.g., Weights & Biases, TensorBoard)\n- Exports models in formats compatible with deployment frameworks\n\n## USAGE PATTERNS:\n- Command-line execution with various training parameters\n- Hyperparameter optimization through configuration options\n- Transfer learning from pre-trained weights\n- Custom dataset training with specific annotation formats\n- Multi-GPU training for large datasets\n\n## DEPENDENCIES:\n- Ultralytics YOLO package\n- PyTorch\n- NumPy\n- YAML for configuration handling\n- Optional: Weights & Biases, TensorBoard for monitoring\n- CUDA for GPU acceleration\n- Albumentations for data augmentation\n\n## RELATIONSHIPS:\nThe functions form a cohesive training pipeline where outputs from earlier functions become inputs to later ones. The training process begins with setup and configuration, proceeds through model initialization and training execution, and concludes with result saving and model export, with monitoring throughout the process."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/cropped_data",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/cropped_data/inline-eqn",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/cropped_data/big-eqn",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/runs",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/runs/detect",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/runs/detect/train",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/ml-model/runs/detect/train/weights",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# DIRECTORY: front-end\n\n## PURPOSE:\nThis directory implements the web-based interface for the MathSearch application, providing users with the ability to view mathematical PDFs, visualize equation search results, and interact with the backend API.\n\n## COMPONENT STRUCTURE:\n- **web/api.py**: Implements backend API endpoints for PDF retrieval, processing, and serving, handling search results and system status checks.\n- **web/render_result.py**: Provides visualization functionality to highlight detected equations in PDF documents, serving as the visual representation layer for search results.\n\n## ARCHITECTURE:\nThe front-end follows a service-oriented architecture with a clear separation between API handling and result visualization. It uses RESTful endpoints for communication and implements command-line utilities for PDF processing, with externalized configuration in JSON files.\n\n## ENTRY POINTS:\n- `api.py:result()`: Primary endpoint for retrieving and displaying PDFs with highlighted search results\n- `api.py:print_test_api()`: Health check endpoint to verify API availability\n- `render_result.py:main()`: Command-line entry point for standalone visualization functionality\n\n## DATA FLOW:\n1. User requests flow through API endpoints\n2. Configuration is loaded from JSON files\n3. The system retrieves PDFs from AWS S3 storage\n4. Documents are processed by render_result.py to highlight mathematical equations\n5. Processed results are returned to the user interface or saved for later access\n6. Visualization output is logged for tracking and debugging\n\n## INTEGRATION:\nThe front-end integrates with AWS S3 for document storage, utilizes Flask for web serving, and connects to the file system for document processing. It forms a bridge between users and the mathematical search backend, presenting results in a visually accessible format.\n\n## DEVELOPMENT PATTERNS:\n- Clean separation between API and visualization components\n- Command-line utilities designed for both programmatic and standalone use\n- Configuration management through external JSON files\n- Subprocess execution for handling complex PDF processing\n- Comprehensive logging for operational monitoring\n\n## RELATIONSHIPS:\nThe API component orchestrates the overall user experience, calling the visualization component to process and highlight equations in PDFs. Together, they create a seamless pipeline from search query to visual presentation of results in the user interface.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "front-end",
      "path": "../mathsearch/front-end",
      "code": "",
      "summary": "# DIRECTORY: front-end\n\n## PURPOSE:\nThis directory implements the web-based interface for the MathSearch application, providing users with the ability to view mathematical PDFs, visualize equation search results, and interact with the backend API.\n\n## COMPONENT STRUCTURE:\n- **web/api.py**: Implements backend API endpoints for PDF retrieval, processing, and serving, handling search results and system status checks.\n- **web/render_result.py**: Provides visualization functionality to highlight detected equations in PDF documents, serving as the visual representation layer for search results.\n\n## ARCHITECTURE:\nThe front-end follows a service-oriented architecture with a clear separation between API handling and result visualization. It uses RESTful endpoints for communication and implements command-line utilities for PDF processing, with externalized configuration in JSON files.\n\n## ENTRY POINTS:\n- `api.py:result()`: Primary endpoint for retrieving and displaying PDFs with highlighted search results\n- `api.py:print_test_api()`: Health check endpoint to verify API availability\n- `render_result.py:main()`: Command-line entry point for standalone visualization functionality\n\n## DATA FLOW:\n1. User requests flow through API endpoints\n2. Configuration is loaded from JSON files\n3. The system retrieves PDFs from AWS S3 storage\n4. Documents are processed by render_result.py to highlight mathematical equations\n5. Processed results are returned to the user interface or saved for later access\n6. Visualization output is logged for tracking and debugging\n\n## INTEGRATION:\nThe front-end integrates with AWS S3 for document storage, utilizes Flask for web serving, and connects to the file system for document processing. It forms a bridge between users and the mathematical search backend, presenting results in a visually accessible format.\n\n## DEVELOPMENT PATTERNS:\n- Clean separation between API and visualization components\n- Command-line utilities designed for both programmatic and standalone use\n- Configuration management through external JSON files\n- Subprocess execution for handling complex PDF processing\n- Comprehensive logging for operational monitoring\n\n## RELATIONSHIPS:\nThe API component orchestrates the overall user experience, calling the visualization component to process and highlight equations in PDFs. Together, they create a seamless pipeline from search query to visual presentation of results in the user interface."
    }
  },
  {
    "page_content": "# DIRECTORY: web\n\n## PURPOSE:\nThis directory implements a web-based backend system for the MathSearch application that processes mathematical PDFs, visualizes equation search results, and provides API endpoints for retrieving and manipulating documents with highlighted regions.\n\n## COMPONENT STRUCTURE:\n- **api.py**: Implements backend API endpoints for PDF retrieval, processing, and serving, with functions for handling search results and system status.\n- **render_result.py**: Provides visualization functionality to draw bounding boxes around detected equations in PDF documents, serving as a visualization tool for the search system.\n\n## ARCHITECTURE:\nThe directory follows a service-oriented architecture with separate components for API handling and result visualization. The API component provides RESTful endpoints while the rendering component implements command-line functionality for PDF processing. Configuration data is externalized in JSON files, promoting separation of concerns.\n\n## ENTRY POINTS:\n- `api.py:result()`: Main entry point for retrieving and processing PDFs with search results\n- `api.py:print_test_api()`: Entry point for checking API server availability\n- `render_result.py:main()`: Command-line entry point for the visualization functionality\n\n## DATA FLOW:\n1. Client requests arrive via API endpoints\n2. Configuration is loaded from JSON files\n3. PDFs are retrieved from AWS S3 storage\n4. Documents are processed using the render_result.py script to highlight equation regions\n5. Processed results are returned to the client or saved for later access\n6. Visualization results are logged to result files\n\n## INTEGRATION:\nThe system integrates with AWS S3 for document storage, uses Flask for web serving, and connects to the file system for reading/writing files. The render_result.py script is called as a subprocess from the API, creating a pipeline for processing mathematical documents and search results.\n\n## DEVELOPMENT PATTERNS:\n- Separation of concerns between API handling and visualization functionality\n- Command-line utilities that can be called programmatically or independently\n- External configuration via JSON files\n- Subprocess execution for complex processing tasks\n- Logging of operations for debugging and diagnostics\n\n## RELATIONSHIPS:\nThe API component (api.py) orchestrates the overall workflow, calling the visualization component (render_result.py) to process PDFs and highlight equation regions. Both components work together to provide a complete pipeline from search results to highlighted document presentation for the frontend client.",
    "metadata": {
      "type": "DIRECTORY",
      "name": "web",
      "path": "../mathsearch/front-end/web",
      "code": "",
      "summary": "# DIRECTORY: web\n\n## PURPOSE:\nThis directory implements a web-based backend system for the MathSearch application that processes mathematical PDFs, visualizes equation search results, and provides API endpoints for retrieving and manipulating documents with highlighted regions.\n\n## COMPONENT STRUCTURE:\n- **api.py**: Implements backend API endpoints for PDF retrieval, processing, and serving, with functions for handling search results and system status.\n- **render_result.py**: Provides visualization functionality to draw bounding boxes around detected equations in PDF documents, serving as a visualization tool for the search system.\n\n## ARCHITECTURE:\nThe directory follows a service-oriented architecture with separate components for API handling and result visualization. The API component provides RESTful endpoints while the rendering component implements command-line functionality for PDF processing. Configuration data is externalized in JSON files, promoting separation of concerns.\n\n## ENTRY POINTS:\n- `api.py:result()`: Main entry point for retrieving and processing PDFs with search results\n- `api.py:print_test_api()`: Entry point for checking API server availability\n- `render_result.py:main()`: Command-line entry point for the visualization functionality\n\n## DATA FLOW:\n1. Client requests arrive via API endpoints\n2. Configuration is loaded from JSON files\n3. PDFs are retrieved from AWS S3 storage\n4. Documents are processed using the render_result.py script to highlight equation regions\n5. Processed results are returned to the client or saved for later access\n6. Visualization results are logged to result files\n\n## INTEGRATION:\nThe system integrates with AWS S3 for document storage, uses Flask for web serving, and connects to the file system for reading/writing files. The render_result.py script is called as a subprocess from the API, creating a pipeline for processing mathematical documents and search results.\n\n## DEVELOPMENT PATTERNS:\n- Separation of concerns between API handling and visualization functionality\n- Command-line utilities that can be called programmatically or independently\n- External configuration via JSON files\n- Subprocess execution for complex processing tasks\n- Logging of operations for debugging and diagnostics\n\n## RELATIONSHIPS:\nThe API component (api.py) orchestrates the overall workflow, calling the visualization component (render_result.py) to process PDFs and highlight equation regions. Both components work together to provide a complete pipeline from search results to highlighted document presentation for the frontend client."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/web/test",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/web/img_in",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/web/info",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: api.py\n\n## OVERVIEW:\nThis file implements a backend API for the MathSearch application, providing endpoints for PDF retrieval, processing, and manipulation with a focus on handling search results that highlight specific regions in documents.\n\n## KEY COMPONENTS:\n- `result()`: Retrieves PDFs from AWS S3, processes them with coordinate information, and generates highlighted results\n- `example_response()`: Serves PDF files to clients using paths from a configuration file\n- `response_pages()`: Retrieves page data from a JSON configuration file\n- `result_error()`: Returns a standard error message for model execution failures\n- `print_test_api()`: Simple endpoint that confirms API server availability\n- `print_test()`: Diagnostic function that prints and returns operational status\n\n## ARCHITECTURE:\nThe file follows a service-oriented architecture pattern with distinct endpoint handlers that each perform specific tasks related to PDF processing and retrieval. Configuration is externalized in JSON files, and processing utilizes external scripts.\n\n## DATA FLOW:\n1. Incoming requests are handled by appropriate endpoint functions\n2. Configuration data is read from JSON files \n3. For PDF processing, files are retrieved from AWS S3\n4. External scripts (like render_result.py) are called to process PDFs with coordinates\n5. Processed results are returned to the client or saved for later access\n\n## INTEGRATION POINTS:\n- AWS S3 for PDF storage and retrieval\n- External Python script (render_result.py) for PDF processing\n- JSON configuration files for storing settings and paths\n- Implied Flask framework for web routing and file serving\n\n## USAGE PATTERNS:\n- Retrieving and serving PDF documents to frontend clients\n- Processing search results by highlighting regions in PDFs based on coordinate data\n- Health checking the API server through test endpoints\n- Accessing page configuration data for the application\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3 operations\n- subprocess: For executing external Python scripts\n- json: For parsing configuration files\n- Flask framework (implied by send_file usage)\n- File system access for reading/writing files\n\n## RELATIONSHIPS:\nThe functions work together as part of a document processing pipeline where PDFs are retrieved, processed with search results, and served to users. Test functions ensure service availability while configuration functions provide centralized access to application settings.",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "api.py",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "from flask import Flask, request, make_response, send_file\nimport json\nfrom flask_cors import CORS, cross_origin\nimport urllib.request\nimport requests\nimport subprocess\nimport time\nimport boto3\nimport json\n\n\"\"\"\nDeploy flask. See https://github.com/CornellDataScience/MathSearch/blob/front-end/front-end/README.md\n#! Be sure to add api/ prefix for flask deployment, root for react app only\n\n\"\"\"\n\napp = Flask(__name__)\nCORS(app)\n# app = Flask(__name__, static_folder='../mathsearch/build', static_url_path='/')\n\n# curl -i -X GET -H \"Content-Type: application/json\" -d \"{\\\"file\\\":\\\"012330fd-7c87-4236-8f4c-b39f3ea72968_pdf\\\",\\\"coords\\\":\\\"0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889\\\"}\" http://localhost:8001/api/result\n\n# for frontend react to retrieve result from saved frontend EC2\n# waits for backend to finish running model\n@app.route('/api/result')\ndef result():\n\tprint(\"getting result from backend!\")\n\t# start = time.time()\n\tdata = request.json\n\tfilename = data[\"file\"]\n\tcoords = data[\"coords\"]\n\tvenv_py = \"/home/ubuntu/MathSearch/front-end/venv/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/front-end/web/render_result.py\"\n\tinfo = \"-f \" + filename + \" -c \" + coords\n\tcoords_lst = coords.split()\n\tpage_lst = []\n\n\tMATHSEARCH_BUCKET='mathsearch-intermediary'\n\tlocal_pdf = \"/home/ubuntu/MathSearch/front-end/web/pdf_in/\" + filename\n\ts3 = boto3.client(\"s3\")\n\ts3.download_file(\n        Bucket=MATHSEARCH_BUCKET, Key=\"inputs/\"+filename, Filename=local_pdf\n    )\n\tsubprocess.call([venv_py, python_file, info])\n\tfor i in range(0,len(coords_lst),5):\n\t\tpage_lst.append(int(coords_lst[i]))\n\t# json = {\n\t# \t\"pdf\":filename,\n\t# \t\"pages\":page_lst\n\t# }\n\t# end = time.time()\n\n\tinfo = {\n\t\t\"pdf\": \"/home/ubuntu/MathSearch/front-end/web/pdf_out/\"+filename[:-4]+\".pdf\",\n\t\t\"pages\": page_lst\n\t}\n\tINFO_PATH = \"/home/ubuntu/MathSearch/front-end/web/info/info.json\"\n\t# with open(INFO_PATH,\"w\") as json_file:\n\t# \tjson.dump(info, json_file, indent=4, separators=(\",\",\":\"))\n\tprint(\"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\")\n\treturn \"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\"\n\n\t# response = make_response(send_file(f'pdf_out/ex1.pdf'))\n\t# response.headers['pages'] = page_lst\n\t# return response\n\n\t#? broken code\n\t# with open('/home/ubuntu/MathSearch/front-end/web/pdf_out/'+filename, 'rb') as f:\n\t# \tpdf = f.read()\n\t# # pdf = \"temp\"\n\t# response_body = {\n\t# \t\"pdf\": pdf,\n\t# \t\"pages\": page_lst\n\t# }\n\t# print(response_body)\n\t# response = make_response(response_body)\n\t# response.headers['Content-Type'] = 'application/json'\n\t# time_str = \"PDF saved! Time used: \" + str(end - start)\n\t# return info+\"\\nresult page\\n\"+filename+\"\\n\"+time_str+\"\\n\"\n\n# example\n@app.route(\"/api/response_pdf\")\ndef example_response():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpdf_file = data['pdf']\n\treturn send_file(pdf_file, mimetype='application/pdf')\n\t# pdf_file = 'pdf_out/ex1.pdf'\n\t# with open('pdf_out/ex1.pdf', 'rb') as f:\n\t# \tpdf = f.read()\n\tpages = [1, 2, 56]\n\t# response_body = {\n\t# \t\"pdf\": pdf,\n\t# \t\"pages\": pages\n\t# }\n\t# response = make_response(response_body)\n\t# response.headers['Content-Type'] = 'application/json'\n\t# return response\n\t# response = make_response(send_file(pdf_file))\n\t# response.headers['pages'] = pages\n\t# return response\n\treturn send_file(pdf_file, mimetype='application/pdf')\n\n@app.route(\"/api/response_pages\")\ndef response_pages():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpages = data['pages']\n\treturn pages\n\n@app.route(\"/api/error\")\ndef result_error():\n\treturn \"Error occurred during running of the model\"\n\n@app.route('/api/test')\ndef print_test_api():\n\treturn \"yesssssss the site is up - api/test\\n\"\n\n@app.route('/test')\ndef print_test():\n\tprint(\"called test\")\n\treturn \"yesss! the site is up - update - debug - /test\\n\"\n\n# not needed, handled by nginx now\n# @app.route('/')\n# def index():\n#     return app.send_static_file('index.html')\n\nif __name__ == \"__main__\":\n\tapp.run(debug=True)",
      "summary": "# FILE: api.py\n\n## OVERVIEW:\nThis file implements a backend API for the MathSearch application, providing endpoints for PDF retrieval, processing, and manipulation with a focus on handling search results that highlight specific regions in documents.\n\n## KEY COMPONENTS:\n- `result()`: Retrieves PDFs from AWS S3, processes them with coordinate information, and generates highlighted results\n- `example_response()`: Serves PDF files to clients using paths from a configuration file\n- `response_pages()`: Retrieves page data from a JSON configuration file\n- `result_error()`: Returns a standard error message for model execution failures\n- `print_test_api()`: Simple endpoint that confirms API server availability\n- `print_test()`: Diagnostic function that prints and returns operational status\n\n## ARCHITECTURE:\nThe file follows a service-oriented architecture pattern with distinct endpoint handlers that each perform specific tasks related to PDF processing and retrieval. Configuration is externalized in JSON files, and processing utilizes external scripts.\n\n## DATA FLOW:\n1. Incoming requests are handled by appropriate endpoint functions\n2. Configuration data is read from JSON files \n3. For PDF processing, files are retrieved from AWS S3\n4. External scripts (like render_result.py) are called to process PDFs with coordinates\n5. Processed results are returned to the client or saved for later access\n\n## INTEGRATION POINTS:\n- AWS S3 for PDF storage and retrieval\n- External Python script (render_result.py) for PDF processing\n- JSON configuration files for storing settings and paths\n- Implied Flask framework for web routing and file serving\n\n## USAGE PATTERNS:\n- Retrieving and serving PDF documents to frontend clients\n- Processing search results by highlighting regions in PDFs based on coordinate data\n- Health checking the API server through test endpoints\n- Accessing page configuration data for the application\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3 operations\n- subprocess: For executing external Python scripts\n- json: For parsing configuration files\n- Flask framework (implied by send_file usage)\n- File system access for reading/writing files\n\n## RELATIONSHIPS:\nThe functions work together as part of a document processing pipeline where PDFs are retrieved, processed with search results, and served to users. Test functions ensure service availability while configuration functions provide centralized access to application settings."
    }
  },
  {
    "page_content": "# FUNCTION: result\n\n## PURPOSE:\nRetrieves a PDF file from AWS S3, processes it with coordinate information from a frontend request, and generates a result PDF with highlighted regions. The function serves as a backend endpoint handler for the MathSearch application.\n\n## INPUTS:\n- No direct parameters, but uses JSON data from request object:\n  - `file`: String - filename of the PDF to process\n  - `coords`: String - space-separated list of coordinates for highlighting\n\n## OUTPUTS:\n- String - confirmation message indicating successful processing\n\n## KEY STEPS:\n- Extract JSON data from the request to get filename and coordinates\n- Download the specified PDF from AWS S3 bucket to a local path\n- Call an external Python script (`render_result.py`) to process the PDF using the coordinates\n- Extract page numbers from the coordinates list\n- Return a confirmation message\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3 operations\n- subprocess: For executing external Python script\n- request: For accessing the incoming HTTP request data\n\n## USAGE CONTEXT:\nUsed as an API endpoint handler in the MathSearch application to process search results and highlight regions of interest in PDF documents in response to user queries.\n\n## EDGE CASES:\n- No explicit error handling for failed S3 downloads or subprocess calls\n- No validation of input parameters before processing\n- Assumes the request JSON contains the expected fields\n\n## RELATIONSHIPS:\n- Interacts with AWS S3 storage for retrieving PDFs\n- Calls `render_result.py` script for PDF processing\n- Part of the backend processing pipeline for MathSearch\n- Results are saved to a specific location for frontend access\n\ndef result():\n\tprint(\"getting result from backend!\")\n\t# start = time.time()\n\tdata = request.json\n\tfilename = data[\"file\"]\n\tcoords = data[\"coords\"]\n\tvenv_py = \"/home/ubuntu/MathSearch/front-end/venv/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/front-end/web/render_result.py\"\n\tinfo = \"-f \" + filename + \" -c \" + coords\n\tcoords_lst = coords.split()\n\tpage_lst = []\n\n\tMATHSEARCH_BUCKET='mathsearch-intermediary'\n\tlocal_pdf = \"/home/ubuntu/MathSearch/front-end/web/pdf_in/\" + filename\n\ts3 = boto3.client(\"s3\")\n\ts3.download_file(\n        Bucket=MATHSEARCH_BUCKET, Key=\"inputs/\"+filename, Filename=local_pdf\n    )\n\tsubprocess.call([venv_py, python_file, info])\n\tfor i in range(0,len(coords_lst),5):\n\t\tpage_lst.append(int(coords_lst[i]))\n\t# json = {\n\t# \t\"pdf\":filename,\n\t# \t\"pages\":page_lst\n\t# }\n\t# end = time.time()\n\n\tinfo = {\n\t\t\"pdf\": \"/home/ubuntu/MathSearch/front-end/web/pdf_out/\"+filename[:-4]+\".pdf\",\n\t\t\"pages\": page_lst\n\t}\n\tINFO_PATH = \"/home/ubuntu/MathSearch/front-end/web/info/info.json\"\n\t# with open(INFO_PATH,\"w\") as json_file:\n\t# \tjson.dump(info, json_file, indent=4, separators=(\",\",\":\"))\n\tprint(\"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\")\n\treturn \"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "result",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def result():\n\tprint(\"getting result from backend!\")\n\t# start = time.time()\n\tdata = request.json\n\tfilename = data[\"file\"]\n\tcoords = data[\"coords\"]\n\tvenv_py = \"/home/ubuntu/MathSearch/front-end/venv/bin/python3\"\n\tpython_file = \"/home/ubuntu/MathSearch/front-end/web/render_result.py\"\n\tinfo = \"-f \" + filename + \" -c \" + coords\n\tcoords_lst = coords.split()\n\tpage_lst = []\n\n\tMATHSEARCH_BUCKET='mathsearch-intermediary'\n\tlocal_pdf = \"/home/ubuntu/MathSearch/front-end/web/pdf_in/\" + filename\n\ts3 = boto3.client(\"s3\")\n\ts3.download_file(\n        Bucket=MATHSEARCH_BUCKET, Key=\"inputs/\"+filename, Filename=local_pdf\n    )\n\tsubprocess.call([venv_py, python_file, info])\n\tfor i in range(0,len(coords_lst),5):\n\t\tpage_lst.append(int(coords_lst[i]))\n\t# json = {\n\t# \t\"pdf\":filename,\n\t# \t\"pages\":page_lst\n\t# }\n\t# end = time.time()\n\n\tinfo = {\n\t\t\"pdf\": \"/home/ubuntu/MathSearch/front-end/web/pdf_out/\"+filename[:-4]+\".pdf\",\n\t\t\"pages\": page_lst\n\t}\n\tINFO_PATH = \"/home/ubuntu/MathSearch/front-end/web/info/info.json\"\n\t# with open(INFO_PATH,\"w\") as json_file:\n\t# \tjson.dump(info, json_file, indent=4, separators=(\",\",\":\"))\n\tprint(\"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\")\n\treturn \"got result from backend! saved to /home/ubuntu/MathSearch/front-end/web/info/info.json\"",
      "summary": "# FUNCTION: result\n\n## PURPOSE:\nRetrieves a PDF file from AWS S3, processes it with coordinate information from a frontend request, and generates a result PDF with highlighted regions. The function serves as a backend endpoint handler for the MathSearch application.\n\n## INPUTS:\n- No direct parameters, but uses JSON data from request object:\n  - `file`: String - filename of the PDF to process\n  - `coords`: String - space-separated list of coordinates for highlighting\n\n## OUTPUTS:\n- String - confirmation message indicating successful processing\n\n## KEY STEPS:\n- Extract JSON data from the request to get filename and coordinates\n- Download the specified PDF from AWS S3 bucket to a local path\n- Call an external Python script (`render_result.py`) to process the PDF using the coordinates\n- Extract page numbers from the coordinates list\n- Return a confirmation message\n\n## DEPENDENCIES:\n- boto3: AWS SDK for S3 operations\n- subprocess: For executing external Python script\n- request: For accessing the incoming HTTP request data\n\n## USAGE CONTEXT:\nUsed as an API endpoint handler in the MathSearch application to process search results and highlight regions of interest in PDF documents in response to user queries.\n\n## EDGE CASES:\n- No explicit error handling for failed S3 downloads or subprocess calls\n- No validation of input parameters before processing\n- Assumes the request JSON contains the expected fields\n\n## RELATIONSHIPS:\n- Interacts with AWS S3 storage for retrieving PDFs\n- Calls `render_result.py` script for PDF processing\n- Part of the backend processing pipeline for MathSearch\n- Results are saved to a specific location for frontend access"
    }
  },
  {
    "page_content": "# FUNCTION: example_response\n\n## PURPOSE:\nServes a PDF file to the client by reading its path from a JSON configuration file. It provides an endpoint for PDF document retrieval in a web application.\n\n## INPUTS:\nNone - The function does not accept any parameters.\n\n## OUTPUTS:\n- `send_file` response object (Flask) - Returns the PDF file with appropriate MIME type for browser rendering.\n\n## KEY STEPS:\n- Opens and reads the 'info/info.json' configuration file\n- Parses the JSON data to extract the PDF file path\n- Returns the PDF file using Flask's send_file function with PDF MIME type\n\n## DEPENDENCIES:\n- json - For parsing the JSON configuration file\n- send_file - Flask function for serving files to clients\n- (implied) Flask - Web framework providing the send_file functionality\n\n## USAGE CONTEXT:\nUsed as a route handler in a Flask web application to serve PDF files when clients request them, likely as part of a document management or viewing system.\n\n## EDGE CASES:\n- No error handling for missing JSON file or PDF file\n- Commented code suggests this function was previously designed to handle specific page selections (variable 'pages' is defined but unused)\n- No validation of the PDF path from the configuration file\n\n## RELATIONSHIPS:\n- Likely part of a document viewing API endpoint\n- Related to PDF processing functionality in the larger application\n- Configuration is externalized in 'info/info.json'\n\ndef example_response():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpdf_file = data['pdf']\n\treturn send_file(pdf_file, mimetype='application/pdf')\n\t# pdf_file = 'pdf_out/ex1.pdf'\n\t# with open('pdf_out/ex1.pdf', 'rb') as f:\n\t# \tpdf = f.read()\n\tpages = [1, 2, 56]\n\t# response_body = {\n\t# \t\"pdf\": pdf,\n\t# \t\"pages\": pages\n\t# }\n\t# response = make_response(response_body)\n\t# response.headers['Content-Type'] = 'application/json'\n\t# return response\n\t# response = make_response(send_file(pdf_file))\n\t# response.headers['pages'] = pages\n\t# return response\n\treturn send_file(pdf_file, mimetype='application/pdf')",
    "metadata": {
      "type": "FUNCTION",
      "name": "example_response",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def example_response():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpdf_file = data['pdf']\n\treturn send_file(pdf_file, mimetype='application/pdf')\n\t# pdf_file = 'pdf_out/ex1.pdf'\n\t# with open('pdf_out/ex1.pdf', 'rb') as f:\n\t# \tpdf = f.read()\n\tpages = [1, 2, 56]\n\t# response_body = {\n\t# \t\"pdf\": pdf,\n\t# \t\"pages\": pages\n\t# }\n\t# response = make_response(response_body)\n\t# response.headers['Content-Type'] = 'application/json'\n\t# return response\n\t# response = make_response(send_file(pdf_file))\n\t# response.headers['pages'] = pages\n\t# return response\n\treturn send_file(pdf_file, mimetype='application/pdf')",
      "summary": "# FUNCTION: example_response\n\n## PURPOSE:\nServes a PDF file to the client by reading its path from a JSON configuration file. It provides an endpoint for PDF document retrieval in a web application.\n\n## INPUTS:\nNone - The function does not accept any parameters.\n\n## OUTPUTS:\n- `send_file` response object (Flask) - Returns the PDF file with appropriate MIME type for browser rendering.\n\n## KEY STEPS:\n- Opens and reads the 'info/info.json' configuration file\n- Parses the JSON data to extract the PDF file path\n- Returns the PDF file using Flask's send_file function with PDF MIME type\n\n## DEPENDENCIES:\n- json - For parsing the JSON configuration file\n- send_file - Flask function for serving files to clients\n- (implied) Flask - Web framework providing the send_file functionality\n\n## USAGE CONTEXT:\nUsed as a route handler in a Flask web application to serve PDF files when clients request them, likely as part of a document management or viewing system.\n\n## EDGE CASES:\n- No error handling for missing JSON file or PDF file\n- Commented code suggests this function was previously designed to handle specific page selections (variable 'pages' is defined but unused)\n- No validation of the PDF path from the configuration file\n\n## RELATIONSHIPS:\n- Likely part of a document viewing API endpoint\n- Related to PDF processing functionality in the larger application\n- Configuration is externalized in 'info/info.json'"
    }
  },
  {
    "page_content": "# FUNCTION: response_pages\n\n## PURPOSE:\nRetrieves the 'pages' data from a JSON configuration file. This function provides access to page information stored in the 'info.json' file.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- `pages` (list or dictionary): The 'pages' data structure from the info.json file\n\n## KEY STEPS:\n- Open the 'info/info.json' file for reading\n- Read the file content and parse it as JSON\n- Extract the 'pages' element from the parsed JSON data\n- Return the pages data\n\n## DEPENDENCIES:\n- json: Standard library module for JSON parsing\n- File system access for reading the info.json file\n\n## USAGE CONTEXT:\nThis function is likely used when the application needs to know about page configuration, possibly for routing, navigation, or to display information about available pages.\n\n## EDGE CASES:\n- May raise exceptions if the file doesn't exist or has invalid permissions\n- May raise a KeyError if the 'pages' key doesn't exist in the JSON data\n- Doesn't handle JSON parsing errors explicitly\n\n## RELATIONSHIPS:\nLikely part of a configuration management system that provides page metadata to other parts of the application.\n\ndef response_pages():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpages = data['pages']\n\treturn pages",
    "metadata": {
      "type": "FUNCTION",
      "name": "response_pages",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def response_pages():\n\tf = open ('info/info.json', \"r\")\n\tdata = json.loads(f.read())\n\tpages = data['pages']\n\treturn pages",
      "summary": "# FUNCTION: response_pages\n\n## PURPOSE:\nRetrieves the 'pages' data from a JSON configuration file. This function provides access to page information stored in the 'info.json' file.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- `pages` (list or dictionary): The 'pages' data structure from the info.json file\n\n## KEY STEPS:\n- Open the 'info/info.json' file for reading\n- Read the file content and parse it as JSON\n- Extract the 'pages' element from the parsed JSON data\n- Return the pages data\n\n## DEPENDENCIES:\n- json: Standard library module for JSON parsing\n- File system access for reading the info.json file\n\n## USAGE CONTEXT:\nThis function is likely used when the application needs to know about page configuration, possibly for routing, navigation, or to display information about available pages.\n\n## EDGE CASES:\n- May raise exceptions if the file doesn't exist or has invalid permissions\n- May raise a KeyError if the 'pages' key doesn't exist in the JSON data\n- Doesn't handle JSON parsing errors explicitly\n\n## RELATIONSHIPS:\nLikely part of a configuration management system that provides page metadata to other parts of the application."
    }
  },
  {
    "page_content": "# Standardized Function Summary\n\nFUNCTION: result_error\n\nPURPOSE:\nReturns a standard error message indicating a problem occurred during model execution. This function provides a consistent error message for handling model runtime failures.\n\nINPUTS:\nNone\n\nOUTPUTS:\nString - A fixed error message stating \"Error occurred during running of the model\"\n\nKEY STEPS:\n- Return a predefined error message string\n\nDEPENDENCIES:\nNone\n\nUSAGE CONTEXT:\nThis function is typically called when exception handling detects a failure during model execution, providing a standardized message that can be displayed to users or logged for troubleshooting.\n\nEDGE CASES:\nNone, as this is a simple string return function with no parameters or conditional logic.\n\nRELATIONSHIPS:\nLikely part of an error handling system for a model execution pipeline, working alongside other error reporting or logging functions.\n\ndef result_error():\n\treturn \"Error occurred during running of the model\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "result_error",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def result_error():\n\treturn \"Error occurred during running of the model\"",
      "summary": "# Standardized Function Summary\n\nFUNCTION: result_error\n\nPURPOSE:\nReturns a standard error message indicating a problem occurred during model execution. This function provides a consistent error message for handling model runtime failures.\n\nINPUTS:\nNone\n\nOUTPUTS:\nString - A fixed error message stating \"Error occurred during running of the model\"\n\nKEY STEPS:\n- Return a predefined error message string\n\nDEPENDENCIES:\nNone\n\nUSAGE CONTEXT:\nThis function is typically called when exception handling detects a failure during model execution, providing a standardized message that can be displayed to users or logged for troubleshooting.\n\nEDGE CASES:\nNone, as this is a simple string return function with no parameters or conditional logic.\n\nRELATIONSHIPS:\nLikely part of an error handling system for a model execution pipeline, working alongside other error reporting or logging functions."
    }
  },
  {
    "page_content": "# FUNCTION: print_test_api\n\n## PURPOSE:\nA simple test function that returns a confirmation message to verify that the API server is running properly.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- string: Returns the text \"yesssssss the site is up - api/test\\n\" to confirm API endpoint availability\n\n## KEY STEPS:\n- Returns a hardcoded string confirmation message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nThis function is typically used as a health check endpoint in an API to verify that the server is operational. It can be called by monitoring tools or during development to confirm the API is responding.\n\n## EDGE CASES:\nNone - this is a simple function with no conditional logic or error handling\n\n## RELATIONSHIPS:\nLikely part of an API routing system where it would be mapped to a \"test\" or \"health check\" endpoint, possibly used by monitoring systems to verify service availability\n\ndef print_test_api():\n\treturn \"yesssssss the site is up - api/test\\n\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "print_test_api",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def print_test_api():\n\treturn \"yesssssss the site is up - api/test\\n\"",
      "summary": "# FUNCTION: print_test_api\n\n## PURPOSE:\nA simple test function that returns a confirmation message to verify that the API server is running properly.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- string: Returns the text \"yesssssss the site is up - api/test\\n\" to confirm API endpoint availability\n\n## KEY STEPS:\n- Returns a hardcoded string confirmation message\n\n## DEPENDENCIES:\nNone\n\n## USAGE CONTEXT:\nThis function is typically used as a health check endpoint in an API to verify that the server is operational. It can be called by monitoring tools or during development to confirm the API is responding.\n\n## EDGE CASES:\nNone - this is a simple function with no conditional logic or error handling\n\n## RELATIONSHIPS:\nLikely part of an API routing system where it would be mapped to a \"test\" or \"health check\" endpoint, possibly used by monitoring systems to verify service availability"
    }
  },
  {
    "page_content": "# FUNCTION: print_test\n\n## PURPOSE:\nA simple diagnostic function that prints a status message and returns a confirmation string indicating the site is operational.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: Returns the message \"yesss! the site is up - update - debug - /test\\n\" confirming the site is running.\n\n## KEY STEPS:\n- Prints the message \"called test\" to the console/log\n- Returns a string confirming the site's operational status\n\n## DEPENDENCIES:\n- Python's built-in print function\n\n## USAGE CONTEXT:\nTypically used as a simple endpoint handler or health check to verify that a web application or service is running properly. The \"/test\" in the return string suggests it may be mapped to a \"/test\" route in a web framework.\n\n## EDGE CASES:\nNone significant; this is a simple function with no input validation or error handling needed.\n\n## RELATIONSHIPS:\nLikely part of a web application's diagnostic or debugging toolset, possibly triggered by visiting a test endpoint to confirm the application is responding.\n\ndef print_test():\n\tprint(\"called test\")\n\treturn \"yesss! the site is up - update - debug - /test\\n\"",
    "metadata": {
      "type": "FUNCTION",
      "name": "print_test",
      "path": "../mathsearch/front-end/web/api.py",
      "code": "def print_test():\n\tprint(\"called test\")\n\treturn \"yesss! the site is up - update - debug - /test\\n\"",
      "summary": "# FUNCTION: print_test\n\n## PURPOSE:\nA simple diagnostic function that prints a status message and returns a confirmation string indicating the site is operational.\n\n## INPUTS:\nNone\n\n## OUTPUTS:\n- String: Returns the message \"yesss! the site is up - update - debug - /test\\n\" confirming the site is running.\n\n## KEY STEPS:\n- Prints the message \"called test\" to the console/log\n- Returns a string confirming the site's operational status\n\n## DEPENDENCIES:\n- Python's built-in print function\n\n## USAGE CONTEXT:\nTypically used as a simple endpoint handler or health check to verify that a web application or service is running properly. The \"/test\" in the return string suggests it may be mapped to a \"/test\" route in a web framework.\n\n## EDGE CASES:\nNone significant; this is a simple function with no input validation or error handling needed.\n\n## RELATIONSHIPS:\nLikely part of a web application's diagnostic or debugging toolset, possibly triggered by visiting a test endpoint to confirm the application is responding."
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/web/pdf_in",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "# FILE: render_result.py\n\n## OVERVIEW:\nThis file implements functionality to visualize equation detection results by drawing bounding boxes on PDF documents, primarily serving as a visualization tool within a math search system.\n\n## KEY COMPONENTS:\n- `draw_bounding_box`: Draws a visual bounding box on an image to highlight detected objects or regions of interest\n- `main`: Orchestrates the entire process of extracting PDF pages, drawing equation bounding boxes, and creating annotated versions of PDF documents\n\n## ARCHITECTURE:\nThe file follows a simple two-function architecture: a utility function (`draw_bounding_box`) that handles the core visualization task, and a controller function (`main`) that manages the workflow from command-line input to final output generation.\n\n## DATA FLOW:\n1. Command-line arguments flow into the `main` function, providing the PDF filename and bounding box coordinates\n2. PDF pages are converted to images\n3. Images and coordinates flow to the `draw_bounding_box` function\n4. Annotated images are converted back to PDF format\n5. A final PDF is created combining original and annotated pages\n6. Operation details are logged to a result log file\n\n## INTEGRATION POINTS:\n- Interfaces with command line via `argparse` for receiving input parameters\n- Connects to the file system for reading PDFs and writing annotated outputs\n- Integrates with a broader math search system as indicated by the log file naming\n\n## USAGE PATTERNS:\n- Run as a command-line tool with arguments specifying a PDF file and equation coordinates\n- Used to visualize the output of equation detection algorithms for debugging or result presentation\n- Part of a processing pipeline for mathematical document analysis\n\n## DEPENDENCIES:\n- External: OpenCV (cv2), argparse, pdf2image, PyPDF2, PIL (Image), time\n- Internal: The `draw_bounding_box` function used by `main`\n\n## RELATIONSHIPS:\n- `main` serves as the controller that orchestrates the entire annotation process\n- `draw_bounding_box` provides the core visualization functionality\n- Together they form a pipeline for converting detection results into human-readable visual annotations",
    "metadata": {
      "type": "PYTHON_FILE",
      "name": "render_result.py",
      "path": "../mathsearch/front-end/web/render_result.py",
      "code": "import cv2\nimport sys\nimport argparse\nimport PyPDF2\nimport pdf2image\nfrom PIL import Image\nimport json\nimport time\n\n'''\nTakes in two parameters: -f filename -c coordinates\nSave pdf [filename.pdf] with bounding boxes and the result page list [filename.json] to dir pdf_out\n\n@ Param:\n-f filename.pdf\n-c in (regex) syntax of: [page_number x y w h]+\n\n@ Example:\npython3 render_result.py -f ex1.pdf -c 0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889\n\n@ Author: Emerald\n\n'''\t\n\n\n# page.scale_to(width: float, height: float)\u2192 None\n\n#? Code that pdf -> png -> pdf\ndef draw_bounding_box(image_path_in, bounding_box, image_path_out):\n\timage = cv2.imread(image_path_in)\n\theight, width, _ = image.shape\n\tx, y, w, h = bounding_box\n\tx1 = int((x - w/2) * width)\n\ty1 = int((y - h/2) * height)\n\tx2 = int((x + w/2) * width)\n\ty2 = int((y + h/2) * height)\n\tupper_left = (x1, y1)\n\tbottom_right = (x2, y2)\n\n\tRED = (0,0,255)\n\tBLUE = (255,0,0)\n\tGREEN = (0,255,0)\n\tSKYBLUE = (255,191,0)\n\n\t# note cv2 uses BGR color instead of RGB\n\tcv2.rectangle(image, upper_left, bottom_right, SKYBLUE, 3)\n\tcv2.imwrite(image_path_out, image)\ndef main(argv):\n\tif len(argv)==1:\n\t\targv = argv[0].split()\n\t# print(type(argv),type(argv[0]),argv)\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\tprog='render_results.py',\n\t\t\t\t\tdescription='render yolov5 equation bonding box on image',\n\t\t\t\t\tepilog='Example usage: \\npython3 render_result.py -f ex1.pdf -c 0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889')\n\t\n\tparser.add_argument('-f','--file', help='pdf file name', required=True)\n\tparser.add_argument('-c','--coordinates', nargs='+', help='bounding box coordinates', required=True)\n\n\tIMG_IN_DIR = \"img_in/\"\n\tIMG_OUT_DIR = \"img_out/\"\n\tPDF_IN_DIR = \"pdf_in/\"\n\tPDF_OUT_DIR = \"pdf_out/\"\n\n\tpdf_name = parser.parse_args(argv).file\n\tpdf_in = PDF_IN_DIR + pdf_name\n\tpdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n\tpdf_no_ext = pdf_name[:-4]\n\n\tbounding_boxes = [float(x) for x in parser.parse_args(argv).coordinates]\n\t\n\tif(len(bounding_boxes) % 5 != 0):\n\t\tprint(\"Invalid number of coordinates, must be multiple of 5\")\n\t\treturn\n\t\n\ttable = {}\n\tfor i in range(0, len(bounding_boxes), 5):\n\t\ttable[int(bounding_boxes[i])] = bounding_boxes[i+1:i+5]\n\tresult_pages = list(table.keys())\n\n\t# Done 1: get result list, convert need box page in the pdf to png, save to /img_in\n\t# Done 2: call draw_bounding_boxes for each png, save to /img_out\n\timages = pdf2image.convert_from_path(pdf_in)\n\tfor i in result_pages:\n\t\timage_path_in = IMG_IN_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\timages[i].save(image_path_in)\n\t\timage_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\tdraw_bounding_box(image_path_in,table[i],image_path_out)\n\t\t# save img as pdf\n\t\timage = Image.open(image_path_out).convert('RGB')\n\t\timage.save(image_path_out[:-4]+\".pdf\")\n\t\n\t# Done 3: merge the rendered images to the pdf, save to /pdf_out\n\twith open(pdf_in, 'rb') as file:\n\t\twith open(pdf_out, 'wb') as pdf_out:\n\t\t\tpdf = PyPDF2.PdfReader(file)\n\t\t\toutput = PyPDF2.PdfWriter()\n\t\t\tfor i, page in enumerate(pdf.pages):\n\t\t\t\tif i in result_pages:\n\t\t\t\t\tnew_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n\t\t\t\t\tnew_page.scale_by(0.36)\n\t\t\t\t\toutput.add_page(new_page)\n\t\t\t\telse:\n\t\t\t\t\toutput.add_page(page)\n\t\t\toutput.write(pdf_out)\n\t\n\t# Done 4: save the result list to json file\n\t# result_pages_json = PDF_OUT_DIR + pdf_no_ext+\".json\"\n\t# with open(result_pages_json,'w') as file:\n\t# \tjson.dump(result_pages, file, indent=4, separators=(\",\", \":\"))\n\t\n\twith open(\"/home/ubuntu/MathSearch/front-end/web/result_log\",\"a\") as file:\n\t\targv_str = \" \".join(str(x) for x in argv)\n\t\tprint(argv_str)\n\t\tfile.write(time.strftime(\"%H:%M:%S\", time.localtime()) + \"\\t\" + argv_str)\n\t\tfile.write(\"\\n\")\n\n\n# python3 render_result.py -f ex1.pdf -c 0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889\nif __name__ == \"__main__\":\n\tstart = time.time()\n\tmain(sys.argv[1:])\n\tend = time.time()\n\tprint(\"PDF saved! Time used:\",end - start)\n\n\"\"\"\nbox\n\nx1,y1 -------\n|\t\t\t|\n|\t\t\t|\n--------- x2,y2\n\n--------x2,y1\n|\t\t\t|\n|\t\t\t|\nx1,y2--------\n\n\"\"\"",
      "summary": "# FILE: render_result.py\n\n## OVERVIEW:\nThis file implements functionality to visualize equation detection results by drawing bounding boxes on PDF documents, primarily serving as a visualization tool within a math search system.\n\n## KEY COMPONENTS:\n- `draw_bounding_box`: Draws a visual bounding box on an image to highlight detected objects or regions of interest\n- `main`: Orchestrates the entire process of extracting PDF pages, drawing equation bounding boxes, and creating annotated versions of PDF documents\n\n## ARCHITECTURE:\nThe file follows a simple two-function architecture: a utility function (`draw_bounding_box`) that handles the core visualization task, and a controller function (`main`) that manages the workflow from command-line input to final output generation.\n\n## DATA FLOW:\n1. Command-line arguments flow into the `main` function, providing the PDF filename and bounding box coordinates\n2. PDF pages are converted to images\n3. Images and coordinates flow to the `draw_bounding_box` function\n4. Annotated images are converted back to PDF format\n5. A final PDF is created combining original and annotated pages\n6. Operation details are logged to a result log file\n\n## INTEGRATION POINTS:\n- Interfaces with command line via `argparse` for receiving input parameters\n- Connects to the file system for reading PDFs and writing annotated outputs\n- Integrates with a broader math search system as indicated by the log file naming\n\n## USAGE PATTERNS:\n- Run as a command-line tool with arguments specifying a PDF file and equation coordinates\n- Used to visualize the output of equation detection algorithms for debugging or result presentation\n- Part of a processing pipeline for mathematical document analysis\n\n## DEPENDENCIES:\n- External: OpenCV (cv2), argparse, pdf2image, PyPDF2, PIL (Image), time\n- Internal: The `draw_bounding_box` function used by `main`\n\n## RELATIONSHIPS:\n- `main` serves as the controller that orchestrates the entire annotation process\n- `draw_bounding_box` provides the core visualization functionality\n- Together they form a pipeline for converting detection results into human-readable visual annotations"
    }
  },
  {
    "page_content": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws a bounding box on an image and saves the result to a new file. This function visualizes object detection or region of interest annotations.\n\n## INPUTS:\n- `image_path_in` (string): Path to the input image file\n- `bounding_box` (tuple): Normalized coordinates (x, y, w, h) where x,y is the center and w,h are width and height\n- `image_path_out` (string): Path where the output image with bounding box will be saved\n\n## OUTPUTS:\n- None: The function saves the modified image to the specified output path\n\n## KEY STEPS:\n- Load the image using OpenCV\n- Convert normalized bounding box coordinates to pixel coordinates\n- Calculate top-left and bottom-right corners of the box\n- Draw a sky blue rectangle on the image\n- Save the modified image to the output path\n\n## DEPENDENCIES:\n- cv2 (OpenCV): Used for image loading, manipulation and saving\n\n## USAGE CONTEXT:\n- Typically used in object detection pipelines to visualize detection results\n- Useful for debugging or creating visualization outputs in image processing applications\n\n## EDGE CASES:\n- No explicit error handling for invalid paths or bounding box values\n- Assumes bounding box coordinates are normalized (between 0 and 1)\n\n## RELATIONSHIPS:\n- Likely part of a larger computer vision or image processing system\n- May be used alongside other object detection or image annotation functions\n\ndef draw_bounding_box(image_path_in, bounding_box, image_path_out):\n\timage = cv2.imread(image_path_in)\n\theight, width, _ = image.shape\n\tx, y, w, h = bounding_box\n\tx1 = int((x - w/2) * width)\n\ty1 = int((y - h/2) * height)\n\tx2 = int((x + w/2) * width)\n\ty2 = int((y + h/2) * height)\n\tupper_left = (x1, y1)\n\tbottom_right = (x2, y2)\n\n\tRED = (0,0,255)\n\tBLUE = (255,0,0)\n\tGREEN = (0,255,0)\n\tSKYBLUE = (255,191,0)\n\n\t# note cv2 uses BGR color instead of RGB\n\tcv2.rectangle(image, upper_left, bottom_right, SKYBLUE, 3)\n\tcv2.imwrite(image_path_out, image)",
    "metadata": {
      "type": "FUNCTION",
      "name": "draw_bounding_box",
      "path": "../mathsearch/front-end/web/render_result.py",
      "code": "def draw_bounding_box(image_path_in, bounding_box, image_path_out):\n\timage = cv2.imread(image_path_in)\n\theight, width, _ = image.shape\n\tx, y, w, h = bounding_box\n\tx1 = int((x - w/2) * width)\n\ty1 = int((y - h/2) * height)\n\tx2 = int((x + w/2) * width)\n\ty2 = int((y + h/2) * height)\n\tupper_left = (x1, y1)\n\tbottom_right = (x2, y2)\n\n\tRED = (0,0,255)\n\tBLUE = (255,0,0)\n\tGREEN = (0,255,0)\n\tSKYBLUE = (255,191,0)\n\n\t# note cv2 uses BGR color instead of RGB\n\tcv2.rectangle(image, upper_left, bottom_right, SKYBLUE, 3)\n\tcv2.imwrite(image_path_out, image)",
      "summary": "# FUNCTION: draw_bounding_box\n\n## PURPOSE:\nDraws a bounding box on an image and saves the result to a new file. This function visualizes object detection or region of interest annotations.\n\n## INPUTS:\n- `image_path_in` (string): Path to the input image file\n- `bounding_box` (tuple): Normalized coordinates (x, y, w, h) where x,y is the center and w,h are width and height\n- `image_path_out` (string): Path where the output image with bounding box will be saved\n\n## OUTPUTS:\n- None: The function saves the modified image to the specified output path\n\n## KEY STEPS:\n- Load the image using OpenCV\n- Convert normalized bounding box coordinates to pixel coordinates\n- Calculate top-left and bottom-right corners of the box\n- Draw a sky blue rectangle on the image\n- Save the modified image to the output path\n\n## DEPENDENCIES:\n- cv2 (OpenCV): Used for image loading, manipulation and saving\n\n## USAGE CONTEXT:\n- Typically used in object detection pipelines to visualize detection results\n- Useful for debugging or creating visualization outputs in image processing applications\n\n## EDGE CASES:\n- No explicit error handling for invalid paths or bounding box values\n- Assumes bounding box coordinates are normalized (between 0 and 1)\n\n## RELATIONSHIPS:\n- Likely part of a larger computer vision or image processing system\n- May be used alongside other object detection or image annotation functions"
    }
  },
  {
    "page_content": "# FUNCTION: main\n\n## PURPOSE:\nProcesses PDF files by rendering YOLOv5 equation bounding boxes on specific pages, creating annotated versions of the original PDF with visual highlighting of detected equations.\n\n## INPUTS:\n- `argv` (list): Command line arguments, containing the PDF filename and coordinates for bounding boxes\n\n## OUTPUTS:\n- None (void): The function creates annotated files in output directories but doesn't return a value\n\n## KEY STEPS:\n- Parse command line arguments to extract PDF filename and bounding box coordinates\n- Organize bounding box coordinates into a dictionary by page number\n- Convert specified pages from the PDF to PNG images\n- Draw bounding boxes on these images using the `draw_bounding_box` function\n- Convert annotated images back to PDF\n- Create a new PDF by combining original pages with the annotated ones\n- Log the operation details to a result log file\n\n## DEPENDENCIES:\n- `argparse`: For command line argument parsing\n- `pdf2image`: For converting PDF pages to images\n- `PyPDF2`: For PDF manipulation operations\n- `Image` (from PIL): For image handling\n- `draw_bounding_box`: External function to draw bounding boxes on images\n- `time`: For timestamping log entries\n\n## USAGE CONTEXT:\nUsed as the entry point of a command-line tool for visualizing equation detection results, typically integrated into a math search system for highlighting detected equations in PDF documents.\n\n## EDGE CASES:\n- Validates that the number of bounding box coordinates is a multiple of 5 (page number + 4 coordinates)\n- Will terminate early if the coordinate count is invalid\n\n## RELATIONSHIPS:\n- Acts as the main controller function for the PDF annotation pipeline\n- Interacts with external storage directories for input/output files\n- Part of a larger math search system as indicated by the log file path\n\ndef main(argv):\n\tif len(argv)==1:\n\t\targv = argv[0].split()\n\t# print(type(argv),type(argv[0]),argv)\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\tprog='render_results.py',\n\t\t\t\t\tdescription='render yolov5 equation bonding box on image',\n\t\t\t\t\tepilog='Example usage: \\npython3 render_result.py -f ex1.pdf -c 0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889')\n\t\n\tparser.add_argument('-f','--file', help='pdf file name', required=True)\n\tparser.add_argument('-c','--coordinates', nargs='+', help='bounding box coordinates', required=True)\n\n\tIMG_IN_DIR = \"img_in/\"\n\tIMG_OUT_DIR = \"img_out/\"\n\tPDF_IN_DIR = \"pdf_in/\"\n\tPDF_OUT_DIR = \"pdf_out/\"\n\n\tpdf_name = parser.parse_args(argv).file\n\tpdf_in = PDF_IN_DIR + pdf_name\n\tpdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n\tpdf_no_ext = pdf_name[:-4]\n\n\tbounding_boxes = [float(x) for x in parser.parse_args(argv).coordinates]\n\t\n\tif(len(bounding_boxes) % 5 != 0):\n\t\tprint(\"Invalid number of coordinates, must be multiple of 5\")\n\t\treturn\n\t\n\ttable = {}\n\tfor i in range(0, len(bounding_boxes), 5):\n\t\ttable[int(bounding_boxes[i])] = bounding_boxes[i+1:i+5]\n\tresult_pages = list(table.keys())\n\n\t# Done 1: get result list, convert need box page in the pdf to png, save to /img_in\n\t# Done 2: call draw_bounding_boxes for each png, save to /img_out\n\timages = pdf2image.convert_from_path(pdf_in)\n\tfor i in result_pages:\n\t\timage_path_in = IMG_IN_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\timages[i].save(image_path_in)\n\t\timage_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\tdraw_bounding_box(image_path_in,table[i],image_path_out)\n\t\t# save img as pdf\n\t\timage = Image.open(image_path_out).convert('RGB')\n\t\timage.save(image_path_out[:-4]+\".pdf\")\n\t\n\t# Done 3: merge the rendered images to the pdf, save to /pdf_out\n\twith open(pdf_in, 'rb') as file:\n\t\twith open(pdf_out, 'wb') as pdf_out:\n\t\t\tpdf = PyPDF2.PdfReader(file)\n\t\t\toutput = PyPDF2.PdfWriter()\n\t\t\tfor i, page in enumerate(pdf.pages):\n\t\t\t\tif i in result_pages:\n\t\t\t\t\tnew_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n\t\t\t\t\tnew_page.scale_by(0.36)\n\t\t\t\t\toutput.add_page(new_page)\n\t\t\t\telse:\n\t\t\t\t\toutput.add_page(page)\n\t\t\toutput.write(pdf_out)\n\t\n\t# Done 4: save the result list to json file\n\t# result_pages_json = PDF_OUT_DIR + pdf_no_ext+\".json\"\n\t# with open(result_pages_json,'w') as file:\n\t# \tjson.dump(result_pages, file, indent=4, separators=(\",\", \":\"))\n\t\n\twith open(\"/home/ubuntu/MathSearch/front-end/web/result_log\",\"a\") as file:\n\t\targv_str = \" \".join(str(x) for x in argv)\n\t\tprint(argv_str)\n\t\tfile.write(time.strftime(\"%H:%M:%S\", time.localtime()) + \"\\t\" + argv_str)\n\t\tfile.write(\"\\n\")",
    "metadata": {
      "type": "FUNCTION",
      "name": "main",
      "path": "../mathsearch/front-end/web/render_result.py",
      "code": "def main(argv):\n\tif len(argv)==1:\n\t\targv = argv[0].split()\n\t# print(type(argv),type(argv[0]),argv)\n\tparser = argparse.ArgumentParser(\n\t\t\t\t\tprog='render_results.py',\n\t\t\t\t\tdescription='render yolov5 equation bonding box on image',\n\t\t\t\t\tepilog='Example usage: \\npython3 render_result.py -f ex1.pdf -c 0 0.3392857142857143 0.17142857142857146 0.30952380952380953 0.12698412698412698 1 0.32242063492063494 0.4380952380952381 0.26785714285714285 0.08888888888888889')\n\t\n\tparser.add_argument('-f','--file', help='pdf file name', required=True)\n\tparser.add_argument('-c','--coordinates', nargs='+', help='bounding box coordinates', required=True)\n\n\tIMG_IN_DIR = \"img_in/\"\n\tIMG_OUT_DIR = \"img_out/\"\n\tPDF_IN_DIR = \"pdf_in/\"\n\tPDF_OUT_DIR = \"pdf_out/\"\n\n\tpdf_name = parser.parse_args(argv).file\n\tpdf_in = PDF_IN_DIR + pdf_name\n\tpdf_out = PDF_OUT_DIR + pdf_name[:-4]+\".pdf\"\n\tpdf_no_ext = pdf_name[:-4]\n\n\tbounding_boxes = [float(x) for x in parser.parse_args(argv).coordinates]\n\t\n\tif(len(bounding_boxes) % 5 != 0):\n\t\tprint(\"Invalid number of coordinates, must be multiple of 5\")\n\t\treturn\n\t\n\ttable = {}\n\tfor i in range(0, len(bounding_boxes), 5):\n\t\ttable[int(bounding_boxes[i])] = bounding_boxes[i+1:i+5]\n\tresult_pages = list(table.keys())\n\n\t# Done 1: get result list, convert need box page in the pdf to png, save to /img_in\n\t# Done 2: call draw_bounding_boxes for each png, save to /img_out\n\timages = pdf2image.convert_from_path(pdf_in)\n\tfor i in result_pages:\n\t\timage_path_in = IMG_IN_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\timages[i].save(image_path_in)\n\t\timage_path_out = IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".png\"\n\t\tdraw_bounding_box(image_path_in,table[i],image_path_out)\n\t\t# save img as pdf\n\t\timage = Image.open(image_path_out).convert('RGB')\n\t\timage.save(image_path_out[:-4]+\".pdf\")\n\t\n\t# Done 3: merge the rendered images to the pdf, save to /pdf_out\n\twith open(pdf_in, 'rb') as file:\n\t\twith open(pdf_out, 'wb') as pdf_out:\n\t\t\tpdf = PyPDF2.PdfReader(file)\n\t\t\toutput = PyPDF2.PdfWriter()\n\t\t\tfor i, page in enumerate(pdf.pages):\n\t\t\t\tif i in result_pages:\n\t\t\t\t\tnew_page = PyPDF2.PdfReader(IMG_OUT_DIR + pdf_no_ext + \"_\"+ str(i) + \".pdf\").pages[0]\n\t\t\t\t\tnew_page.scale_by(0.36)\n\t\t\t\t\toutput.add_page(new_page)\n\t\t\t\telse:\n\t\t\t\t\toutput.add_page(page)\n\t\t\toutput.write(pdf_out)\n\t\n\t# Done 4: save the result list to json file\n\t# result_pages_json = PDF_OUT_DIR + pdf_no_ext+\".json\"\n\t# with open(result_pages_json,'w') as file:\n\t# \tjson.dump(result_pages, file, indent=4, separators=(\",\", \":\"))\n\t\n\twith open(\"/home/ubuntu/MathSearch/front-end/web/result_log\",\"a\") as file:\n\t\targv_str = \" \".join(str(x) for x in argv)\n\t\tprint(argv_str)\n\t\tfile.write(time.strftime(\"%H:%M:%S\", time.localtime()) + \"\\t\" + argv_str)\n\t\tfile.write(\"\\n\")",
      "summary": "# FUNCTION: main\n\n## PURPOSE:\nProcesses PDF files by rendering YOLOv5 equation bounding boxes on specific pages, creating annotated versions of the original PDF with visual highlighting of detected equations.\n\n## INPUTS:\n- `argv` (list): Command line arguments, containing the PDF filename and coordinates for bounding boxes\n\n## OUTPUTS:\n- None (void): The function creates annotated files in output directories but doesn't return a value\n\n## KEY STEPS:\n- Parse command line arguments to extract PDF filename and bounding box coordinates\n- Organize bounding box coordinates into a dictionary by page number\n- Convert specified pages from the PDF to PNG images\n- Draw bounding boxes on these images using the `draw_bounding_box` function\n- Convert annotated images back to PDF\n- Create a new PDF by combining original pages with the annotated ones\n- Log the operation details to a result log file\n\n## DEPENDENCIES:\n- `argparse`: For command line argument parsing\n- `pdf2image`: For converting PDF pages to images\n- `PyPDF2`: For PDF manipulation operations\n- `Image` (from PIL): For image handling\n- `draw_bounding_box`: External function to draw bounding boxes on images\n- `time`: For timestamping log entries\n\n## USAGE CONTEXT:\nUsed as the entry point of a command-line tool for visualizing equation detection results, typically integrated into a math search system for highlighting detected equations in PDF documents.\n\n## EDGE CASES:\n- Validates that the number of bounding box coordinates is a multiple of 5 (page number + 4 coordinates)\n- Will terminate early if the coordinate count is invalid\n\n## RELATIONSHIPS:\n- Acts as the main controller function for the PDF annotation pipeline\n- Interacts with external storage directories for input/output files\n- Part of a larger math search system as indicated by the log file path"
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/etc",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch/public",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch/src",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch/src/components",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch/src/components/pages",
      "code": "",
      "summary": ""
    }
  },
  {
    "page_content": "",
    "metadata": {
      "type": "DIRECTORY",
      "name": "",
      "path": "../mathsearch/front-end/mathsearch/src/components/pages/archive",
      "code": "",
      "summary": ""
    }
  }
]